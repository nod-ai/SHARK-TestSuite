module {
  func.func @torch_jit(%arg0: !torch.vtensor<[1,3,224,224],f32>) -> !torch.vtensor<[1,1000],f32> attributes {torch.onnx_meta.ir_version = 8 : si64, torch.onnx_meta.opset_version = 17 : si64, torch.onnx_meta.producer_name = "pytorch", torch.onnx_meta.producer_version = "1.13.1"} {
    %float1.525880e-05 = torch.constant.float 1.52587890625E-5
    %float3.051760e-05 = torch.constant.float 3.0517578125E-5
    %float6.103520e-05 = torch.constant.float 6.103515625E-5
    %float1.220700e-04 = torch.constant.float 1.220703125E-4
    %int14 = torch.constant.int 14
    %float2.441410e-04 = torch.constant.float 2.44140625E-4
    %float4.882810e-04 = torch.constant.float 4.8828125E-4
    %float1.953130e-03 = torch.constant.float 0.001953125
    %float1.562500e-02 = torch.constant.float 1.562500e-02
    %float3.125000e-02 = torch.constant.float 3.125000e-02
    %float7.812500e-03 = torch.constant.float 7.812500e-03
    %float3.906250e-03 = torch.constant.float 3.906250e-03
    %0 = torch.vtensor.literal(dense<1.00488281> : tensor<f32>) : !torch.vtensor<[],f32>
    %int7 = torch.constant.int 7
    %false = torch.constant.bool false
    %int0 = torch.constant.int 0
    %int2 = torch.constant.int 2
    %int1 = torch.constant.int 1
    %int3 = torch.constant.int 3
    %1 = torch.vtensor.literal(dense_resource<__elided__> : tensor<64x3x7x7xf32>) : !torch.vtensor<[64,3,7,7],f32>
    %2 = torch.vtensor.literal(dense_resource<__elided__> : tensor<64xf32>) : !torch.vtensor<[64],f32>
    %3 = torch.vtensor.literal(dense_resource<__elided__> : tensor<64x64x1x1xf32>) : !torch.vtensor<[64,64,1,1],f32>
    %4 = torch.vtensor.literal(dense_resource<__elided__> : tensor<64xf32>) : !torch.vtensor<[64],f32>
    %5 = torch.vtensor.literal(dense_resource<__elided__> : tensor<64x64x3x3xf32>) : !torch.vtensor<[64,64,3,3],f32>
    %6 = torch.vtensor.literal(dense_resource<__elided__> : tensor<64xf32>) : !torch.vtensor<[64],f32>
    %7 = torch.vtensor.literal(dense_resource<__elided__> : tensor<256x64x1x1xf32>) : !torch.vtensor<[256,64,1,1],f32>
    %8 = torch.vtensor.literal(dense_resource<__elided__> : tensor<256xf32>) : !torch.vtensor<[256],f32>
    %9 = torch.vtensor.literal(dense_resource<__elided__> : tensor<256x64x1x1xf32>) : !torch.vtensor<[256,64,1,1],f32>
    %10 = torch.vtensor.literal(dense_resource<__elided__> : tensor<256xf32>) : !torch.vtensor<[256],f32>
    %11 = torch.vtensor.literal(dense_resource<__elided__> : tensor<64x256x1x1xf32>) : !torch.vtensor<[64,256,1,1],f32>
    %12 = torch.vtensor.literal(dense_resource<__elided__> : tensor<64xf32>) : !torch.vtensor<[64],f32>
    %13 = torch.vtensor.literal(dense_resource<__elided__> : tensor<64x64x3x3xf32>) : !torch.vtensor<[64,64,3,3],f32>
    %14 = torch.vtensor.literal(dense_resource<__elided__> : tensor<64xf32>) : !torch.vtensor<[64],f32>
    %15 = torch.vtensor.literal(dense_resource<__elided__> : tensor<256x64x1x1xf32>) : !torch.vtensor<[256,64,1,1],f32>
    %16 = torch.vtensor.literal(dense_resource<__elided__> : tensor<256xf32>) : !torch.vtensor<[256],f32>
    %17 = torch.vtensor.literal(dense_resource<__elided__> : tensor<64x256x1x1xf32>) : !torch.vtensor<[64,256,1,1],f32>
    %18 = torch.vtensor.literal(dense_resource<__elided__> : tensor<64xf32>) : !torch.vtensor<[64],f32>
    %19 = torch.vtensor.literal(dense_resource<__elided__> : tensor<64x64x3x3xf32>) : !torch.vtensor<[64,64,3,3],f32>
    %20 = torch.vtensor.literal(dense_resource<__elided__> : tensor<64xf32>) : !torch.vtensor<[64],f32>
    %21 = torch.vtensor.literal(dense_resource<__elided__> : tensor<256x64x1x1xf32>) : !torch.vtensor<[256,64,1,1],f32>
    %22 = torch.vtensor.literal(dense_resource<__elided__> : tensor<256xf32>) : !torch.vtensor<[256],f32>
    %23 = torch.vtensor.literal(dense_resource<__elided__> : tensor<128x256x1x1xf32>) : !torch.vtensor<[128,256,1,1],f32>
    %24 = torch.vtensor.literal(dense_resource<__elided__> : tensor<128xf32>) : !torch.vtensor<[128],f32>
    %25 = torch.vtensor.literal(dense_resource<__elided__> : tensor<128x128x3x3xf32>) : !torch.vtensor<[128,128,3,3],f32>
    %26 = torch.vtensor.literal(dense_resource<__elided__> : tensor<128xf32>) : !torch.vtensor<[128],f32>
    %27 = torch.vtensor.literal(dense_resource<__elided__> : tensor<512x128x1x1xf32>) : !torch.vtensor<[512,128,1,1],f32>
    %28 = torch.vtensor.literal(dense_resource<__elided__> : tensor<512xf32>) : !torch.vtensor<[512],f32>
    %29 = torch.vtensor.literal(dense_resource<__elided__> : tensor<512x256x1x1xf32>) : !torch.vtensor<[512,256,1,1],f32>
    %30 = torch.vtensor.literal(dense_resource<__elided__> : tensor<512xf32>) : !torch.vtensor<[512],f32>
    %31 = torch.vtensor.literal(dense_resource<__elided__> : tensor<128x512x1x1xf32>) : !torch.vtensor<[128,512,1,1],f32>
    %32 = torch.vtensor.literal(dense_resource<__elided__> : tensor<128xf32>) : !torch.vtensor<[128],f32>
    %33 = torch.vtensor.literal(dense_resource<__elided__> : tensor<128x128x3x3xf32>) : !torch.vtensor<[128,128,3,3],f32>
    %34 = torch.vtensor.literal(dense_resource<__elided__> : tensor<128xf32>) : !torch.vtensor<[128],f32>
    %35 = torch.vtensor.literal(dense_resource<__elided__> : tensor<512x128x1x1xf32>) : !torch.vtensor<[512,128,1,1],f32>
    %36 = torch.vtensor.literal(dense_resource<__elided__> : tensor<512xf32>) : !torch.vtensor<[512],f32>
    %37 = torch.vtensor.literal(dense_resource<__elided__> : tensor<128x512x1x1xf32>) : !torch.vtensor<[128,512,1,1],f32>
    %38 = torch.vtensor.literal(dense_resource<__elided__> : tensor<128xf32>) : !torch.vtensor<[128],f32>
    %39 = torch.vtensor.literal(dense_resource<__elided__> : tensor<128x128x3x3xf32>) : !torch.vtensor<[128,128,3,3],f32>
    %40 = torch.vtensor.literal(dense_resource<__elided__> : tensor<128xf32>) : !torch.vtensor<[128],f32>
    %41 = torch.vtensor.literal(dense_resource<__elided__> : tensor<512x128x1x1xf32>) : !torch.vtensor<[512,128,1,1],f32>
    %42 = torch.vtensor.literal(dense_resource<__elided__> : tensor<512xf32>) : !torch.vtensor<[512],f32>
    %43 = torch.vtensor.literal(dense_resource<__elided__> : tensor<128x512x1x1xf32>) : !torch.vtensor<[128,512,1,1],f32>
    %44 = torch.vtensor.literal(dense_resource<__elided__> : tensor<128xf32>) : !torch.vtensor<[128],f32>
    %45 = torch.vtensor.literal(dense_resource<__elided__> : tensor<128x128x3x3xf32>) : !torch.vtensor<[128,128,3,3],f32>
    %46 = torch.vtensor.literal(dense_resource<__elided__> : tensor<128xf32>) : !torch.vtensor<[128],f32>
    %47 = torch.vtensor.literal(dense_resource<__elided__> : tensor<512x128x1x1xf32>) : !torch.vtensor<[512,128,1,1],f32>
    %48 = torch.vtensor.literal(dense_resource<__elided__> : tensor<512xf32>) : !torch.vtensor<[512],f32>
    %49 = torch.vtensor.literal(dense_resource<__elided__> : tensor<256x512x1x1xf32>) : !torch.vtensor<[256,512,1,1],f32>
    %50 = torch.vtensor.literal(dense_resource<__elided__> : tensor<256xf32>) : !torch.vtensor<[256],f32>
    %51 = torch.vtensor.literal(dense_resource<__elided__> : tensor<256x256x3x3xf32>) : !torch.vtensor<[256,256,3,3],f32>
    %52 = torch.vtensor.literal(dense_resource<__elided__> : tensor<256xf32>) : !torch.vtensor<[256],f32>
    %53 = torch.vtensor.literal(dense_resource<__elided__> : tensor<1024x256x1x1xf32>) : !torch.vtensor<[1024,256,1,1],f32>
    %54 = torch.vtensor.literal(dense_resource<__elided__> : tensor<1024xf32>) : !torch.vtensor<[1024],f32>
    %55 = torch.vtensor.literal(dense_resource<__elided__> : tensor<1024x512x1x1xf32>) : !torch.vtensor<[1024,512,1,1],f32>
    %56 = torch.vtensor.literal(dense_resource<__elided__> : tensor<1024xf32>) : !torch.vtensor<[1024],f32>
    %57 = torch.vtensor.literal(dense_resource<__elided__> : tensor<256x1024x1x1xf32>) : !torch.vtensor<[256,1024,1,1],f32>
    %58 = torch.vtensor.literal(dense_resource<__elided__> : tensor<256xf32>) : !torch.vtensor<[256],f32>
    %59 = torch.vtensor.literal(dense_resource<__elided__> : tensor<256x256x3x3xf32>) : !torch.vtensor<[256,256,3,3],f32>
    %60 = torch.vtensor.literal(dense_resource<__elided__> : tensor<256xf32>) : !torch.vtensor<[256],f32>
    %61 = torch.vtensor.literal(dense_resource<__elided__> : tensor<1024x256x1x1xf32>) : !torch.vtensor<[1024,256,1,1],f32>
    %62 = torch.vtensor.literal(dense_resource<__elided__> : tensor<1024xf32>) : !torch.vtensor<[1024],f32>
    %63 = torch.vtensor.literal(dense_resource<__elided__> : tensor<256x1024x1x1xf32>) : !torch.vtensor<[256,1024,1,1],f32>
    %64 = torch.vtensor.literal(dense_resource<__elided__> : tensor<256xf32>) : !torch.vtensor<[256],f32>
    %65 = torch.vtensor.literal(dense_resource<__elided__> : tensor<256x256x3x3xf32>) : !torch.vtensor<[256,256,3,3],f32>
    %66 = torch.vtensor.literal(dense_resource<__elided__> : tensor<256xf32>) : !torch.vtensor<[256],f32>
    %67 = torch.vtensor.literal(dense_resource<__elided__> : tensor<1024x256x1x1xf32>) : !torch.vtensor<[1024,256,1,1],f32>
    %68 = torch.vtensor.literal(dense_resource<__elided__> : tensor<1024xf32>) : !torch.vtensor<[1024],f32>
    %69 = torch.vtensor.literal(dense_resource<__elided__> : tensor<256x1024x1x1xf32>) : !torch.vtensor<[256,1024,1,1],f32>
    %70 = torch.vtensor.literal(dense_resource<__elided__> : tensor<256xf32>) : !torch.vtensor<[256],f32>
    %71 = torch.vtensor.literal(dense_resource<__elided__> : tensor<256x256x3x3xf32>) : !torch.vtensor<[256,256,3,3],f32>
    %72 = torch.vtensor.literal(dense_resource<__elided__> : tensor<256xf32>) : !torch.vtensor<[256],f32>
    %73 = torch.vtensor.literal(dense_resource<__elided__> : tensor<1024x256x1x1xf32>) : !torch.vtensor<[1024,256,1,1],f32>
    %74 = torch.vtensor.literal(dense_resource<__elided__> : tensor<1024xf32>) : !torch.vtensor<[1024],f32>
    %75 = torch.vtensor.literal(dense_resource<__elided__> : tensor<256x1024x1x1xf32>) : !torch.vtensor<[256,1024,1,1],f32>
    %76 = torch.vtensor.literal(dense_resource<__elided__> : tensor<256xf32>) : !torch.vtensor<[256],f32>
    %77 = torch.vtensor.literal(dense_resource<__elided__> : tensor<256x256x3x3xf32>) : !torch.vtensor<[256,256,3,3],f32>
    %78 = torch.vtensor.literal(dense_resource<__elided__> : tensor<256xf32>) : !torch.vtensor<[256],f32>
    %79 = torch.vtensor.literal(dense_resource<__elided__> : tensor<1024x256x1x1xf32>) : !torch.vtensor<[1024,256,1,1],f32>
    %80 = torch.vtensor.literal(dense_resource<__elided__> : tensor<1024xf32>) : !torch.vtensor<[1024],f32>
    %81 = torch.vtensor.literal(dense_resource<__elided__> : tensor<256x1024x1x1xf32>) : !torch.vtensor<[256,1024,1,1],f32>
    %82 = torch.vtensor.literal(dense_resource<__elided__> : tensor<256xf32>) : !torch.vtensor<[256],f32>
    %83 = torch.vtensor.literal(dense_resource<__elided__> : tensor<256x256x3x3xf32>) : !torch.vtensor<[256,256,3,3],f32>
    %84 = torch.vtensor.literal(dense_resource<__elided__> : tensor<256xf32>) : !torch.vtensor<[256],f32>
    %85 = torch.vtensor.literal(dense_resource<__elided__> : tensor<1024x256x1x1xf32>) : !torch.vtensor<[1024,256,1,1],f32>
    %86 = torch.vtensor.literal(dense_resource<__elided__> : tensor<1024xf32>) : !torch.vtensor<[1024],f32>
    %87 = torch.vtensor.literal(dense_resource<__elided__> : tensor<512x1024x1x1xf32>) : !torch.vtensor<[512,1024,1,1],f32>
    %88 = torch.vtensor.literal(dense_resource<__elided__> : tensor<512xf32>) : !torch.vtensor<[512],f32>
    %89 = torch.vtensor.literal(dense_resource<__elided__> : tensor<512x512x3x3xf32>) : !torch.vtensor<[512,512,3,3],f32>
    %90 = torch.vtensor.literal(dense_resource<__elided__> : tensor<512xf32>) : !torch.vtensor<[512],f32>
    %91 = torch.vtensor.literal(dense_resource<__elided__> : tensor<2048x512x1x1xf32>) : !torch.vtensor<[2048,512,1,1],f32>
    %92 = torch.vtensor.literal(dense_resource<__elided__> : tensor<2048xf32>) : !torch.vtensor<[2048],f32>
    %93 = torch.vtensor.literal(dense_resource<__elided__> : tensor<2048x1024x1x1xf32>) : !torch.vtensor<[2048,1024,1,1],f32>
    %94 = torch.vtensor.literal(dense_resource<__elided__> : tensor<2048xf32>) : !torch.vtensor<[2048],f32>
    %95 = torch.vtensor.literal(dense_resource<__elided__> : tensor<512x2048x1x1xf32>) : !torch.vtensor<[512,2048,1,1],f32>
    %96 = torch.vtensor.literal(dense_resource<__elided__> : tensor<512xf32>) : !torch.vtensor<[512],f32>
    %97 = torch.vtensor.literal(dense_resource<__elided__> : tensor<512x512x3x3xf32>) : !torch.vtensor<[512,512,3,3],f32>
    %98 = torch.vtensor.literal(dense_resource<__elided__> : tensor<512xf32>) : !torch.vtensor<[512],f32>
    %99 = torch.vtensor.literal(dense_resource<__elided__> : tensor<2048x512x1x1xf32>) : !torch.vtensor<[2048,512,1,1],f32>
    %100 = torch.vtensor.literal(dense_resource<__elided__> : tensor<2048xf32>) : !torch.vtensor<[2048],f32>
    %101 = torch.vtensor.literal(dense_resource<__elided__> : tensor<512x2048x1x1xf32>) : !torch.vtensor<[512,2048,1,1],f32>
    %102 = torch.vtensor.literal(dense_resource<__elided__> : tensor<512xf32>) : !torch.vtensor<[512],f32>
    %103 = torch.vtensor.literal(dense_resource<__elided__> : tensor<512x512x3x3xf32>) : !torch.vtensor<[512,512,3,3],f32>
    %104 = torch.vtensor.literal(dense_resource<__elided__> : tensor<512xf32>) : !torch.vtensor<[512],f32>
    %105 = torch.vtensor.literal(dense_resource<__elided__> : tensor<2048x512x1x1xf32>) : !torch.vtensor<[2048,512,1,1],f32>
    %106 = torch.vtensor.literal(dense_resource<__elided__> : tensor<2048xf32>) : !torch.vtensor<[2048],f32>
    %107 = torch.vtensor.literal(dense_resource<__elided__> : tensor<1000x2048xf32>) : !torch.vtensor<[1000,2048],f32>
    %108 = torch.vtensor.literal(dense_resource<__elided__> : tensor<1000xf32>) : !torch.vtensor<[1000],f32>
    %none = torch.constant.none
    %int12 = torch.constant.int 12
    %float6.250000e-02 = torch.constant.float 6.250000e-02
    %109 = torch.aten.quantize_per_tensor %arg0, %float6.250000e-02, %int0, %int12 : !torch.vtensor<[1,3,224,224],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,3,224,224],!torch.qint8>
    %110 = torch.aten.int_repr %109 : !torch.vtensor<[1,3,224,224],!torch.qint8> -> !torch.vtensor<[1,3,224,224],si8>
    %111 = torch.aten._make_per_tensor_quantized_tensor %110, %float6.250000e-02, %int0 : !torch.vtensor<[1,3,224,224],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,3,224,224],!torch.qint8>
    %112 = torch.aten.quantize_per_tensor %1, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[64,3,7,7],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[64,3,7,7],!torch.qint8>
    %113 = torch.aten.int_repr %112 : !torch.vtensor<[64,3,7,7],!torch.qint8> -> !torch.vtensor<[64,3,7,7],si8>
    %114 = torch.aten._make_per_tensor_quantized_tensor %113, %float3.906250e-03, %int0 : !torch.vtensor<[64,3,7,7],si8>, !torch.float, !torch.int -> !torch.vtensor<[64,3,7,7],!torch.qint8>
    %115 = torch.aten.quantize_per_tensor %2, %float7.812500e-03, %int0, %int12 : !torch.vtensor<[64],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[64],!torch.qint8>
    %116 = torch.aten.int_repr %115 : !torch.vtensor<[64],!torch.qint8> -> !torch.vtensor<[64],si8>
    %117 = torch.aten._make_per_tensor_quantized_tensor %116, %float7.812500e-03, %int0 : !torch.vtensor<[64],si8>, !torch.float, !torch.int -> !torch.vtensor<[64],!torch.qint8>
    %118 = torch.aten.dequantize.self %117 : !torch.vtensor<[64],!torch.qint8> -> !torch.vtensor<[64],f32>
    %119 = torch.prim.ListConstruct %int3, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %120 = torch.prim.ListConstruct %int1, %int1 : (!torch.int, !torch.int) -> !torch.list<int>
    %121 = torch.prim.ListConstruct %int2, %int2 : (!torch.int, !torch.int) -> !torch.list<int>
    %122 = torch.prim.ListConstruct %int0, %int0 : (!torch.int, !torch.int) -> !torch.list<int>
    %123 = torch.aten.quantize_per_tensor %118, %float2.441410e-04, %int0, %int14 : !torch.vtensor<[64],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[64],!torch.qint32>
    %124 = torch.aten.int_repr %123 : !torch.vtensor<[64],!torch.qint32> -> !torch.vtensor<[64],si32>
    %125 = torch.aten.convolution %111, %114, %124, %121, %119, %120, %false, %122, %int1 : !torch.vtensor<[1,3,224,224],!torch.qint8>, !torch.vtensor<[64,3,7,7],!torch.qint8>, !torch.vtensor<[64],si32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,64,112,112],si32>
    %126 = torch.aten._make_per_tensor_quantized_tensor %125, %float2.441410e-04, %int0 : !torch.vtensor<[1,64,112,112],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,112,112],!torch.qint32>
    %127 = torch.aten.relu %126 : !torch.vtensor<[1,64,112,112],!torch.qint32> -> !torch.vtensor<[1,64,112,112],!torch.qint32>
    %128 = torch.aten.int_repr %127 : !torch.vtensor<[1,64,112,112],!torch.qint32> -> !torch.vtensor<[1,64,112,112],si32>
    %129 = torch.aten._make_per_tensor_quantized_tensor %128, %float2.441410e-04, %int0 : !torch.vtensor<[1,64,112,112],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,112,112],!torch.qint32>
    %130 = torch.aten.dequantize.tensor %129 : !torch.vtensor<[1,64,112,112],!torch.qint32> -> !torch.vtensor<[1,64,112,112],f32>
    %131 = torch.aten.quantize_per_tensor %130, %float3.125000e-02, %int0, %int12 : !torch.vtensor<[1,64,112,112],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,64,112,112],!torch.qint8>
    %132 = torch.aten.int_repr %131 : !torch.vtensor<[1,64,112,112],!torch.qint8> -> !torch.vtensor<[1,64,112,112],si8>
    %133 = torch.aten._make_per_tensor_quantized_tensor %132, %float3.125000e-02, %int0 : !torch.vtensor<[1,64,112,112],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,64,112,112],!torch.qint8>
    %134 = torch.aten.dequantize.self %133 : !torch.vtensor<[1,64,112,112],!torch.qint8> -> !torch.vtensor<[1,64,112,112],f32>
    %135 = torch.aten.max_pool2d %134, %119, %121, %120, %120, %false : !torch.vtensor<[1,64,112,112],f32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool -> !torch.vtensor<[1,64,56,56],f32>
    %136 = torch.aten.quantize_per_tensor %135, %float3.125000e-02, %int0, %int12 : !torch.vtensor<[1,64,56,56],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,64,56,56],!torch.qint8>
    %137 = torch.aten.int_repr %136 : !torch.vtensor<[1,64,56,56],!torch.qint8> -> !torch.vtensor<[1,64,56,56],si8>
    %138 = torch.aten._make_per_tensor_quantized_tensor %137, %float3.125000e-02, %int0 : !torch.vtensor<[1,64,56,56],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,64,56,56],!torch.qint8>
    %139 = torch.aten.quantize_per_tensor %3, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[64,64,1,1],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[64,64,1,1],!torch.qint8>
    %140 = torch.aten.int_repr %139 : !torch.vtensor<[64,64,1,1],!torch.qint8> -> !torch.vtensor<[64,64,1,1],si8>
    %141 = torch.aten._make_per_tensor_quantized_tensor %140, %float3.906250e-03, %int0 : !torch.vtensor<[64,64,1,1],si8>, !torch.float, !torch.int -> !torch.vtensor<[64,64,1,1],!torch.qint8>
    %142 = torch.aten.quantize_per_tensor %4, %float7.812500e-03, %int0, %int12 : !torch.vtensor<[64],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[64],!torch.qint8>
    %143 = torch.aten.int_repr %142 : !torch.vtensor<[64],!torch.qint8> -> !torch.vtensor<[64],si8>
    %144 = torch.aten._make_per_tensor_quantized_tensor %143, %float7.812500e-03, %int0 : !torch.vtensor<[64],si8>, !torch.float, !torch.int -> !torch.vtensor<[64],!torch.qint8>
    %145 = torch.aten.dequantize.self %144 : !torch.vtensor<[64],!torch.qint8> -> !torch.vtensor<[64],f32>
    %146 = torch.aten.quantize_per_tensor %145, %float1.220700e-04, %int0, %int14 : !torch.vtensor<[64],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[64],!torch.qint32>
    %147 = torch.aten.int_repr %146 : !torch.vtensor<[64],!torch.qint32> -> !torch.vtensor<[64],si32>
    %148 = torch.aten.convolution %138, %141, %147, %120, %122, %120, %false, %122, %int1 : !torch.vtensor<[1,64,56,56],!torch.qint8>, !torch.vtensor<[64,64,1,1],!torch.qint8>, !torch.vtensor<[64],si32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,64,56,56],si32>
    %149 = torch.aten._make_per_tensor_quantized_tensor %148, %float1.220700e-04, %int0 : !torch.vtensor<[1,64,56,56],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,56,56],!torch.qint32>
    %150 = torch.aten.relu %149 : !torch.vtensor<[1,64,56,56],!torch.qint32> -> !torch.vtensor<[1,64,56,56],!torch.qint32>
    %151 = torch.aten.int_repr %150 : !torch.vtensor<[1,64,56,56],!torch.qint32> -> !torch.vtensor<[1,64,56,56],si32>
    %152 = torch.aten._make_per_tensor_quantized_tensor %151, %float1.220700e-04, %int0 : !torch.vtensor<[1,64,56,56],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,56,56],!torch.qint32>
    %153 = torch.aten.dequantize.tensor %152 : !torch.vtensor<[1,64,56,56],!torch.qint32> -> !torch.vtensor<[1,64,56,56],f32>
    %154 = torch.aten.quantize_per_tensor %153, %float1.562500e-02, %int0, %int12 : !torch.vtensor<[1,64,56,56],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,64,56,56],!torch.qint8>
    %155 = torch.aten.int_repr %154 : !torch.vtensor<[1,64,56,56],!torch.qint8> -> !torch.vtensor<[1,64,56,56],si8>
    %156 = torch.aten._make_per_tensor_quantized_tensor %155, %float1.562500e-02, %int0 : !torch.vtensor<[1,64,56,56],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,64,56,56],!torch.qint8>
    %157 = torch.aten.quantize_per_tensor %5, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[64,64,3,3],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[64,64,3,3],!torch.qint8>
    %158 = torch.aten.int_repr %157 : !torch.vtensor<[64,64,3,3],!torch.qint8> -> !torch.vtensor<[64,64,3,3],si8>
    %159 = torch.aten._make_per_tensor_quantized_tensor %158, %float3.906250e-03, %int0 : !torch.vtensor<[64,64,3,3],si8>, !torch.float, !torch.int -> !torch.vtensor<[64,64,3,3],!torch.qint8>
    %160 = torch.aten.quantize_per_tensor %6, %float3.125000e-02, %int0, %int12 : !torch.vtensor<[64],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[64],!torch.qint8>
    %161 = torch.aten.int_repr %160 : !torch.vtensor<[64],!torch.qint8> -> !torch.vtensor<[64],si8>
    %162 = torch.aten._make_per_tensor_quantized_tensor %161, %float3.125000e-02, %int0 : !torch.vtensor<[64],si8>, !torch.float, !torch.int -> !torch.vtensor<[64],!torch.qint8>
    %163 = torch.aten.dequantize.self %162 : !torch.vtensor<[64],!torch.qint8> -> !torch.vtensor<[64],f32>
    %164 = torch.aten.quantize_per_tensor %163, %float6.103520e-05, %int0, %int14 : !torch.vtensor<[64],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[64],!torch.qint32>
    %165 = torch.aten.int_repr %164 : !torch.vtensor<[64],!torch.qint32> -> !torch.vtensor<[64],si32>
    %166 = torch.aten.convolution %156, %159, %165, %120, %120, %120, %false, %122, %int1 : !torch.vtensor<[1,64,56,56],!torch.qint8>, !torch.vtensor<[64,64,3,3],!torch.qint8>, !torch.vtensor<[64],si32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,64,56,56],si32>
    %167 = torch.aten._make_per_tensor_quantized_tensor %166, %float6.103520e-05, %int0 : !torch.vtensor<[1,64,56,56],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,56,56],!torch.qint32>
    %168 = torch.aten.relu %167 : !torch.vtensor<[1,64,56,56],!torch.qint32> -> !torch.vtensor<[1,64,56,56],!torch.qint32>
    %169 = torch.aten.int_repr %168 : !torch.vtensor<[1,64,56,56],!torch.qint32> -> !torch.vtensor<[1,64,56,56],si32>
    %170 = torch.aten._make_per_tensor_quantized_tensor %169, %float6.103520e-05, %int0 : !torch.vtensor<[1,64,56,56],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,56,56],!torch.qint32>
    %171 = torch.aten.dequantize.tensor %170 : !torch.vtensor<[1,64,56,56],!torch.qint32> -> !torch.vtensor<[1,64,56,56],f32>
    %172 = torch.aten.quantize_per_tensor %171, %float1.562500e-02, %int0, %int12 : !torch.vtensor<[1,64,56,56],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,64,56,56],!torch.qint8>
    %173 = torch.aten.int_repr %172 : !torch.vtensor<[1,64,56,56],!torch.qint8> -> !torch.vtensor<[1,64,56,56],si8>
    %174 = torch.aten._make_per_tensor_quantized_tensor %173, %float1.562500e-02, %int0 : !torch.vtensor<[1,64,56,56],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,64,56,56],!torch.qint8>
    %175 = torch.aten.quantize_per_tensor %7, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[256,64,1,1],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[256,64,1,1],!torch.qint8>
    %176 = torch.aten.int_repr %175 : !torch.vtensor<[256,64,1,1],!torch.qint8> -> !torch.vtensor<[256,64,1,1],si8>
    %177 = torch.aten._make_per_tensor_quantized_tensor %176, %float3.906250e-03, %int0 : !torch.vtensor<[256,64,1,1],si8>, !torch.float, !torch.int -> !torch.vtensor<[256,64,1,1],!torch.qint8>
    %178 = torch.aten.quantize_per_tensor %8, %float7.812500e-03, %int0, %int12 : !torch.vtensor<[256],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[256],!torch.qint8>
    %179 = torch.aten.int_repr %178 : !torch.vtensor<[256],!torch.qint8> -> !torch.vtensor<[256],si8>
    %180 = torch.aten._make_per_tensor_quantized_tensor %179, %float7.812500e-03, %int0 : !torch.vtensor<[256],si8>, !torch.float, !torch.int -> !torch.vtensor<[256],!torch.qint8>
    %181 = torch.aten.dequantize.self %180 : !torch.vtensor<[256],!torch.qint8> -> !torch.vtensor<[256],f32>
    %182 = torch.aten.quantize_per_tensor %181, %float6.103520e-05, %int0, %int14 : !torch.vtensor<[256],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[256],!torch.qint32>
    %183 = torch.aten.int_repr %182 : !torch.vtensor<[256],!torch.qint32> -> !torch.vtensor<[256],si32>
    %184 = torch.aten.convolution %174, %177, %183, %120, %122, %120, %false, %122, %int1 : !torch.vtensor<[1,64,56,56],!torch.qint8>, !torch.vtensor<[256,64,1,1],!torch.qint8>, !torch.vtensor<[256],si32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,256,56,56],si32>
    %185 = torch.aten._make_per_tensor_quantized_tensor %184, %float6.103520e-05, %int0 : !torch.vtensor<[1,256,56,56],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,256,56,56],!torch.qint32>
    %186 = torch.aten.dequantize.tensor %185 : !torch.vtensor<[1,256,56,56],!torch.qint32> -> !torch.vtensor<[1,256,56,56],f32>
    %187 = torch.aten.quantize_per_tensor %186, %float7.812500e-03, %int0, %int12 : !torch.vtensor<[1,256,56,56],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,256,56,56],!torch.qint8>
    %188 = torch.aten.int_repr %187 : !torch.vtensor<[1,256,56,56],!torch.qint8> -> !torch.vtensor<[1,256,56,56],si8>
    %189 = torch.aten._make_per_tensor_quantized_tensor %188, %float7.812500e-03, %int0 : !torch.vtensor<[1,256,56,56],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,256,56,56],!torch.qint8>
    %190 = torch.aten.dequantize.self %189 : !torch.vtensor<[1,256,56,56],!torch.qint8> -> !torch.vtensor<[1,256,56,56],f32>
    %191 = torch.aten.quantize_per_tensor %9, %float1.562500e-02, %int0, %int12 : !torch.vtensor<[256,64,1,1],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[256,64,1,1],!torch.qint8>
    %192 = torch.aten.int_repr %191 : !torch.vtensor<[256,64,1,1],!torch.qint8> -> !torch.vtensor<[256,64,1,1],si8>
    %193 = torch.aten._make_per_tensor_quantized_tensor %192, %float1.562500e-02, %int0 : !torch.vtensor<[256,64,1,1],si8>, !torch.float, !torch.int -> !torch.vtensor<[256,64,1,1],!torch.qint8>
    %194 = torch.aten.quantize_per_tensor %10, %float3.125000e-02, %int0, %int12 : !torch.vtensor<[256],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[256],!torch.qint8>
    %195 = torch.aten.int_repr %194 : !torch.vtensor<[256],!torch.qint8> -> !torch.vtensor<[256],si8>
    %196 = torch.aten._make_per_tensor_quantized_tensor %195, %float3.125000e-02, %int0 : !torch.vtensor<[256],si8>, !torch.float, !torch.int -> !torch.vtensor<[256],!torch.qint8>
    %197 = torch.aten.dequantize.self %196 : !torch.vtensor<[256],!torch.qint8> -> !torch.vtensor<[256],f32>
    %198 = torch.aten.quantize_per_tensor %197, %float4.882810e-04, %int0, %int14 : !torch.vtensor<[256],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[256],!torch.qint32>
    %199 = torch.aten.int_repr %198 : !torch.vtensor<[256],!torch.qint32> -> !torch.vtensor<[256],si32>
    %200 = torch.aten.convolution %138, %193, %199, %120, %122, %120, %false, %122, %int1 : !torch.vtensor<[1,64,56,56],!torch.qint8>, !torch.vtensor<[256,64,1,1],!torch.qint8>, !torch.vtensor<[256],si32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,256,56,56],si32>
    %201 = torch.aten._make_per_tensor_quantized_tensor %200, %float4.882810e-04, %int0 : !torch.vtensor<[1,256,56,56],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,256,56,56],!torch.qint32>
    %202 = torch.aten.dequantize.tensor %201 : !torch.vtensor<[1,256,56,56],!torch.qint32> -> !torch.vtensor<[1,256,56,56],f32>
    %203 = torch.aten.quantize_per_tensor %202, %float1.562500e-02, %int0, %int12 : !torch.vtensor<[1,256,56,56],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,256,56,56],!torch.qint8>
    %204 = torch.aten.int_repr %203 : !torch.vtensor<[1,256,56,56],!torch.qint8> -> !torch.vtensor<[1,256,56,56],si8>
    %205 = torch.aten._make_per_tensor_quantized_tensor %204, %float1.562500e-02, %int0 : !torch.vtensor<[1,256,56,56],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,256,56,56],!torch.qint8>
    %206 = torch.aten.dequantize.self %205 : !torch.vtensor<[1,256,56,56],!torch.qint8> -> !torch.vtensor<[1,256,56,56],f32>
    %207 = torch.aten.add.Tensor %190, %206, %int1 : !torch.vtensor<[1,256,56,56],f32>, !torch.vtensor<[1,256,56,56],f32>, !torch.int -> !torch.vtensor<[1,256,56,56],f32>
    %208 = torch.aten.relu %207 : !torch.vtensor<[1,256,56,56],f32> -> !torch.vtensor<[1,256,56,56],f32>
    %209 = torch.aten.quantize_per_tensor %208, %float1.562500e-02, %int0, %int12 : !torch.vtensor<[1,256,56,56],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,256,56,56],!torch.qint8>
    %210 = torch.aten.int_repr %209 : !torch.vtensor<[1,256,56,56],!torch.qint8> -> !torch.vtensor<[1,256,56,56],si8>
    %211 = torch.aten._make_per_tensor_quantized_tensor %210, %float1.562500e-02, %int0 : !torch.vtensor<[1,256,56,56],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,256,56,56],!torch.qint8>
    %212 = torch.aten.dequantize.self %211 : !torch.vtensor<[1,256,56,56],!torch.qint8> -> !torch.vtensor<[1,256,56,56],f32>
    %213 = torch.aten.quantize_per_tensor %11, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[64,256,1,1],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[64,256,1,1],!torch.qint8>
    %214 = torch.aten.int_repr %213 : !torch.vtensor<[64,256,1,1],!torch.qint8> -> !torch.vtensor<[64,256,1,1],si8>
    %215 = torch.aten._make_per_tensor_quantized_tensor %214, %float3.906250e-03, %int0 : !torch.vtensor<[64,256,1,1],si8>, !torch.float, !torch.int -> !torch.vtensor<[64,256,1,1],!torch.qint8>
    %216 = torch.aten.quantize_per_tensor %12, %float7.812500e-03, %int0, %int12 : !torch.vtensor<[64],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[64],!torch.qint8>
    %217 = torch.aten.int_repr %216 : !torch.vtensor<[64],!torch.qint8> -> !torch.vtensor<[64],si8>
    %218 = torch.aten._make_per_tensor_quantized_tensor %217, %float7.812500e-03, %int0 : !torch.vtensor<[64],si8>, !torch.float, !torch.int -> !torch.vtensor<[64],!torch.qint8>
    %219 = torch.aten.dequantize.self %218 : !torch.vtensor<[64],!torch.qint8> -> !torch.vtensor<[64],f32>
    %220 = torch.aten.quantize_per_tensor %219, %float6.103520e-05, %int0, %int14 : !torch.vtensor<[64],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[64],!torch.qint32>
    %221 = torch.aten.int_repr %220 : !torch.vtensor<[64],!torch.qint32> -> !torch.vtensor<[64],si32>
    %222 = torch.aten.convolution %211, %215, %221, %120, %122, %120, %false, %122, %int1 : !torch.vtensor<[1,256,56,56],!torch.qint8>, !torch.vtensor<[64,256,1,1],!torch.qint8>, !torch.vtensor<[64],si32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,64,56,56],si32>
    %223 = torch.aten._make_per_tensor_quantized_tensor %222, %float6.103520e-05, %int0 : !torch.vtensor<[1,64,56,56],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,56,56],!torch.qint32>
    %224 = torch.aten.relu %223 : !torch.vtensor<[1,64,56,56],!torch.qint32> -> !torch.vtensor<[1,64,56,56],!torch.qint32>
    %225 = torch.aten.int_repr %224 : !torch.vtensor<[1,64,56,56],!torch.qint32> -> !torch.vtensor<[1,64,56,56],si32>
    %226 = torch.aten._make_per_tensor_quantized_tensor %225, %float6.103520e-05, %int0 : !torch.vtensor<[1,64,56,56],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,56,56],!torch.qint32>
    %227 = torch.aten.dequantize.tensor %226 : !torch.vtensor<[1,64,56,56],!torch.qint32> -> !torch.vtensor<[1,64,56,56],f32>
    %228 = torch.aten.quantize_per_tensor %227, %float1.562500e-02, %int0, %int12 : !torch.vtensor<[1,64,56,56],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,64,56,56],!torch.qint8>
    %229 = torch.aten.int_repr %228 : !torch.vtensor<[1,64,56,56],!torch.qint8> -> !torch.vtensor<[1,64,56,56],si8>
    %230 = torch.aten._make_per_tensor_quantized_tensor %229, %float1.562500e-02, %int0 : !torch.vtensor<[1,64,56,56],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,64,56,56],!torch.qint8>
    %231 = torch.aten.quantize_per_tensor %13, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[64,64,3,3],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[64,64,3,3],!torch.qint8>
    %232 = torch.aten.int_repr %231 : !torch.vtensor<[64,64,3,3],!torch.qint8> -> !torch.vtensor<[64,64,3,3],si8>
    %233 = torch.aten._make_per_tensor_quantized_tensor %232, %float3.906250e-03, %int0 : !torch.vtensor<[64,64,3,3],si8>, !torch.float, !torch.int -> !torch.vtensor<[64,64,3,3],!torch.qint8>
    %234 = torch.aten.quantize_per_tensor %14, %float7.812500e-03, %int0, %int12 : !torch.vtensor<[64],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[64],!torch.qint8>
    %235 = torch.aten.int_repr %234 : !torch.vtensor<[64],!torch.qint8> -> !torch.vtensor<[64],si8>
    %236 = torch.aten._make_per_tensor_quantized_tensor %235, %float7.812500e-03, %int0 : !torch.vtensor<[64],si8>, !torch.float, !torch.int -> !torch.vtensor<[64],!torch.qint8>
    %237 = torch.aten.dequantize.self %236 : !torch.vtensor<[64],!torch.qint8> -> !torch.vtensor<[64],f32>
    %238 = torch.aten.quantize_per_tensor %237, %float6.103520e-05, %int0, %int14 : !torch.vtensor<[64],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[64],!torch.qint32>
    %239 = torch.aten.int_repr %238 : !torch.vtensor<[64],!torch.qint32> -> !torch.vtensor<[64],si32>
    %240 = torch.aten.convolution %230, %233, %239, %120, %120, %120, %false, %122, %int1 : !torch.vtensor<[1,64,56,56],!torch.qint8>, !torch.vtensor<[64,64,3,3],!torch.qint8>, !torch.vtensor<[64],si32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,64,56,56],si32>
    %241 = torch.aten._make_per_tensor_quantized_tensor %240, %float6.103520e-05, %int0 : !torch.vtensor<[1,64,56,56],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,56,56],!torch.qint32>
    %242 = torch.aten.relu %241 : !torch.vtensor<[1,64,56,56],!torch.qint32> -> !torch.vtensor<[1,64,56,56],!torch.qint32>
    %243 = torch.aten.int_repr %242 : !torch.vtensor<[1,64,56,56],!torch.qint32> -> !torch.vtensor<[1,64,56,56],si32>
    %244 = torch.aten._make_per_tensor_quantized_tensor %243, %float6.103520e-05, %int0 : !torch.vtensor<[1,64,56,56],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,56,56],!torch.qint32>
    %245 = torch.aten.dequantize.tensor %244 : !torch.vtensor<[1,64,56,56],!torch.qint32> -> !torch.vtensor<[1,64,56,56],f32>
    %246 = torch.aten.quantize_per_tensor %245, %float1.562500e-02, %int0, %int12 : !torch.vtensor<[1,64,56,56],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,64,56,56],!torch.qint8>
    %247 = torch.aten.int_repr %246 : !torch.vtensor<[1,64,56,56],!torch.qint8> -> !torch.vtensor<[1,64,56,56],si8>
    %248 = torch.aten._make_per_tensor_quantized_tensor %247, %float1.562500e-02, %int0 : !torch.vtensor<[1,64,56,56],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,64,56,56],!torch.qint8>
    %249 = torch.aten.quantize_per_tensor %15, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[256,64,1,1],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[256,64,1,1],!torch.qint8>
    %250 = torch.aten.int_repr %249 : !torch.vtensor<[256,64,1,1],!torch.qint8> -> !torch.vtensor<[256,64,1,1],si8>
    %251 = torch.aten._make_per_tensor_quantized_tensor %250, %float3.906250e-03, %int0 : !torch.vtensor<[256,64,1,1],si8>, !torch.float, !torch.int -> !torch.vtensor<[256,64,1,1],!torch.qint8>
    %252 = torch.aten.quantize_per_tensor %16, %float7.812500e-03, %int0, %int12 : !torch.vtensor<[256],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[256],!torch.qint8>
    %253 = torch.aten.int_repr %252 : !torch.vtensor<[256],!torch.qint8> -> !torch.vtensor<[256],si8>
    %254 = torch.aten._make_per_tensor_quantized_tensor %253, %float7.812500e-03, %int0 : !torch.vtensor<[256],si8>, !torch.float, !torch.int -> !torch.vtensor<[256],!torch.qint8>
    %255 = torch.aten.dequantize.self %254 : !torch.vtensor<[256],!torch.qint8> -> !torch.vtensor<[256],f32>
    %256 = torch.aten.quantize_per_tensor %255, %float6.103520e-05, %int0, %int14 : !torch.vtensor<[256],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[256],!torch.qint32>
    %257 = torch.aten.int_repr %256 : !torch.vtensor<[256],!torch.qint32> -> !torch.vtensor<[256],si32>
    %258 = torch.aten.convolution %248, %251, %257, %120, %122, %120, %false, %122, %int1 : !torch.vtensor<[1,64,56,56],!torch.qint8>, !torch.vtensor<[256,64,1,1],!torch.qint8>, !torch.vtensor<[256],si32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,256,56,56],si32>
    %259 = torch.aten._make_per_tensor_quantized_tensor %258, %float6.103520e-05, %int0 : !torch.vtensor<[1,256,56,56],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,256,56,56],!torch.qint32>
    %260 = torch.aten.dequantize.tensor %259 : !torch.vtensor<[1,256,56,56],!torch.qint32> -> !torch.vtensor<[1,256,56,56],f32>
    %261 = torch.aten.quantize_per_tensor %260, %float7.812500e-03, %int0, %int12 : !torch.vtensor<[1,256,56,56],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,256,56,56],!torch.qint8>
    %262 = torch.aten.int_repr %261 : !torch.vtensor<[1,256,56,56],!torch.qint8> -> !torch.vtensor<[1,256,56,56],si8>
    %263 = torch.aten._make_per_tensor_quantized_tensor %262, %float7.812500e-03, %int0 : !torch.vtensor<[1,256,56,56],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,256,56,56],!torch.qint8>
    %264 = torch.aten.dequantize.self %263 : !torch.vtensor<[1,256,56,56],!torch.qint8> -> !torch.vtensor<[1,256,56,56],f32>
    %265 = torch.aten.add.Tensor %264, %212, %int1 : !torch.vtensor<[1,256,56,56],f32>, !torch.vtensor<[1,256,56,56],f32>, !torch.int -> !torch.vtensor<[1,256,56,56],f32>
    %266 = torch.aten.relu %265 : !torch.vtensor<[1,256,56,56],f32> -> !torch.vtensor<[1,256,56,56],f32>
    %267 = torch.aten.quantize_per_tensor %266, %float1.562500e-02, %int0, %int12 : !torch.vtensor<[1,256,56,56],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,256,56,56],!torch.qint8>
    %268 = torch.aten.int_repr %267 : !torch.vtensor<[1,256,56,56],!torch.qint8> -> !torch.vtensor<[1,256,56,56],si8>
    %269 = torch.aten._make_per_tensor_quantized_tensor %268, %float1.562500e-02, %int0 : !torch.vtensor<[1,256,56,56],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,256,56,56],!torch.qint8>
    %270 = torch.aten.dequantize.self %269 : !torch.vtensor<[1,256,56,56],!torch.qint8> -> !torch.vtensor<[1,256,56,56],f32>
    %271 = torch.aten.quantize_per_tensor %17, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[64,256,1,1],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[64,256,1,1],!torch.qint8>
    %272 = torch.aten.int_repr %271 : !torch.vtensor<[64,256,1,1],!torch.qint8> -> !torch.vtensor<[64,256,1,1],si8>
    %273 = torch.aten._make_per_tensor_quantized_tensor %272, %float3.906250e-03, %int0 : !torch.vtensor<[64,256,1,1],si8>, !torch.float, !torch.int -> !torch.vtensor<[64,256,1,1],!torch.qint8>
    %274 = torch.aten.quantize_per_tensor %18, %float1.562500e-02, %int0, %int12 : !torch.vtensor<[64],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[64],!torch.qint8>
    %275 = torch.aten.int_repr %274 : !torch.vtensor<[64],!torch.qint8> -> !torch.vtensor<[64],si8>
    %276 = torch.aten._make_per_tensor_quantized_tensor %275, %float1.562500e-02, %int0 : !torch.vtensor<[64],si8>, !torch.float, !torch.int -> !torch.vtensor<[64],!torch.qint8>
    %277 = torch.aten.dequantize.self %276 : !torch.vtensor<[64],!torch.qint8> -> !torch.vtensor<[64],f32>
    %278 = torch.aten.quantize_per_tensor %277, %float6.103520e-05, %int0, %int14 : !torch.vtensor<[64],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[64],!torch.qint32>
    %279 = torch.aten.int_repr %278 : !torch.vtensor<[64],!torch.qint32> -> !torch.vtensor<[64],si32>
    %280 = torch.aten.convolution %269, %273, %279, %120, %122, %120, %false, %122, %int1 : !torch.vtensor<[1,256,56,56],!torch.qint8>, !torch.vtensor<[64,256,1,1],!torch.qint8>, !torch.vtensor<[64],si32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,64,56,56],si32>
    %281 = torch.aten._make_per_tensor_quantized_tensor %280, %float6.103520e-05, %int0 : !torch.vtensor<[1,64,56,56],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,56,56],!torch.qint32>
    %282 = torch.aten.relu %281 : !torch.vtensor<[1,64,56,56],!torch.qint32> -> !torch.vtensor<[1,64,56,56],!torch.qint32>
    %283 = torch.aten.int_repr %282 : !torch.vtensor<[1,64,56,56],!torch.qint32> -> !torch.vtensor<[1,64,56,56],si32>
    %284 = torch.aten._make_per_tensor_quantized_tensor %283, %float6.103520e-05, %int0 : !torch.vtensor<[1,64,56,56],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,56,56],!torch.qint32>
    %285 = torch.aten.dequantize.tensor %284 : !torch.vtensor<[1,64,56,56],!torch.qint32> -> !torch.vtensor<[1,64,56,56],f32>
    %286 = torch.aten.quantize_per_tensor %285, %float7.812500e-03, %int0, %int12 : !torch.vtensor<[1,64,56,56],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,64,56,56],!torch.qint8>
    %287 = torch.aten.int_repr %286 : !torch.vtensor<[1,64,56,56],!torch.qint8> -> !torch.vtensor<[1,64,56,56],si8>
    %288 = torch.aten._make_per_tensor_quantized_tensor %287, %float7.812500e-03, %int0 : !torch.vtensor<[1,64,56,56],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,64,56,56],!torch.qint8>
    %289 = torch.aten.quantize_per_tensor %19, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[64,64,3,3],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[64,64,3,3],!torch.qint8>
    %290 = torch.aten.int_repr %289 : !torch.vtensor<[64,64,3,3],!torch.qint8> -> !torch.vtensor<[64,64,3,3],si8>
    %291 = torch.aten._make_per_tensor_quantized_tensor %290, %float3.906250e-03, %int0 : !torch.vtensor<[64,64,3,3],si8>, !torch.float, !torch.int -> !torch.vtensor<[64,64,3,3],!torch.qint8>
    %292 = torch.aten.quantize_per_tensor %20, %float7.812500e-03, %int0, %int12 : !torch.vtensor<[64],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[64],!torch.qint8>
    %293 = torch.aten.int_repr %292 : !torch.vtensor<[64],!torch.qint8> -> !torch.vtensor<[64],si8>
    %294 = torch.aten._make_per_tensor_quantized_tensor %293, %float7.812500e-03, %int0 : !torch.vtensor<[64],si8>, !torch.float, !torch.int -> !torch.vtensor<[64],!torch.qint8>
    %295 = torch.aten.dequantize.self %294 : !torch.vtensor<[64],!torch.qint8> -> !torch.vtensor<[64],f32>
    %296 = torch.aten.quantize_per_tensor %295, %float3.051760e-05, %int0, %int14 : !torch.vtensor<[64],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[64],!torch.qint32>
    %297 = torch.aten.int_repr %296 : !torch.vtensor<[64],!torch.qint32> -> !torch.vtensor<[64],si32>
    %298 = torch.aten.convolution %288, %291, %297, %120, %120, %120, %false, %122, %int1 : !torch.vtensor<[1,64,56,56],!torch.qint8>, !torch.vtensor<[64,64,3,3],!torch.qint8>, !torch.vtensor<[64],si32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,64,56,56],si32>
    %299 = torch.aten._make_per_tensor_quantized_tensor %298, %float3.051760e-05, %int0 : !torch.vtensor<[1,64,56,56],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,56,56],!torch.qint32>
    %300 = torch.aten.relu %299 : !torch.vtensor<[1,64,56,56],!torch.qint32> -> !torch.vtensor<[1,64,56,56],!torch.qint32>
    %301 = torch.aten.int_repr %300 : !torch.vtensor<[1,64,56,56],!torch.qint32> -> !torch.vtensor<[1,64,56,56],si32>
    %302 = torch.aten._make_per_tensor_quantized_tensor %301, %float3.051760e-05, %int0 : !torch.vtensor<[1,64,56,56],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,64,56,56],!torch.qint32>
    %303 = torch.aten.dequantize.tensor %302 : !torch.vtensor<[1,64,56,56],!torch.qint32> -> !torch.vtensor<[1,64,56,56],f32>
    %304 = torch.aten.quantize_per_tensor %303, %float1.562500e-02, %int0, %int12 : !torch.vtensor<[1,64,56,56],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,64,56,56],!torch.qint8>
    %305 = torch.aten.int_repr %304 : !torch.vtensor<[1,64,56,56],!torch.qint8> -> !torch.vtensor<[1,64,56,56],si8>
    %306 = torch.aten._make_per_tensor_quantized_tensor %305, %float1.562500e-02, %int0 : !torch.vtensor<[1,64,56,56],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,64,56,56],!torch.qint8>
    %307 = torch.aten.quantize_per_tensor %21, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[256,64,1,1],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[256,64,1,1],!torch.qint8>
    %308 = torch.aten.int_repr %307 : !torch.vtensor<[256,64,1,1],!torch.qint8> -> !torch.vtensor<[256,64,1,1],si8>
    %309 = torch.aten._make_per_tensor_quantized_tensor %308, %float3.906250e-03, %int0 : !torch.vtensor<[256,64,1,1],si8>, !torch.float, !torch.int -> !torch.vtensor<[256,64,1,1],!torch.qint8>
    %310 = torch.aten.quantize_per_tensor %22, %float7.812500e-03, %int0, %int12 : !torch.vtensor<[256],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[256],!torch.qint8>
    %311 = torch.aten.int_repr %310 : !torch.vtensor<[256],!torch.qint8> -> !torch.vtensor<[256],si8>
    %312 = torch.aten._make_per_tensor_quantized_tensor %311, %float7.812500e-03, %int0 : !torch.vtensor<[256],si8>, !torch.float, !torch.int -> !torch.vtensor<[256],!torch.qint8>
    %313 = torch.aten.dequantize.self %312 : !torch.vtensor<[256],!torch.qint8> -> !torch.vtensor<[256],f32>
    %314 = torch.aten.quantize_per_tensor %313, %float6.103520e-05, %int0, %int14 : !torch.vtensor<[256],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[256],!torch.qint32>
    %315 = torch.aten.int_repr %314 : !torch.vtensor<[256],!torch.qint32> -> !torch.vtensor<[256],si32>
    %316 = torch.aten.convolution %306, %309, %315, %120, %122, %120, %false, %122, %int1 : !torch.vtensor<[1,64,56,56],!torch.qint8>, !torch.vtensor<[256,64,1,1],!torch.qint8>, !torch.vtensor<[256],si32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,256,56,56],si32>
    %317 = torch.aten._make_per_tensor_quantized_tensor %316, %float6.103520e-05, %int0 : !torch.vtensor<[1,256,56,56],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,256,56,56],!torch.qint32>
    %318 = torch.aten.dequantize.tensor %317 : !torch.vtensor<[1,256,56,56],!torch.qint32> -> !torch.vtensor<[1,256,56,56],f32>
    %319 = torch.aten.quantize_per_tensor %318, %float7.812500e-03, %int0, %int12 : !torch.vtensor<[1,256,56,56],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,256,56,56],!torch.qint8>
    %320 = torch.aten.int_repr %319 : !torch.vtensor<[1,256,56,56],!torch.qint8> -> !torch.vtensor<[1,256,56,56],si8>
    %321 = torch.aten._make_per_tensor_quantized_tensor %320, %float7.812500e-03, %int0 : !torch.vtensor<[1,256,56,56],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,256,56,56],!torch.qint8>
    %322 = torch.aten.dequantize.self %321 : !torch.vtensor<[1,256,56,56],!torch.qint8> -> !torch.vtensor<[1,256,56,56],f32>
    %323 = torch.aten.add.Tensor %322, %270, %int1 : !torch.vtensor<[1,256,56,56],f32>, !torch.vtensor<[1,256,56,56],f32>, !torch.int -> !torch.vtensor<[1,256,56,56],f32>
    %324 = torch.aten.relu %323 : !torch.vtensor<[1,256,56,56],f32> -> !torch.vtensor<[1,256,56,56],f32>
    %325 = torch.aten.quantize_per_tensor %324, %float1.562500e-02, %int0, %int12 : !torch.vtensor<[1,256,56,56],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,256,56,56],!torch.qint8>
    %326 = torch.aten.int_repr %325 : !torch.vtensor<[1,256,56,56],!torch.qint8> -> !torch.vtensor<[1,256,56,56],si8>
    %327 = torch.aten._make_per_tensor_quantized_tensor %326, %float1.562500e-02, %int0 : !torch.vtensor<[1,256,56,56],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,256,56,56],!torch.qint8>
    %328 = torch.aten.quantize_per_tensor %23, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[128,256,1,1],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[128,256,1,1],!torch.qint8>
    %329 = torch.aten.int_repr %328 : !torch.vtensor<[128,256,1,1],!torch.qint8> -> !torch.vtensor<[128,256,1,1],si8>
    %330 = torch.aten._make_per_tensor_quantized_tensor %329, %float3.906250e-03, %int0 : !torch.vtensor<[128,256,1,1],si8>, !torch.float, !torch.int -> !torch.vtensor<[128,256,1,1],!torch.qint8>
    %331 = torch.aten.quantize_per_tensor %24, %float7.812500e-03, %int0, %int12 : !torch.vtensor<[128],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[128],!torch.qint8>
    %332 = torch.aten.int_repr %331 : !torch.vtensor<[128],!torch.qint8> -> !torch.vtensor<[128],si8>
    %333 = torch.aten._make_per_tensor_quantized_tensor %332, %float7.812500e-03, %int0 : !torch.vtensor<[128],si8>, !torch.float, !torch.int -> !torch.vtensor<[128],!torch.qint8>
    %334 = torch.aten.dequantize.self %333 : !torch.vtensor<[128],!torch.qint8> -> !torch.vtensor<[128],f32>
    %335 = torch.aten.quantize_per_tensor %334, %float6.103520e-05, %int0, %int14 : !torch.vtensor<[128],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[128],!torch.qint32>
    %336 = torch.aten.int_repr %335 : !torch.vtensor<[128],!torch.qint32> -> !torch.vtensor<[128],si32>
    %337 = torch.aten.convolution %327, %330, %336, %120, %122, %120, %false, %122, %int1 : !torch.vtensor<[1,256,56,56],!torch.qint8>, !torch.vtensor<[128,256,1,1],!torch.qint8>, !torch.vtensor<[128],si32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,128,56,56],si32>
    %338 = torch.aten._make_per_tensor_quantized_tensor %337, %float6.103520e-05, %int0 : !torch.vtensor<[1,128,56,56],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,128,56,56],!torch.qint32>
    %339 = torch.aten.relu %338 : !torch.vtensor<[1,128,56,56],!torch.qint32> -> !torch.vtensor<[1,128,56,56],!torch.qint32>
    %340 = torch.aten.int_repr %339 : !torch.vtensor<[1,128,56,56],!torch.qint32> -> !torch.vtensor<[1,128,56,56],si32>
    %341 = torch.aten._make_per_tensor_quantized_tensor %340, %float6.103520e-05, %int0 : !torch.vtensor<[1,128,56,56],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,128,56,56],!torch.qint32>
    %342 = torch.aten.dequantize.tensor %341 : !torch.vtensor<[1,128,56,56],!torch.qint32> -> !torch.vtensor<[1,128,56,56],f32>
    %343 = torch.aten.quantize_per_tensor %342, %float7.812500e-03, %int0, %int12 : !torch.vtensor<[1,128,56,56],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,128,56,56],!torch.qint8>
    %344 = torch.aten.int_repr %343 : !torch.vtensor<[1,128,56,56],!torch.qint8> -> !torch.vtensor<[1,128,56,56],si8>
    %345 = torch.aten._make_per_tensor_quantized_tensor %344, %float7.812500e-03, %int0 : !torch.vtensor<[1,128,56,56],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,128,56,56],!torch.qint8>
    %346 = torch.aten.quantize_per_tensor %25, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[128,128,3,3],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[128,128,3,3],!torch.qint8>
    %347 = torch.aten.int_repr %346 : !torch.vtensor<[128,128,3,3],!torch.qint8> -> !torch.vtensor<[128,128,3,3],si8>
    %348 = torch.aten._make_per_tensor_quantized_tensor %347, %float3.906250e-03, %int0 : !torch.vtensor<[128,128,3,3],si8>, !torch.float, !torch.int -> !torch.vtensor<[128,128,3,3],!torch.qint8>
    %349 = torch.aten.quantize_per_tensor %26, %float7.812500e-03, %int0, %int12 : !torch.vtensor<[128],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[128],!torch.qint8>
    %350 = torch.aten.int_repr %349 : !torch.vtensor<[128],!torch.qint8> -> !torch.vtensor<[128],si8>
    %351 = torch.aten._make_per_tensor_quantized_tensor %350, %float7.812500e-03, %int0 : !torch.vtensor<[128],si8>, !torch.float, !torch.int -> !torch.vtensor<[128],!torch.qint8>
    %352 = torch.aten.dequantize.self %351 : !torch.vtensor<[128],!torch.qint8> -> !torch.vtensor<[128],f32>
    %353 = torch.aten.quantize_per_tensor %352, %float3.051760e-05, %int0, %int14 : !torch.vtensor<[128],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[128],!torch.qint32>
    %354 = torch.aten.int_repr %353 : !torch.vtensor<[128],!torch.qint32> -> !torch.vtensor<[128],si32>
    %355 = torch.aten.convolution %345, %348, %354, %121, %120, %120, %false, %122, %int1 : !torch.vtensor<[1,128,56,56],!torch.qint8>, !torch.vtensor<[128,128,3,3],!torch.qint8>, !torch.vtensor<[128],si32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,128,28,28],si32>
    %356 = torch.aten._make_per_tensor_quantized_tensor %355, %float3.051760e-05, %int0 : !torch.vtensor<[1,128,28,28],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,128,28,28],!torch.qint32>
    %357 = torch.aten.relu %356 : !torch.vtensor<[1,128,28,28],!torch.qint32> -> !torch.vtensor<[1,128,28,28],!torch.qint32>
    %358 = torch.aten.int_repr %357 : !torch.vtensor<[1,128,28,28],!torch.qint32> -> !torch.vtensor<[1,128,28,28],si32>
    %359 = torch.aten._make_per_tensor_quantized_tensor %358, %float3.051760e-05, %int0 : !torch.vtensor<[1,128,28,28],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,128,28,28],!torch.qint32>
    %360 = torch.aten.dequantize.tensor %359 : !torch.vtensor<[1,128,28,28],!torch.qint32> -> !torch.vtensor<[1,128,28,28],f32>
    %361 = torch.aten.quantize_per_tensor %360, %float1.562500e-02, %int0, %int12 : !torch.vtensor<[1,128,28,28],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,128,28,28],!torch.qint8>
    %362 = torch.aten.int_repr %361 : !torch.vtensor<[1,128,28,28],!torch.qint8> -> !torch.vtensor<[1,128,28,28],si8>
    %363 = torch.aten._make_per_tensor_quantized_tensor %362, %float1.562500e-02, %int0 : !torch.vtensor<[1,128,28,28],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,128,28,28],!torch.qint8>
    %364 = torch.aten.quantize_per_tensor %27, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[512,128,1,1],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[512,128,1,1],!torch.qint8>
    %365 = torch.aten.int_repr %364 : !torch.vtensor<[512,128,1,1],!torch.qint8> -> !torch.vtensor<[512,128,1,1],si8>
    %366 = torch.aten._make_per_tensor_quantized_tensor %365, %float3.906250e-03, %int0 : !torch.vtensor<[512,128,1,1],si8>, !torch.float, !torch.int -> !torch.vtensor<[512,128,1,1],!torch.qint8>
    %367 = torch.aten.quantize_per_tensor %28, %float7.812500e-03, %int0, %int12 : !torch.vtensor<[512],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[512],!torch.qint8>
    %368 = torch.aten.int_repr %367 : !torch.vtensor<[512],!torch.qint8> -> !torch.vtensor<[512],si8>
    %369 = torch.aten._make_per_tensor_quantized_tensor %368, %float7.812500e-03, %int0 : !torch.vtensor<[512],si8>, !torch.float, !torch.int -> !torch.vtensor<[512],!torch.qint8>
    %370 = torch.aten.dequantize.self %369 : !torch.vtensor<[512],!torch.qint8> -> !torch.vtensor<[512],f32>
    %371 = torch.aten.quantize_per_tensor %370, %float6.103520e-05, %int0, %int14 : !torch.vtensor<[512],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[512],!torch.qint32>
    %372 = torch.aten.int_repr %371 : !torch.vtensor<[512],!torch.qint32> -> !torch.vtensor<[512],si32>
    %373 = torch.aten.convolution %363, %366, %372, %120, %122, %120, %false, %122, %int1 : !torch.vtensor<[1,128,28,28],!torch.qint8>, !torch.vtensor<[512,128,1,1],!torch.qint8>, !torch.vtensor<[512],si32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,512,28,28],si32>
    %374 = torch.aten._make_per_tensor_quantized_tensor %373, %float6.103520e-05, %int0 : !torch.vtensor<[1,512,28,28],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,512,28,28],!torch.qint32>
    %375 = torch.aten.dequantize.tensor %374 : !torch.vtensor<[1,512,28,28],!torch.qint32> -> !torch.vtensor<[1,512,28,28],f32>
    %376 = torch.aten.quantize_per_tensor %375, %float7.812500e-03, %int0, %int12 : !torch.vtensor<[1,512,28,28],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,512,28,28],!torch.qint8>
    %377 = torch.aten.int_repr %376 : !torch.vtensor<[1,512,28,28],!torch.qint8> -> !torch.vtensor<[1,512,28,28],si8>
    %378 = torch.aten._make_per_tensor_quantized_tensor %377, %float7.812500e-03, %int0 : !torch.vtensor<[1,512,28,28],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,512,28,28],!torch.qint8>
    %379 = torch.aten.dequantize.self %378 : !torch.vtensor<[1,512,28,28],!torch.qint8> -> !torch.vtensor<[1,512,28,28],f32>
    %380 = torch.aten.quantize_per_tensor %29, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[512,256,1,1],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[512,256,1,1],!torch.qint8>
    %381 = torch.aten.int_repr %380 : !torch.vtensor<[512,256,1,1],!torch.qint8> -> !torch.vtensor<[512,256,1,1],si8>
    %382 = torch.aten._make_per_tensor_quantized_tensor %381, %float3.906250e-03, %int0 : !torch.vtensor<[512,256,1,1],si8>, !torch.float, !torch.int -> !torch.vtensor<[512,256,1,1],!torch.qint8>
    %383 = torch.aten.quantize_per_tensor %30, %float1.562500e-02, %int0, %int12 : !torch.vtensor<[512],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[512],!torch.qint8>
    %384 = torch.aten.int_repr %383 : !torch.vtensor<[512],!torch.qint8> -> !torch.vtensor<[512],si8>
    %385 = torch.aten._make_per_tensor_quantized_tensor %384, %float1.562500e-02, %int0 : !torch.vtensor<[512],si8>, !torch.float, !torch.int -> !torch.vtensor<[512],!torch.qint8>
    %386 = torch.aten.dequantize.self %385 : !torch.vtensor<[512],!torch.qint8> -> !torch.vtensor<[512],f32>
    %387 = torch.aten.quantize_per_tensor %386, %float6.103520e-05, %int0, %int14 : !torch.vtensor<[512],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[512],!torch.qint32>
    %388 = torch.aten.int_repr %387 : !torch.vtensor<[512],!torch.qint32> -> !torch.vtensor<[512],si32>
    %389 = torch.aten.convolution %327, %382, %388, %121, %122, %120, %false, %122, %int1 : !torch.vtensor<[1,256,56,56],!torch.qint8>, !torch.vtensor<[512,256,1,1],!torch.qint8>, !torch.vtensor<[512],si32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,512,28,28],si32>
    %390 = torch.aten._make_per_tensor_quantized_tensor %389, %float6.103520e-05, %int0 : !torch.vtensor<[1,512,28,28],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,512,28,28],!torch.qint32>
    %391 = torch.aten.dequantize.tensor %390 : !torch.vtensor<[1,512,28,28],!torch.qint32> -> !torch.vtensor<[1,512,28,28],f32>
    %392 = torch.aten.quantize_per_tensor %391, %float1.562500e-02, %int0, %int12 : !torch.vtensor<[1,512,28,28],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,512,28,28],!torch.qint8>
    %393 = torch.aten.int_repr %392 : !torch.vtensor<[1,512,28,28],!torch.qint8> -> !torch.vtensor<[1,512,28,28],si8>
    %394 = torch.aten._make_per_tensor_quantized_tensor %393, %float1.562500e-02, %int0 : !torch.vtensor<[1,512,28,28],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,512,28,28],!torch.qint8>
    %395 = torch.aten.dequantize.self %394 : !torch.vtensor<[1,512,28,28],!torch.qint8> -> !torch.vtensor<[1,512,28,28],f32>
    %396 = torch.aten.add.Tensor %379, %395, %int1 : !torch.vtensor<[1,512,28,28],f32>, !torch.vtensor<[1,512,28,28],f32>, !torch.int -> !torch.vtensor<[1,512,28,28],f32>
    %397 = torch.aten.relu %396 : !torch.vtensor<[1,512,28,28],f32> -> !torch.vtensor<[1,512,28,28],f32>
    %398 = torch.aten.quantize_per_tensor %397, %float1.562500e-02, %int0, %int12 : !torch.vtensor<[1,512,28,28],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,512,28,28],!torch.qint8>
    %399 = torch.aten.int_repr %398 : !torch.vtensor<[1,512,28,28],!torch.qint8> -> !torch.vtensor<[1,512,28,28],si8>
    %400 = torch.aten._make_per_tensor_quantized_tensor %399, %float1.562500e-02, %int0 : !torch.vtensor<[1,512,28,28],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,512,28,28],!torch.qint8>
    %401 = torch.aten.dequantize.self %400 : !torch.vtensor<[1,512,28,28],!torch.qint8> -> !torch.vtensor<[1,512,28,28],f32>
    %402 = torch.aten.quantize_per_tensor %31, %float1.953130e-03, %int0, %int12 : !torch.vtensor<[128,512,1,1],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[128,512,1,1],!torch.qint8>
    %403 = torch.aten.int_repr %402 : !torch.vtensor<[128,512,1,1],!torch.qint8> -> !torch.vtensor<[128,512,1,1],si8>
    %404 = torch.aten._make_per_tensor_quantized_tensor %403, %float1.953130e-03, %int0 : !torch.vtensor<[128,512,1,1],si8>, !torch.float, !torch.int -> !torch.vtensor<[128,512,1,1],!torch.qint8>
    %405 = torch.aten.quantize_per_tensor %32, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[128],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[128],!torch.qint8>
    %406 = torch.aten.int_repr %405 : !torch.vtensor<[128],!torch.qint8> -> !torch.vtensor<[128],si8>
    %407 = torch.aten._make_per_tensor_quantized_tensor %406, %float3.906250e-03, %int0 : !torch.vtensor<[128],si8>, !torch.float, !torch.int -> !torch.vtensor<[128],!torch.qint8>
    %408 = torch.aten.dequantize.self %407 : !torch.vtensor<[128],!torch.qint8> -> !torch.vtensor<[128],f32>
    %409 = torch.aten.quantize_per_tensor %408, %float3.051760e-05, %int0, %int14 : !torch.vtensor<[128],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[128],!torch.qint32>
    %410 = torch.aten.int_repr %409 : !torch.vtensor<[128],!torch.qint32> -> !torch.vtensor<[128],si32>
    %411 = torch.aten.convolution %400, %404, %410, %120, %122, %120, %false, %122, %int1 : !torch.vtensor<[1,512,28,28],!torch.qint8>, !torch.vtensor<[128,512,1,1],!torch.qint8>, !torch.vtensor<[128],si32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,128,28,28],si32>
    %412 = torch.aten._make_per_tensor_quantized_tensor %411, %float3.051760e-05, %int0 : !torch.vtensor<[1,128,28,28],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,128,28,28],!torch.qint32>
    %413 = torch.aten.relu %412 : !torch.vtensor<[1,128,28,28],!torch.qint32> -> !torch.vtensor<[1,128,28,28],!torch.qint32>
    %414 = torch.aten.int_repr %413 : !torch.vtensor<[1,128,28,28],!torch.qint32> -> !torch.vtensor<[1,128,28,28],si32>
    %415 = torch.aten._make_per_tensor_quantized_tensor %414, %float3.051760e-05, %int0 : !torch.vtensor<[1,128,28,28],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,128,28,28],!torch.qint32>
    %416 = torch.aten.dequantize.tensor %415 : !torch.vtensor<[1,128,28,28],!torch.qint32> -> !torch.vtensor<[1,128,28,28],f32>
    %417 = torch.aten.quantize_per_tensor %416, %float1.562500e-02, %int0, %int12 : !torch.vtensor<[1,128,28,28],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,128,28,28],!torch.qint8>
    %418 = torch.aten.int_repr %417 : !torch.vtensor<[1,128,28,28],!torch.qint8> -> !torch.vtensor<[1,128,28,28],si8>
    %419 = torch.aten._make_per_tensor_quantized_tensor %418, %float1.562500e-02, %int0 : !torch.vtensor<[1,128,28,28],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,128,28,28],!torch.qint8>
    %420 = torch.aten.quantize_per_tensor %33, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[128,128,3,3],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[128,128,3,3],!torch.qint8>
    %421 = torch.aten.int_repr %420 : !torch.vtensor<[128,128,3,3],!torch.qint8> -> !torch.vtensor<[128,128,3,3],si8>
    %422 = torch.aten._make_per_tensor_quantized_tensor %421, %float3.906250e-03, %int0 : !torch.vtensor<[128,128,3,3],si8>, !torch.float, !torch.int -> !torch.vtensor<[128,128,3,3],!torch.qint8>
    %423 = torch.aten.quantize_per_tensor %34, %float1.562500e-02, %int0, %int12 : !torch.vtensor<[128],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[128],!torch.qint8>
    %424 = torch.aten.int_repr %423 : !torch.vtensor<[128],!torch.qint8> -> !torch.vtensor<[128],si8>
    %425 = torch.aten._make_per_tensor_quantized_tensor %424, %float1.562500e-02, %int0 : !torch.vtensor<[128],si8>, !torch.float, !torch.int -> !torch.vtensor<[128],!torch.qint8>
    %426 = torch.aten.dequantize.self %425 : !torch.vtensor<[128],!torch.qint8> -> !torch.vtensor<[128],f32>
    %427 = torch.aten.quantize_per_tensor %426, %float6.103520e-05, %int0, %int14 : !torch.vtensor<[128],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[128],!torch.qint32>
    %428 = torch.aten.int_repr %427 : !torch.vtensor<[128],!torch.qint32> -> !torch.vtensor<[128],si32>
    %429 = torch.aten.convolution %419, %422, %428, %120, %120, %120, %false, %122, %int1 : !torch.vtensor<[1,128,28,28],!torch.qint8>, !torch.vtensor<[128,128,3,3],!torch.qint8>, !torch.vtensor<[128],si32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,128,28,28],si32>
    %430 = torch.aten._make_per_tensor_quantized_tensor %429, %float6.103520e-05, %int0 : !torch.vtensor<[1,128,28,28],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,128,28,28],!torch.qint32>
    %431 = torch.aten.relu %430 : !torch.vtensor<[1,128,28,28],!torch.qint32> -> !torch.vtensor<[1,128,28,28],!torch.qint32>
    %432 = torch.aten.int_repr %431 : !torch.vtensor<[1,128,28,28],!torch.qint32> -> !torch.vtensor<[1,128,28,28],si32>
    %433 = torch.aten._make_per_tensor_quantized_tensor %432, %float6.103520e-05, %int0 : !torch.vtensor<[1,128,28,28],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,128,28,28],!torch.qint32>
    %434 = torch.aten.dequantize.tensor %433 : !torch.vtensor<[1,128,28,28],!torch.qint32> -> !torch.vtensor<[1,128,28,28],f32>
    %435 = torch.aten.quantize_per_tensor %434, %float3.125000e-02, %int0, %int12 : !torch.vtensor<[1,128,28,28],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,128,28,28],!torch.qint8>
    %436 = torch.aten.int_repr %435 : !torch.vtensor<[1,128,28,28],!torch.qint8> -> !torch.vtensor<[1,128,28,28],si8>
    %437 = torch.aten._make_per_tensor_quantized_tensor %436, %float3.125000e-02, %int0 : !torch.vtensor<[1,128,28,28],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,128,28,28],!torch.qint8>
    %438 = torch.aten.quantize_per_tensor %35, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[512,128,1,1],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[512,128,1,1],!torch.qint8>
    %439 = torch.aten.int_repr %438 : !torch.vtensor<[512,128,1,1],!torch.qint8> -> !torch.vtensor<[512,128,1,1],si8>
    %440 = torch.aten._make_per_tensor_quantized_tensor %439, %float3.906250e-03, %int0 : !torch.vtensor<[512,128,1,1],si8>, !torch.float, !torch.int -> !torch.vtensor<[512,128,1,1],!torch.qint8>
    %441 = torch.aten.quantize_per_tensor %36, %float7.812500e-03, %int0, %int12 : !torch.vtensor<[512],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[512],!torch.qint8>
    %442 = torch.aten.int_repr %441 : !torch.vtensor<[512],!torch.qint8> -> !torch.vtensor<[512],si8>
    %443 = torch.aten._make_per_tensor_quantized_tensor %442, %float7.812500e-03, %int0 : !torch.vtensor<[512],si8>, !torch.float, !torch.int -> !torch.vtensor<[512],!torch.qint8>
    %444 = torch.aten.dequantize.self %443 : !torch.vtensor<[512],!torch.qint8> -> !torch.vtensor<[512],f32>
    %445 = torch.aten.quantize_per_tensor %444, %float1.220700e-04, %int0, %int14 : !torch.vtensor<[512],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[512],!torch.qint32>
    %446 = torch.aten.int_repr %445 : !torch.vtensor<[512],!torch.qint32> -> !torch.vtensor<[512],si32>
    %447 = torch.aten.convolution %437, %440, %446, %120, %122, %120, %false, %122, %int1 : !torch.vtensor<[1,128,28,28],!torch.qint8>, !torch.vtensor<[512,128,1,1],!torch.qint8>, !torch.vtensor<[512],si32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,512,28,28],si32>
    %448 = torch.aten._make_per_tensor_quantized_tensor %447, %float1.220700e-04, %int0 : !torch.vtensor<[1,512,28,28],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,512,28,28],!torch.qint32>
    %449 = torch.aten.dequantize.tensor %448 : !torch.vtensor<[1,512,28,28],!torch.qint32> -> !torch.vtensor<[1,512,28,28],f32>
    %450 = torch.aten.quantize_per_tensor %449, %float1.562500e-02, %int0, %int12 : !torch.vtensor<[1,512,28,28],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,512,28,28],!torch.qint8>
    %451 = torch.aten.int_repr %450 : !torch.vtensor<[1,512,28,28],!torch.qint8> -> !torch.vtensor<[1,512,28,28],si8>
    %452 = torch.aten._make_per_tensor_quantized_tensor %451, %float1.562500e-02, %int0 : !torch.vtensor<[1,512,28,28],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,512,28,28],!torch.qint8>
    %453 = torch.aten.dequantize.self %452 : !torch.vtensor<[1,512,28,28],!torch.qint8> -> !torch.vtensor<[1,512,28,28],f32>
    %454 = torch.aten.add.Tensor %453, %401, %int1 : !torch.vtensor<[1,512,28,28],f32>, !torch.vtensor<[1,512,28,28],f32>, !torch.int -> !torch.vtensor<[1,512,28,28],f32>
    %455 = torch.aten.relu %454 : !torch.vtensor<[1,512,28,28],f32> -> !torch.vtensor<[1,512,28,28],f32>
    %456 = torch.aten.quantize_per_tensor %455, %float1.562500e-02, %int0, %int12 : !torch.vtensor<[1,512,28,28],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,512,28,28],!torch.qint8>
    %457 = torch.aten.int_repr %456 : !torch.vtensor<[1,512,28,28],!torch.qint8> -> !torch.vtensor<[1,512,28,28],si8>
    %458 = torch.aten._make_per_tensor_quantized_tensor %457, %float1.562500e-02, %int0 : !torch.vtensor<[1,512,28,28],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,512,28,28],!torch.qint8>
    %459 = torch.aten.dequantize.self %458 : !torch.vtensor<[1,512,28,28],!torch.qint8> -> !torch.vtensor<[1,512,28,28],f32>
    %460 = torch.aten.quantize_per_tensor %37, %float1.953130e-03, %int0, %int12 : !torch.vtensor<[128,512,1,1],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[128,512,1,1],!torch.qint8>
    %461 = torch.aten.int_repr %460 : !torch.vtensor<[128,512,1,1],!torch.qint8> -> !torch.vtensor<[128,512,1,1],si8>
    %462 = torch.aten._make_per_tensor_quantized_tensor %461, %float1.953130e-03, %int0 : !torch.vtensor<[128,512,1,1],si8>, !torch.float, !torch.int -> !torch.vtensor<[128,512,1,1],!torch.qint8>
    %463 = torch.aten.quantize_per_tensor %38, %float7.812500e-03, %int0, %int12 : !torch.vtensor<[128],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[128],!torch.qint8>
    %464 = torch.aten.int_repr %463 : !torch.vtensor<[128],!torch.qint8> -> !torch.vtensor<[128],si8>
    %465 = torch.aten._make_per_tensor_quantized_tensor %464, %float7.812500e-03, %int0 : !torch.vtensor<[128],si8>, !torch.float, !torch.int -> !torch.vtensor<[128],!torch.qint8>
    %466 = torch.aten.dequantize.self %465 : !torch.vtensor<[128],!torch.qint8> -> !torch.vtensor<[128],f32>
    %467 = torch.aten.quantize_per_tensor %466, %float3.051760e-05, %int0, %int14 : !torch.vtensor<[128],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[128],!torch.qint32>
    %468 = torch.aten.int_repr %467 : !torch.vtensor<[128],!torch.qint32> -> !torch.vtensor<[128],si32>
    %469 = torch.aten.convolution %458, %462, %468, %120, %122, %120, %false, %122, %int1 : !torch.vtensor<[1,512,28,28],!torch.qint8>, !torch.vtensor<[128,512,1,1],!torch.qint8>, !torch.vtensor<[128],si32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,128,28,28],si32>
    %470 = torch.aten._make_per_tensor_quantized_tensor %469, %float3.051760e-05, %int0 : !torch.vtensor<[1,128,28,28],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,128,28,28],!torch.qint32>
    %471 = torch.aten.relu %470 : !torch.vtensor<[1,128,28,28],!torch.qint32> -> !torch.vtensor<[1,128,28,28],!torch.qint32>
    %472 = torch.aten.int_repr %471 : !torch.vtensor<[1,128,28,28],!torch.qint32> -> !torch.vtensor<[1,128,28,28],si32>
    %473 = torch.aten._make_per_tensor_quantized_tensor %472, %float3.051760e-05, %int0 : !torch.vtensor<[1,128,28,28],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,128,28,28],!torch.qint32>
    %474 = torch.aten.dequantize.tensor %473 : !torch.vtensor<[1,128,28,28],!torch.qint32> -> !torch.vtensor<[1,128,28,28],f32>
    %475 = torch.aten.quantize_per_tensor %474, %float7.812500e-03, %int0, %int12 : !torch.vtensor<[1,128,28,28],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,128,28,28],!torch.qint8>
    %476 = torch.aten.int_repr %475 : !torch.vtensor<[1,128,28,28],!torch.qint8> -> !torch.vtensor<[1,128,28,28],si8>
    %477 = torch.aten._make_per_tensor_quantized_tensor %476, %float7.812500e-03, %int0 : !torch.vtensor<[1,128,28,28],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,128,28,28],!torch.qint8>
    %478 = torch.aten.quantize_per_tensor %39, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[128,128,3,3],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[128,128,3,3],!torch.qint8>
    %479 = torch.aten.int_repr %478 : !torch.vtensor<[128,128,3,3],!torch.qint8> -> !torch.vtensor<[128,128,3,3],si8>
    %480 = torch.aten._make_per_tensor_quantized_tensor %479, %float3.906250e-03, %int0 : !torch.vtensor<[128,128,3,3],si8>, !torch.float, !torch.int -> !torch.vtensor<[128,128,3,3],!torch.qint8>
    %481 = torch.aten.quantize_per_tensor %40, %float7.812500e-03, %int0, %int12 : !torch.vtensor<[128],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[128],!torch.qint8>
    %482 = torch.aten.int_repr %481 : !torch.vtensor<[128],!torch.qint8> -> !torch.vtensor<[128],si8>
    %483 = torch.aten._make_per_tensor_quantized_tensor %482, %float7.812500e-03, %int0 : !torch.vtensor<[128],si8>, !torch.float, !torch.int -> !torch.vtensor<[128],!torch.qint8>
    %484 = torch.aten.dequantize.self %483 : !torch.vtensor<[128],!torch.qint8> -> !torch.vtensor<[128],f32>
    %485 = torch.aten.quantize_per_tensor %484, %float3.051760e-05, %int0, %int14 : !torch.vtensor<[128],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[128],!torch.qint32>
    %486 = torch.aten.int_repr %485 : !torch.vtensor<[128],!torch.qint32> -> !torch.vtensor<[128],si32>
    %487 = torch.aten.convolution %477, %480, %486, %120, %120, %120, %false, %122, %int1 : !torch.vtensor<[1,128,28,28],!torch.qint8>, !torch.vtensor<[128,128,3,3],!torch.qint8>, !torch.vtensor<[128],si32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,128,28,28],si32>
    %488 = torch.aten._make_per_tensor_quantized_tensor %487, %float3.051760e-05, %int0 : !torch.vtensor<[1,128,28,28],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,128,28,28],!torch.qint32>
    %489 = torch.aten.relu %488 : !torch.vtensor<[1,128,28,28],!torch.qint32> -> !torch.vtensor<[1,128,28,28],!torch.qint32>
    %490 = torch.aten.int_repr %489 : !torch.vtensor<[1,128,28,28],!torch.qint32> -> !torch.vtensor<[1,128,28,28],si32>
    %491 = torch.aten._make_per_tensor_quantized_tensor %490, %float3.051760e-05, %int0 : !torch.vtensor<[1,128,28,28],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,128,28,28],!torch.qint32>
    %492 = torch.aten.dequantize.tensor %491 : !torch.vtensor<[1,128,28,28],!torch.qint32> -> !torch.vtensor<[1,128,28,28],f32>
    %493 = torch.aten.quantize_per_tensor %492, %float1.562500e-02, %int0, %int12 : !torch.vtensor<[1,128,28,28],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,128,28,28],!torch.qint8>
    %494 = torch.aten.int_repr %493 : !torch.vtensor<[1,128,28,28],!torch.qint8> -> !torch.vtensor<[1,128,28,28],si8>
    %495 = torch.aten._make_per_tensor_quantized_tensor %494, %float1.562500e-02, %int0 : !torch.vtensor<[1,128,28,28],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,128,28,28],!torch.qint8>
    %496 = torch.aten.quantize_per_tensor %41, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[512,128,1,1],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[512,128,1,1],!torch.qint8>
    %497 = torch.aten.int_repr %496 : !torch.vtensor<[512,128,1,1],!torch.qint8> -> !torch.vtensor<[512,128,1,1],si8>
    %498 = torch.aten._make_per_tensor_quantized_tensor %497, %float3.906250e-03, %int0 : !torch.vtensor<[512,128,1,1],si8>, !torch.float, !torch.int -> !torch.vtensor<[512,128,1,1],!torch.qint8>
    %499 = torch.aten.quantize_per_tensor %42, %float7.812500e-03, %int0, %int12 : !torch.vtensor<[512],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[512],!torch.qint8>
    %500 = torch.aten.int_repr %499 : !torch.vtensor<[512],!torch.qint8> -> !torch.vtensor<[512],si8>
    %501 = torch.aten._make_per_tensor_quantized_tensor %500, %float7.812500e-03, %int0 : !torch.vtensor<[512],si8>, !torch.float, !torch.int -> !torch.vtensor<[512],!torch.qint8>
    %502 = torch.aten.dequantize.self %501 : !torch.vtensor<[512],!torch.qint8> -> !torch.vtensor<[512],f32>
    %503 = torch.aten.quantize_per_tensor %502, %float6.103520e-05, %int0, %int14 : !torch.vtensor<[512],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[512],!torch.qint32>
    %504 = torch.aten.int_repr %503 : !torch.vtensor<[512],!torch.qint32> -> !torch.vtensor<[512],si32>
    %505 = torch.aten.convolution %495, %498, %504, %120, %122, %120, %false, %122, %int1 : !torch.vtensor<[1,128,28,28],!torch.qint8>, !torch.vtensor<[512,128,1,1],!torch.qint8>, !torch.vtensor<[512],si32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,512,28,28],si32>
    %506 = torch.aten._make_per_tensor_quantized_tensor %505, %float6.103520e-05, %int0 : !torch.vtensor<[1,512,28,28],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,512,28,28],!torch.qint32>
    %507 = torch.aten.dequantize.tensor %506 : !torch.vtensor<[1,512,28,28],!torch.qint32> -> !torch.vtensor<[1,512,28,28],f32>
    %508 = torch.aten.quantize_per_tensor %507, %float7.812500e-03, %int0, %int12 : !torch.vtensor<[1,512,28,28],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,512,28,28],!torch.qint8>
    %509 = torch.aten.int_repr %508 : !torch.vtensor<[1,512,28,28],!torch.qint8> -> !torch.vtensor<[1,512,28,28],si8>
    %510 = torch.aten._make_per_tensor_quantized_tensor %509, %float7.812500e-03, %int0 : !torch.vtensor<[1,512,28,28],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,512,28,28],!torch.qint8>
    %511 = torch.aten.dequantize.self %510 : !torch.vtensor<[1,512,28,28],!torch.qint8> -> !torch.vtensor<[1,512,28,28],f32>
    %512 = torch.aten.add.Tensor %511, %459, %int1 : !torch.vtensor<[1,512,28,28],f32>, !torch.vtensor<[1,512,28,28],f32>, !torch.int -> !torch.vtensor<[1,512,28,28],f32>
    %513 = torch.aten.relu %512 : !torch.vtensor<[1,512,28,28],f32> -> !torch.vtensor<[1,512,28,28],f32>
    %514 = torch.aten.quantize_per_tensor %513, %float1.562500e-02, %int0, %int12 : !torch.vtensor<[1,512,28,28],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,512,28,28],!torch.qint8>
    %515 = torch.aten.int_repr %514 : !torch.vtensor<[1,512,28,28],!torch.qint8> -> !torch.vtensor<[1,512,28,28],si8>
    %516 = torch.aten._make_per_tensor_quantized_tensor %515, %float1.562500e-02, %int0 : !torch.vtensor<[1,512,28,28],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,512,28,28],!torch.qint8>
    %517 = torch.aten.dequantize.self %516 : !torch.vtensor<[1,512,28,28],!torch.qint8> -> !torch.vtensor<[1,512,28,28],f32>
    %518 = torch.aten.quantize_per_tensor %43, %float1.953130e-03, %int0, %int12 : !torch.vtensor<[128,512,1,1],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[128,512,1,1],!torch.qint8>
    %519 = torch.aten.int_repr %518 : !torch.vtensor<[128,512,1,1],!torch.qint8> -> !torch.vtensor<[128,512,1,1],si8>
    %520 = torch.aten._make_per_tensor_quantized_tensor %519, %float1.953130e-03, %int0 : !torch.vtensor<[128,512,1,1],si8>, !torch.float, !torch.int -> !torch.vtensor<[128,512,1,1],!torch.qint8>
    %521 = torch.aten.quantize_per_tensor %44, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[128],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[128],!torch.qint8>
    %522 = torch.aten.int_repr %521 : !torch.vtensor<[128],!torch.qint8> -> !torch.vtensor<[128],si8>
    %523 = torch.aten._make_per_tensor_quantized_tensor %522, %float3.906250e-03, %int0 : !torch.vtensor<[128],si8>, !torch.float, !torch.int -> !torch.vtensor<[128],!torch.qint8>
    %524 = torch.aten.dequantize.self %523 : !torch.vtensor<[128],!torch.qint8> -> !torch.vtensor<[128],f32>
    %525 = torch.aten.quantize_per_tensor %524, %float3.051760e-05, %int0, %int14 : !torch.vtensor<[128],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[128],!torch.qint32>
    %526 = torch.aten.int_repr %525 : !torch.vtensor<[128],!torch.qint32> -> !torch.vtensor<[128],si32>
    %527 = torch.aten.convolution %516, %520, %526, %120, %122, %120, %false, %122, %int1 : !torch.vtensor<[1,512,28,28],!torch.qint8>, !torch.vtensor<[128,512,1,1],!torch.qint8>, !torch.vtensor<[128],si32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,128,28,28],si32>
    %528 = torch.aten._make_per_tensor_quantized_tensor %527, %float3.051760e-05, %int0 : !torch.vtensor<[1,128,28,28],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,128,28,28],!torch.qint32>
    %529 = torch.aten.relu %528 : !torch.vtensor<[1,128,28,28],!torch.qint32> -> !torch.vtensor<[1,128,28,28],!torch.qint32>
    %530 = torch.aten.int_repr %529 : !torch.vtensor<[1,128,28,28],!torch.qint32> -> !torch.vtensor<[1,128,28,28],si32>
    %531 = torch.aten._make_per_tensor_quantized_tensor %530, %float3.051760e-05, %int0 : !torch.vtensor<[1,128,28,28],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,128,28,28],!torch.qint32>
    %532 = torch.aten.dequantize.tensor %531 : !torch.vtensor<[1,128,28,28],!torch.qint32> -> !torch.vtensor<[1,128,28,28],f32>
    %533 = torch.aten.quantize_per_tensor %532, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[1,128,28,28],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,128,28,28],!torch.qint8>
    %534 = torch.aten.int_repr %533 : !torch.vtensor<[1,128,28,28],!torch.qint8> -> !torch.vtensor<[1,128,28,28],si8>
    %535 = torch.aten._make_per_tensor_quantized_tensor %534, %float3.906250e-03, %int0 : !torch.vtensor<[1,128,28,28],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,128,28,28],!torch.qint8>
    %536 = torch.aten.quantize_per_tensor %45, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[128,128,3,3],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[128,128,3,3],!torch.qint8>
    %537 = torch.aten.int_repr %536 : !torch.vtensor<[128,128,3,3],!torch.qint8> -> !torch.vtensor<[128,128,3,3],si8>
    %538 = torch.aten._make_per_tensor_quantized_tensor %537, %float3.906250e-03, %int0 : !torch.vtensor<[128,128,3,3],si8>, !torch.float, !torch.int -> !torch.vtensor<[128,128,3,3],!torch.qint8>
    %539 = torch.aten.quantize_per_tensor %46, %float7.812500e-03, %int0, %int12 : !torch.vtensor<[128],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[128],!torch.qint8>
    %540 = torch.aten.int_repr %539 : !torch.vtensor<[128],!torch.qint8> -> !torch.vtensor<[128],si8>
    %541 = torch.aten._make_per_tensor_quantized_tensor %540, %float7.812500e-03, %int0 : !torch.vtensor<[128],si8>, !torch.float, !torch.int -> !torch.vtensor<[128],!torch.qint8>
    %542 = torch.aten.dequantize.self %541 : !torch.vtensor<[128],!torch.qint8> -> !torch.vtensor<[128],f32>
    %543 = torch.aten.quantize_per_tensor %542, %float1.525880e-05, %int0, %int14 : !torch.vtensor<[128],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[128],!torch.qint32>
    %544 = torch.aten.int_repr %543 : !torch.vtensor<[128],!torch.qint32> -> !torch.vtensor<[128],si32>
    %545 = torch.aten.convolution %535, %538, %544, %120, %120, %120, %false, %122, %int1 : !torch.vtensor<[1,128,28,28],!torch.qint8>, !torch.vtensor<[128,128,3,3],!torch.qint8>, !torch.vtensor<[128],si32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,128,28,28],si32>
    %546 = torch.aten._make_per_tensor_quantized_tensor %545, %float1.525880e-05, %int0 : !torch.vtensor<[1,128,28,28],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,128,28,28],!torch.qint32>
    %547 = torch.aten.relu %546 : !torch.vtensor<[1,128,28,28],!torch.qint32> -> !torch.vtensor<[1,128,28,28],!torch.qint32>
    %548 = torch.aten.int_repr %547 : !torch.vtensor<[1,128,28,28],!torch.qint32> -> !torch.vtensor<[1,128,28,28],si32>
    %549 = torch.aten._make_per_tensor_quantized_tensor %548, %float1.525880e-05, %int0 : !torch.vtensor<[1,128,28,28],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,128,28,28],!torch.qint32>
    %550 = torch.aten.dequantize.tensor %549 : !torch.vtensor<[1,128,28,28],!torch.qint32> -> !torch.vtensor<[1,128,28,28],f32>
    %551 = torch.aten.quantize_per_tensor %550, %float1.562500e-02, %int0, %int12 : !torch.vtensor<[1,128,28,28],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,128,28,28],!torch.qint8>
    %552 = torch.aten.int_repr %551 : !torch.vtensor<[1,128,28,28],!torch.qint8> -> !torch.vtensor<[1,128,28,28],si8>
    %553 = torch.aten._make_per_tensor_quantized_tensor %552, %float1.562500e-02, %int0 : !torch.vtensor<[1,128,28,28],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,128,28,28],!torch.qint8>
    %554 = torch.aten.quantize_per_tensor %47, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[512,128,1,1],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[512,128,1,1],!torch.qint8>
    %555 = torch.aten.int_repr %554 : !torch.vtensor<[512,128,1,1],!torch.qint8> -> !torch.vtensor<[512,128,1,1],si8>
    %556 = torch.aten._make_per_tensor_quantized_tensor %555, %float3.906250e-03, %int0 : !torch.vtensor<[512,128,1,1],si8>, !torch.float, !torch.int -> !torch.vtensor<[512,128,1,1],!torch.qint8>
    %557 = torch.aten.quantize_per_tensor %48, %float7.812500e-03, %int0, %int12 : !torch.vtensor<[512],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[512],!torch.qint8>
    %558 = torch.aten.int_repr %557 : !torch.vtensor<[512],!torch.qint8> -> !torch.vtensor<[512],si8>
    %559 = torch.aten._make_per_tensor_quantized_tensor %558, %float7.812500e-03, %int0 : !torch.vtensor<[512],si8>, !torch.float, !torch.int -> !torch.vtensor<[512],!torch.qint8>
    %560 = torch.aten.dequantize.self %559 : !torch.vtensor<[512],!torch.qint8> -> !torch.vtensor<[512],f32>
    %561 = torch.aten.quantize_per_tensor %560, %float6.103520e-05, %int0, %int14 : !torch.vtensor<[512],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[512],!torch.qint32>
    %562 = torch.aten.int_repr %561 : !torch.vtensor<[512],!torch.qint32> -> !torch.vtensor<[512],si32>
    %563 = torch.aten.convolution %553, %556, %562, %120, %122, %120, %false, %122, %int1 : !torch.vtensor<[1,128,28,28],!torch.qint8>, !torch.vtensor<[512,128,1,1],!torch.qint8>, !torch.vtensor<[512],si32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,512,28,28],si32>
    %564 = torch.aten._make_per_tensor_quantized_tensor %563, %float6.103520e-05, %int0 : !torch.vtensor<[1,512,28,28],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,512,28,28],!torch.qint32>
    %565 = torch.aten.dequantize.tensor %564 : !torch.vtensor<[1,512,28,28],!torch.qint32> -> !torch.vtensor<[1,512,28,28],f32>
    %566 = torch.aten.quantize_per_tensor %565, %float7.812500e-03, %int0, %int12 : !torch.vtensor<[1,512,28,28],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,512,28,28],!torch.qint8>
    %567 = torch.aten.int_repr %566 : !torch.vtensor<[1,512,28,28],!torch.qint8> -> !torch.vtensor<[1,512,28,28],si8>
    %568 = torch.aten._make_per_tensor_quantized_tensor %567, %float7.812500e-03, %int0 : !torch.vtensor<[1,512,28,28],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,512,28,28],!torch.qint8>
    %569 = torch.aten.dequantize.self %568 : !torch.vtensor<[1,512,28,28],!torch.qint8> -> !torch.vtensor<[1,512,28,28],f32>
    %570 = torch.aten.add.Tensor %569, %517, %int1 : !torch.vtensor<[1,512,28,28],f32>, !torch.vtensor<[1,512,28,28],f32>, !torch.int -> !torch.vtensor<[1,512,28,28],f32>
    %571 = torch.aten.relu %570 : !torch.vtensor<[1,512,28,28],f32> -> !torch.vtensor<[1,512,28,28],f32>
    %572 = torch.aten.quantize_per_tensor %571, %float1.562500e-02, %int0, %int12 : !torch.vtensor<[1,512,28,28],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,512,28,28],!torch.qint8>
    %573 = torch.aten.int_repr %572 : !torch.vtensor<[1,512,28,28],!torch.qint8> -> !torch.vtensor<[1,512,28,28],si8>
    %574 = torch.aten._make_per_tensor_quantized_tensor %573, %float1.562500e-02, %int0 : !torch.vtensor<[1,512,28,28],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,512,28,28],!torch.qint8>
    %575 = torch.aten.quantize_per_tensor %49, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[256,512,1,1],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[256,512,1,1],!torch.qint8>
    %576 = torch.aten.int_repr %575 : !torch.vtensor<[256,512,1,1],!torch.qint8> -> !torch.vtensor<[256,512,1,1],si8>
    %577 = torch.aten._make_per_tensor_quantized_tensor %576, %float3.906250e-03, %int0 : !torch.vtensor<[256,512,1,1],si8>, !torch.float, !torch.int -> !torch.vtensor<[256,512,1,1],!torch.qint8>
    %578 = torch.aten.quantize_per_tensor %50, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[256],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[256],!torch.qint8>
    %579 = torch.aten.int_repr %578 : !torch.vtensor<[256],!torch.qint8> -> !torch.vtensor<[256],si8>
    %580 = torch.aten._make_per_tensor_quantized_tensor %579, %float3.906250e-03, %int0 : !torch.vtensor<[256],si8>, !torch.float, !torch.int -> !torch.vtensor<[256],!torch.qint8>
    %581 = torch.aten.dequantize.self %580 : !torch.vtensor<[256],!torch.qint8> -> !torch.vtensor<[256],f32>
    %582 = torch.aten.quantize_per_tensor %581, %float6.103520e-05, %int0, %int14 : !torch.vtensor<[256],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[256],!torch.qint32>
    %583 = torch.aten.int_repr %582 : !torch.vtensor<[256],!torch.qint32> -> !torch.vtensor<[256],si32>
    %584 = torch.aten.convolution %574, %577, %583, %120, %122, %120, %false, %122, %int1 : !torch.vtensor<[1,512,28,28],!torch.qint8>, !torch.vtensor<[256,512,1,1],!torch.qint8>, !torch.vtensor<[256],si32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,256,28,28],si32>
    %585 = torch.aten._make_per_tensor_quantized_tensor %584, %float6.103520e-05, %int0 : !torch.vtensor<[1,256,28,28],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,256,28,28],!torch.qint32>
    %586 = torch.aten.relu %585 : !torch.vtensor<[1,256,28,28],!torch.qint32> -> !torch.vtensor<[1,256,28,28],!torch.qint32>
    %587 = torch.aten.int_repr %586 : !torch.vtensor<[1,256,28,28],!torch.qint32> -> !torch.vtensor<[1,256,28,28],si32>
    %588 = torch.aten._make_per_tensor_quantized_tensor %587, %float6.103520e-05, %int0 : !torch.vtensor<[1,256,28,28],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,256,28,28],!torch.qint32>
    %589 = torch.aten.dequantize.tensor %588 : !torch.vtensor<[1,256,28,28],!torch.qint32> -> !torch.vtensor<[1,256,28,28],f32>
    %590 = torch.aten.quantize_per_tensor %589, %float1.562500e-02, %int0, %int12 : !torch.vtensor<[1,256,28,28],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,256,28,28],!torch.qint8>
    %591 = torch.aten.int_repr %590 : !torch.vtensor<[1,256,28,28],!torch.qint8> -> !torch.vtensor<[1,256,28,28],si8>
    %592 = torch.aten._make_per_tensor_quantized_tensor %591, %float1.562500e-02, %int0 : !torch.vtensor<[1,256,28,28],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,256,28,28],!torch.qint8>
    %593 = torch.aten.quantize_per_tensor %51, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[256,256,3,3],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[256,256,3,3],!torch.qint8>
    %594 = torch.aten.int_repr %593 : !torch.vtensor<[256,256,3,3],!torch.qint8> -> !torch.vtensor<[256,256,3,3],si8>
    %595 = torch.aten._make_per_tensor_quantized_tensor %594, %float3.906250e-03, %int0 : !torch.vtensor<[256,256,3,3],si8>, !torch.float, !torch.int -> !torch.vtensor<[256,256,3,3],!torch.qint8>
    %596 = torch.aten.quantize_per_tensor %52, %float7.812500e-03, %int0, %int12 : !torch.vtensor<[256],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[256],!torch.qint8>
    %597 = torch.aten.int_repr %596 : !torch.vtensor<[256],!torch.qint8> -> !torch.vtensor<[256],si8>
    %598 = torch.aten._make_per_tensor_quantized_tensor %597, %float7.812500e-03, %int0 : !torch.vtensor<[256],si8>, !torch.float, !torch.int -> !torch.vtensor<[256],!torch.qint8>
    %599 = torch.aten.dequantize.self %598 : !torch.vtensor<[256],!torch.qint8> -> !torch.vtensor<[256],f32>
    %600 = torch.aten.quantize_per_tensor %599, %float6.103520e-05, %int0, %int14 : !torch.vtensor<[256],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[256],!torch.qint32>
    %601 = torch.aten.int_repr %600 : !torch.vtensor<[256],!torch.qint32> -> !torch.vtensor<[256],si32>
    %602 = torch.aten.convolution %592, %595, %601, %121, %120, %120, %false, %122, %int1 : !torch.vtensor<[1,256,28,28],!torch.qint8>, !torch.vtensor<[256,256,3,3],!torch.qint8>, !torch.vtensor<[256],si32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,256,14,14],si32>
    %603 = torch.aten._make_per_tensor_quantized_tensor %602, %float6.103520e-05, %int0 : !torch.vtensor<[1,256,14,14],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,256,14,14],!torch.qint32>
    %604 = torch.aten.relu %603 : !torch.vtensor<[1,256,14,14],!torch.qint32> -> !torch.vtensor<[1,256,14,14],!torch.qint32>
    %605 = torch.aten.int_repr %604 : !torch.vtensor<[1,256,14,14],!torch.qint32> -> !torch.vtensor<[1,256,14,14],si32>
    %606 = torch.aten._make_per_tensor_quantized_tensor %605, %float6.103520e-05, %int0 : !torch.vtensor<[1,256,14,14],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,256,14,14],!torch.qint32>
    %607 = torch.aten.dequantize.tensor %606 : !torch.vtensor<[1,256,14,14],!torch.qint32> -> !torch.vtensor<[1,256,14,14],f32>
    %608 = torch.aten.quantize_per_tensor %607, %float1.562500e-02, %int0, %int12 : !torch.vtensor<[1,256,14,14],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,256,14,14],!torch.qint8>
    %609 = torch.aten.int_repr %608 : !torch.vtensor<[1,256,14,14],!torch.qint8> -> !torch.vtensor<[1,256,14,14],si8>
    %610 = torch.aten._make_per_tensor_quantized_tensor %609, %float1.562500e-02, %int0 : !torch.vtensor<[1,256,14,14],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,256,14,14],!torch.qint8>
    %611 = torch.aten.quantize_per_tensor %53, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[1024,256,1,1],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1024,256,1,1],!torch.qint8>
    %612 = torch.aten.int_repr %611 : !torch.vtensor<[1024,256,1,1],!torch.qint8> -> !torch.vtensor<[1024,256,1,1],si8>
    %613 = torch.aten._make_per_tensor_quantized_tensor %612, %float3.906250e-03, %int0 : !torch.vtensor<[1024,256,1,1],si8>, !torch.float, !torch.int -> !torch.vtensor<[1024,256,1,1],!torch.qint8>
    %614 = torch.aten.quantize_per_tensor %54, %float7.812500e-03, %int0, %int12 : !torch.vtensor<[1024],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1024],!torch.qint8>
    %615 = torch.aten.int_repr %614 : !torch.vtensor<[1024],!torch.qint8> -> !torch.vtensor<[1024],si8>
    %616 = torch.aten._make_per_tensor_quantized_tensor %615, %float7.812500e-03, %int0 : !torch.vtensor<[1024],si8>, !torch.float, !torch.int -> !torch.vtensor<[1024],!torch.qint8>
    %617 = torch.aten.dequantize.self %616 : !torch.vtensor<[1024],!torch.qint8> -> !torch.vtensor<[1024],f32>
    %618 = torch.aten.quantize_per_tensor %617, %float6.103520e-05, %int0, %int14 : !torch.vtensor<[1024],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1024],!torch.qint32>
    %619 = torch.aten.int_repr %618 : !torch.vtensor<[1024],!torch.qint32> -> !torch.vtensor<[1024],si32>
    %620 = torch.aten.convolution %610, %613, %619, %120, %122, %120, %false, %122, %int1 : !torch.vtensor<[1,256,14,14],!torch.qint8>, !torch.vtensor<[1024,256,1,1],!torch.qint8>, !torch.vtensor<[1024],si32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,1024,14,14],si32>
    %621 = torch.aten._make_per_tensor_quantized_tensor %620, %float6.103520e-05, %int0 : !torch.vtensor<[1,1024,14,14],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,1024,14,14],!torch.qint32>
    %622 = torch.aten.dequantize.tensor %621 : !torch.vtensor<[1,1024,14,14],!torch.qint32> -> !torch.vtensor<[1,1024,14,14],f32>
    %623 = torch.aten.quantize_per_tensor %622, %float7.812500e-03, %int0, %int12 : !torch.vtensor<[1,1024,14,14],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,1024,14,14],!torch.qint8>
    %624 = torch.aten.int_repr %623 : !torch.vtensor<[1,1024,14,14],!torch.qint8> -> !torch.vtensor<[1,1024,14,14],si8>
    %625 = torch.aten._make_per_tensor_quantized_tensor %624, %float7.812500e-03, %int0 : !torch.vtensor<[1,1024,14,14],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,1024,14,14],!torch.qint8>
    %626 = torch.aten.dequantize.self %625 : !torch.vtensor<[1,1024,14,14],!torch.qint8> -> !torch.vtensor<[1,1024,14,14],f32>
    %627 = torch.aten.quantize_per_tensor %55, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[1024,512,1,1],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1024,512,1,1],!torch.qint8>
    %628 = torch.aten.int_repr %627 : !torch.vtensor<[1024,512,1,1],!torch.qint8> -> !torch.vtensor<[1024,512,1,1],si8>
    %629 = torch.aten._make_per_tensor_quantized_tensor %628, %float3.906250e-03, %int0 : !torch.vtensor<[1024,512,1,1],si8>, !torch.float, !torch.int -> !torch.vtensor<[1024,512,1,1],!torch.qint8>
    %630 = torch.aten.quantize_per_tensor %56, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[1024],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1024],!torch.qint8>
    %631 = torch.aten.int_repr %630 : !torch.vtensor<[1024],!torch.qint8> -> !torch.vtensor<[1024],si8>
    %632 = torch.aten._make_per_tensor_quantized_tensor %631, %float3.906250e-03, %int0 : !torch.vtensor<[1024],si8>, !torch.float, !torch.int -> !torch.vtensor<[1024],!torch.qint8>
    %633 = torch.aten.dequantize.self %632 : !torch.vtensor<[1024],!torch.qint8> -> !torch.vtensor<[1024],f32>
    %634 = torch.aten.quantize_per_tensor %633, %float6.103520e-05, %int0, %int14 : !torch.vtensor<[1024],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1024],!torch.qint32>
    %635 = torch.aten.int_repr %634 : !torch.vtensor<[1024],!torch.qint32> -> !torch.vtensor<[1024],si32>
    %636 = torch.aten.convolution %574, %629, %635, %121, %122, %120, %false, %122, %int1 : !torch.vtensor<[1,512,28,28],!torch.qint8>, !torch.vtensor<[1024,512,1,1],!torch.qint8>, !torch.vtensor<[1024],si32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,1024,14,14],si32>
    %637 = torch.aten._make_per_tensor_quantized_tensor %636, %float6.103520e-05, %int0 : !torch.vtensor<[1,1024,14,14],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,1024,14,14],!torch.qint32>
    %638 = torch.aten.dequantize.tensor %637 : !torch.vtensor<[1,1024,14,14],!torch.qint32> -> !torch.vtensor<[1,1024,14,14],f32>
    %639 = torch.aten.quantize_per_tensor %638, %float7.812500e-03, %int0, %int12 : !torch.vtensor<[1,1024,14,14],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,1024,14,14],!torch.qint8>
    %640 = torch.aten.int_repr %639 : !torch.vtensor<[1,1024,14,14],!torch.qint8> -> !torch.vtensor<[1,1024,14,14],si8>
    %641 = torch.aten._make_per_tensor_quantized_tensor %640, %float7.812500e-03, %int0 : !torch.vtensor<[1,1024,14,14],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,1024,14,14],!torch.qint8>
    %642 = torch.aten.dequantize.self %641 : !torch.vtensor<[1,1024,14,14],!torch.qint8> -> !torch.vtensor<[1,1024,14,14],f32>
    %643 = torch.aten.add.Tensor %626, %642, %int1 : !torch.vtensor<[1,1024,14,14],f32>, !torch.vtensor<[1,1024,14,14],f32>, !torch.int -> !torch.vtensor<[1,1024,14,14],f32>
    %644 = torch.aten.relu %643 : !torch.vtensor<[1,1024,14,14],f32> -> !torch.vtensor<[1,1024,14,14],f32>
    %645 = torch.aten.quantize_per_tensor %644, %float1.562500e-02, %int0, %int12 : !torch.vtensor<[1,1024,14,14],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,1024,14,14],!torch.qint8>
    %646 = torch.aten.int_repr %645 : !torch.vtensor<[1,1024,14,14],!torch.qint8> -> !torch.vtensor<[1,1024,14,14],si8>
    %647 = torch.aten._make_per_tensor_quantized_tensor %646, %float1.562500e-02, %int0 : !torch.vtensor<[1,1024,14,14],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,1024,14,14],!torch.qint8>
    %648 = torch.aten.dequantize.self %647 : !torch.vtensor<[1,1024,14,14],!torch.qint8> -> !torch.vtensor<[1,1024,14,14],f32>
    %649 = torch.aten.quantize_per_tensor %57, %float1.953130e-03, %int0, %int12 : !torch.vtensor<[256,1024,1,1],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[256,1024,1,1],!torch.qint8>
    %650 = torch.aten.int_repr %649 : !torch.vtensor<[256,1024,1,1],!torch.qint8> -> !torch.vtensor<[256,1024,1,1],si8>
    %651 = torch.aten._make_per_tensor_quantized_tensor %650, %float1.953130e-03, %int0 : !torch.vtensor<[256,1024,1,1],si8>, !torch.float, !torch.int -> !torch.vtensor<[256,1024,1,1],!torch.qint8>
    %652 = torch.aten.quantize_per_tensor %58, %float7.812500e-03, %int0, %int12 : !torch.vtensor<[256],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[256],!torch.qint8>
    %653 = torch.aten.int_repr %652 : !torch.vtensor<[256],!torch.qint8> -> !torch.vtensor<[256],si8>
    %654 = torch.aten._make_per_tensor_quantized_tensor %653, %float7.812500e-03, %int0 : !torch.vtensor<[256],si8>, !torch.float, !torch.int -> !torch.vtensor<[256],!torch.qint8>
    %655 = torch.aten.dequantize.self %654 : !torch.vtensor<[256],!torch.qint8> -> !torch.vtensor<[256],f32>
    %656 = torch.aten.quantize_per_tensor %655, %float3.051760e-05, %int0, %int14 : !torch.vtensor<[256],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[256],!torch.qint32>
    %657 = torch.aten.int_repr %656 : !torch.vtensor<[256],!torch.qint32> -> !torch.vtensor<[256],si32>
    %658 = torch.aten.convolution %647, %651, %657, %120, %122, %120, %false, %122, %int1 : !torch.vtensor<[1,1024,14,14],!torch.qint8>, !torch.vtensor<[256,1024,1,1],!torch.qint8>, !torch.vtensor<[256],si32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,256,14,14],si32>
    %659 = torch.aten._make_per_tensor_quantized_tensor %658, %float3.051760e-05, %int0 : !torch.vtensor<[1,256,14,14],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,256,14,14],!torch.qint32>
    %660 = torch.aten.relu %659 : !torch.vtensor<[1,256,14,14],!torch.qint32> -> !torch.vtensor<[1,256,14,14],!torch.qint32>
    %661 = torch.aten.int_repr %660 : !torch.vtensor<[1,256,14,14],!torch.qint32> -> !torch.vtensor<[1,256,14,14],si32>
    %662 = torch.aten._make_per_tensor_quantized_tensor %661, %float3.051760e-05, %int0 : !torch.vtensor<[1,256,14,14],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,256,14,14],!torch.qint32>
    %663 = torch.aten.dequantize.tensor %662 : !torch.vtensor<[1,256,14,14],!torch.qint32> -> !torch.vtensor<[1,256,14,14],f32>
    %664 = torch.aten.quantize_per_tensor %663, %float7.812500e-03, %int0, %int12 : !torch.vtensor<[1,256,14,14],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,256,14,14],!torch.qint8>
    %665 = torch.aten.int_repr %664 : !torch.vtensor<[1,256,14,14],!torch.qint8> -> !torch.vtensor<[1,256,14,14],si8>
    %666 = torch.aten._make_per_tensor_quantized_tensor %665, %float7.812500e-03, %int0 : !torch.vtensor<[1,256,14,14],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,256,14,14],!torch.qint8>
    %667 = torch.aten.quantize_per_tensor %59, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[256,256,3,3],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[256,256,3,3],!torch.qint8>
    %668 = torch.aten.int_repr %667 : !torch.vtensor<[256,256,3,3],!torch.qint8> -> !torch.vtensor<[256,256,3,3],si8>
    %669 = torch.aten._make_per_tensor_quantized_tensor %668, %float3.906250e-03, %int0 : !torch.vtensor<[256,256,3,3],si8>, !torch.float, !torch.int -> !torch.vtensor<[256,256,3,3],!torch.qint8>
    %670 = torch.aten.quantize_per_tensor %60, %float1.562500e-02, %int0, %int12 : !torch.vtensor<[256],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[256],!torch.qint8>
    %671 = torch.aten.int_repr %670 : !torch.vtensor<[256],!torch.qint8> -> !torch.vtensor<[256],si8>
    %672 = torch.aten._make_per_tensor_quantized_tensor %671, %float1.562500e-02, %int0 : !torch.vtensor<[256],si8>, !torch.float, !torch.int -> !torch.vtensor<[256],!torch.qint8>
    %673 = torch.aten.dequantize.self %672 : !torch.vtensor<[256],!torch.qint8> -> !torch.vtensor<[256],f32>
    %674 = torch.aten.quantize_per_tensor %673, %float3.051760e-05, %int0, %int14 : !torch.vtensor<[256],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[256],!torch.qint32>
    %675 = torch.aten.int_repr %674 : !torch.vtensor<[256],!torch.qint32> -> !torch.vtensor<[256],si32>
    %676 = torch.aten.convolution %666, %669, %675, %120, %120, %120, %false, %122, %int1 : !torch.vtensor<[1,256,14,14],!torch.qint8>, !torch.vtensor<[256,256,3,3],!torch.qint8>, !torch.vtensor<[256],si32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,256,14,14],si32>
    %677 = torch.aten._make_per_tensor_quantized_tensor %676, %float3.051760e-05, %int0 : !torch.vtensor<[1,256,14,14],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,256,14,14],!torch.qint32>
    %678 = torch.aten.relu %677 : !torch.vtensor<[1,256,14,14],!torch.qint32> -> !torch.vtensor<[1,256,14,14],!torch.qint32>
    %679 = torch.aten.int_repr %678 : !torch.vtensor<[1,256,14,14],!torch.qint32> -> !torch.vtensor<[1,256,14,14],si32>
    %680 = torch.aten._make_per_tensor_quantized_tensor %679, %float3.051760e-05, %int0 : !torch.vtensor<[1,256,14,14],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,256,14,14],!torch.qint32>
    %681 = torch.aten.dequantize.tensor %680 : !torch.vtensor<[1,256,14,14],!torch.qint32> -> !torch.vtensor<[1,256,14,14],f32>
    %682 = torch.aten.quantize_per_tensor %681, %float1.562500e-02, %int0, %int12 : !torch.vtensor<[1,256,14,14],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,256,14,14],!torch.qint8>
    %683 = torch.aten.int_repr %682 : !torch.vtensor<[1,256,14,14],!torch.qint8> -> !torch.vtensor<[1,256,14,14],si8>
    %684 = torch.aten._make_per_tensor_quantized_tensor %683, %float1.562500e-02, %int0 : !torch.vtensor<[1,256,14,14],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,256,14,14],!torch.qint8>
    %685 = torch.aten.quantize_per_tensor %61, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[1024,256,1,1],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1024,256,1,1],!torch.qint8>
    %686 = torch.aten.int_repr %685 : !torch.vtensor<[1024,256,1,1],!torch.qint8> -> !torch.vtensor<[1024,256,1,1],si8>
    %687 = torch.aten._make_per_tensor_quantized_tensor %686, %float3.906250e-03, %int0 : !torch.vtensor<[1024,256,1,1],si8>, !torch.float, !torch.int -> !torch.vtensor<[1024,256,1,1],!torch.qint8>
    %688 = torch.aten.quantize_per_tensor %62, %float7.812500e-03, %int0, %int12 : !torch.vtensor<[1024],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1024],!torch.qint8>
    %689 = torch.aten.int_repr %688 : !torch.vtensor<[1024],!torch.qint8> -> !torch.vtensor<[1024],si8>
    %690 = torch.aten._make_per_tensor_quantized_tensor %689, %float7.812500e-03, %int0 : !torch.vtensor<[1024],si8>, !torch.float, !torch.int -> !torch.vtensor<[1024],!torch.qint8>
    %691 = torch.aten.dequantize.self %690 : !torch.vtensor<[1024],!torch.qint8> -> !torch.vtensor<[1024],f32>
    %692 = torch.aten.quantize_per_tensor %691, %float6.103520e-05, %int0, %int14 : !torch.vtensor<[1024],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1024],!torch.qint32>
    %693 = torch.aten.int_repr %692 : !torch.vtensor<[1024],!torch.qint32> -> !torch.vtensor<[1024],si32>
    %694 = torch.aten.convolution %684, %687, %693, %120, %122, %120, %false, %122, %int1 : !torch.vtensor<[1,256,14,14],!torch.qint8>, !torch.vtensor<[1024,256,1,1],!torch.qint8>, !torch.vtensor<[1024],si32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,1024,14,14],si32>
    %695 = torch.aten._make_per_tensor_quantized_tensor %694, %float6.103520e-05, %int0 : !torch.vtensor<[1,1024,14,14],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,1024,14,14],!torch.qint32>
    %696 = torch.aten.dequantize.tensor %695 : !torch.vtensor<[1,1024,14,14],!torch.qint32> -> !torch.vtensor<[1,1024,14,14],f32>
    %697 = torch.aten.quantize_per_tensor %696, %float7.812500e-03, %int0, %int12 : !torch.vtensor<[1,1024,14,14],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,1024,14,14],!torch.qint8>
    %698 = torch.aten.int_repr %697 : !torch.vtensor<[1,1024,14,14],!torch.qint8> -> !torch.vtensor<[1,1024,14,14],si8>
    %699 = torch.aten._make_per_tensor_quantized_tensor %698, %float7.812500e-03, %int0 : !torch.vtensor<[1,1024,14,14],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,1024,14,14],!torch.qint8>
    %700 = torch.aten.dequantize.self %699 : !torch.vtensor<[1,1024,14,14],!torch.qint8> -> !torch.vtensor<[1,1024,14,14],f32>
    %701 = torch.aten.add.Tensor %700, %648, %int1 : !torch.vtensor<[1,1024,14,14],f32>, !torch.vtensor<[1,1024,14,14],f32>, !torch.int -> !torch.vtensor<[1,1024,14,14],f32>
    %702 = torch.aten.relu %701 : !torch.vtensor<[1,1024,14,14],f32> -> !torch.vtensor<[1,1024,14,14],f32>
    %703 = torch.aten.quantize_per_tensor %702, %float1.562500e-02, %int0, %int12 : !torch.vtensor<[1,1024,14,14],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,1024,14,14],!torch.qint8>
    %704 = torch.aten.int_repr %703 : !torch.vtensor<[1,1024,14,14],!torch.qint8> -> !torch.vtensor<[1,1024,14,14],si8>
    %705 = torch.aten._make_per_tensor_quantized_tensor %704, %float1.562500e-02, %int0 : !torch.vtensor<[1,1024,14,14],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,1024,14,14],!torch.qint8>
    %706 = torch.aten.dequantize.self %705 : !torch.vtensor<[1,1024,14,14],!torch.qint8> -> !torch.vtensor<[1,1024,14,14],f32>
    %707 = torch.aten.quantize_per_tensor %63, %float1.953130e-03, %int0, %int12 : !torch.vtensor<[256,1024,1,1],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[256,1024,1,1],!torch.qint8>
    %708 = torch.aten.int_repr %707 : !torch.vtensor<[256,1024,1,1],!torch.qint8> -> !torch.vtensor<[256,1024,1,1],si8>
    %709 = torch.aten._make_per_tensor_quantized_tensor %708, %float1.953130e-03, %int0 : !torch.vtensor<[256,1024,1,1],si8>, !torch.float, !torch.int -> !torch.vtensor<[256,1024,1,1],!torch.qint8>
    %710 = torch.aten.quantize_per_tensor %64, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[256],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[256],!torch.qint8>
    %711 = torch.aten.int_repr %710 : !torch.vtensor<[256],!torch.qint8> -> !torch.vtensor<[256],si8>
    %712 = torch.aten._make_per_tensor_quantized_tensor %711, %float3.906250e-03, %int0 : !torch.vtensor<[256],si8>, !torch.float, !torch.int -> !torch.vtensor<[256],!torch.qint8>
    %713 = torch.aten.dequantize.self %712 : !torch.vtensor<[256],!torch.qint8> -> !torch.vtensor<[256],f32>
    %714 = torch.aten.quantize_per_tensor %713, %float3.051760e-05, %int0, %int14 : !torch.vtensor<[256],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[256],!torch.qint32>
    %715 = torch.aten.int_repr %714 : !torch.vtensor<[256],!torch.qint32> -> !torch.vtensor<[256],si32>
    %716 = torch.aten.convolution %705, %709, %715, %120, %122, %120, %false, %122, %int1 : !torch.vtensor<[1,1024,14,14],!torch.qint8>, !torch.vtensor<[256,1024,1,1],!torch.qint8>, !torch.vtensor<[256],si32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,256,14,14],si32>
    %717 = torch.aten._make_per_tensor_quantized_tensor %716, %float3.051760e-05, %int0 : !torch.vtensor<[1,256,14,14],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,256,14,14],!torch.qint32>
    %718 = torch.aten.relu %717 : !torch.vtensor<[1,256,14,14],!torch.qint32> -> !torch.vtensor<[1,256,14,14],!torch.qint32>
    %719 = torch.aten.int_repr %718 : !torch.vtensor<[1,256,14,14],!torch.qint32> -> !torch.vtensor<[1,256,14,14],si32>
    %720 = torch.aten._make_per_tensor_quantized_tensor %719, %float3.051760e-05, %int0 : !torch.vtensor<[1,256,14,14],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,256,14,14],!torch.qint32>
    %721 = torch.aten.dequantize.tensor %720 : !torch.vtensor<[1,256,14,14],!torch.qint32> -> !torch.vtensor<[1,256,14,14],f32>
    %722 = torch.aten.quantize_per_tensor %721, %float7.812500e-03, %int0, %int12 : !torch.vtensor<[1,256,14,14],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,256,14,14],!torch.qint8>
    %723 = torch.aten.int_repr %722 : !torch.vtensor<[1,256,14,14],!torch.qint8> -> !torch.vtensor<[1,256,14,14],si8>
    %724 = torch.aten._make_per_tensor_quantized_tensor %723, %float7.812500e-03, %int0 : !torch.vtensor<[1,256,14,14],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,256,14,14],!torch.qint8>
    %725 = torch.aten.quantize_per_tensor %65, %float1.953130e-03, %int0, %int12 : !torch.vtensor<[256,256,3,3],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[256,256,3,3],!torch.qint8>
    %726 = torch.aten.int_repr %725 : !torch.vtensor<[256,256,3,3],!torch.qint8> -> !torch.vtensor<[256,256,3,3],si8>
    %727 = torch.aten._make_per_tensor_quantized_tensor %726, %float1.953130e-03, %int0 : !torch.vtensor<[256,256,3,3],si8>, !torch.float, !torch.int -> !torch.vtensor<[256,256,3,3],!torch.qint8>
    %728 = torch.aten.quantize_per_tensor %66, %float7.812500e-03, %int0, %int12 : !torch.vtensor<[256],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[256],!torch.qint8>
    %729 = torch.aten.int_repr %728 : !torch.vtensor<[256],!torch.qint8> -> !torch.vtensor<[256],si8>
    %730 = torch.aten._make_per_tensor_quantized_tensor %729, %float7.812500e-03, %int0 : !torch.vtensor<[256],si8>, !torch.float, !torch.int -> !torch.vtensor<[256],!torch.qint8>
    %731 = torch.aten.dequantize.self %730 : !torch.vtensor<[256],!torch.qint8> -> !torch.vtensor<[256],f32>
    %732 = torch.aten.quantize_per_tensor %731, %float1.525880e-05, %int0, %int14 : !torch.vtensor<[256],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[256],!torch.qint32>
    %733 = torch.aten.int_repr %732 : !torch.vtensor<[256],!torch.qint32> -> !torch.vtensor<[256],si32>
    %734 = torch.aten.convolution %724, %727, %733, %120, %120, %120, %false, %122, %int1 : !torch.vtensor<[1,256,14,14],!torch.qint8>, !torch.vtensor<[256,256,3,3],!torch.qint8>, !torch.vtensor<[256],si32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,256,14,14],si32>
    %735 = torch.aten._make_per_tensor_quantized_tensor %734, %float1.525880e-05, %int0 : !torch.vtensor<[1,256,14,14],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,256,14,14],!torch.qint32>
    %736 = torch.aten.relu %735 : !torch.vtensor<[1,256,14,14],!torch.qint32> -> !torch.vtensor<[1,256,14,14],!torch.qint32>
    %737 = torch.aten.int_repr %736 : !torch.vtensor<[1,256,14,14],!torch.qint32> -> !torch.vtensor<[1,256,14,14],si32>
    %738 = torch.aten._make_per_tensor_quantized_tensor %737, %float1.525880e-05, %int0 : !torch.vtensor<[1,256,14,14],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,256,14,14],!torch.qint32>
    %739 = torch.aten.dequantize.tensor %738 : !torch.vtensor<[1,256,14,14],!torch.qint32> -> !torch.vtensor<[1,256,14,14],f32>
    %740 = torch.aten.quantize_per_tensor %739, %float7.812500e-03, %int0, %int12 : !torch.vtensor<[1,256,14,14],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,256,14,14],!torch.qint8>
    %741 = torch.aten.int_repr %740 : !torch.vtensor<[1,256,14,14],!torch.qint8> -> !torch.vtensor<[1,256,14,14],si8>
    %742 = torch.aten._make_per_tensor_quantized_tensor %741, %float7.812500e-03, %int0 : !torch.vtensor<[1,256,14,14],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,256,14,14],!torch.qint8>
    %743 = torch.aten.quantize_per_tensor %67, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[1024,256,1,1],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1024,256,1,1],!torch.qint8>
    %744 = torch.aten.int_repr %743 : !torch.vtensor<[1024,256,1,1],!torch.qint8> -> !torch.vtensor<[1024,256,1,1],si8>
    %745 = torch.aten._make_per_tensor_quantized_tensor %744, %float3.906250e-03, %int0 : !torch.vtensor<[1024,256,1,1],si8>, !torch.float, !torch.int -> !torch.vtensor<[1024,256,1,1],!torch.qint8>
    %746 = torch.aten.quantize_per_tensor %68, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[1024],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1024],!torch.qint8>
    %747 = torch.aten.int_repr %746 : !torch.vtensor<[1024],!torch.qint8> -> !torch.vtensor<[1024],si8>
    %748 = torch.aten._make_per_tensor_quantized_tensor %747, %float3.906250e-03, %int0 : !torch.vtensor<[1024],si8>, !torch.float, !torch.int -> !torch.vtensor<[1024],!torch.qint8>
    %749 = torch.aten.dequantize.self %748 : !torch.vtensor<[1024],!torch.qint8> -> !torch.vtensor<[1024],f32>
    %750 = torch.aten.quantize_per_tensor %749, %float3.051760e-05, %int0, %int14 : !torch.vtensor<[1024],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1024],!torch.qint32>
    %751 = torch.aten.int_repr %750 : !torch.vtensor<[1024],!torch.qint32> -> !torch.vtensor<[1024],si32>
    %752 = torch.aten.convolution %742, %745, %751, %120, %122, %120, %false, %122, %int1 : !torch.vtensor<[1,256,14,14],!torch.qint8>, !torch.vtensor<[1024,256,1,1],!torch.qint8>, !torch.vtensor<[1024],si32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,1024,14,14],si32>
    %753 = torch.aten._make_per_tensor_quantized_tensor %752, %float3.051760e-05, %int0 : !torch.vtensor<[1,1024,14,14],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,1024,14,14],!torch.qint32>
    %754 = torch.aten.dequantize.tensor %753 : !torch.vtensor<[1,1024,14,14],!torch.qint32> -> !torch.vtensor<[1,1024,14,14],f32>
    %755 = torch.aten.quantize_per_tensor %754, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[1,1024,14,14],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,1024,14,14],!torch.qint8>
    %756 = torch.aten.int_repr %755 : !torch.vtensor<[1,1024,14,14],!torch.qint8> -> !torch.vtensor<[1,1024,14,14],si8>
    %757 = torch.aten._make_per_tensor_quantized_tensor %756, %float3.906250e-03, %int0 : !torch.vtensor<[1,1024,14,14],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,1024,14,14],!torch.qint8>
    %758 = torch.aten.dequantize.self %757 : !torch.vtensor<[1,1024,14,14],!torch.qint8> -> !torch.vtensor<[1,1024,14,14],f32>
    %759 = torch.aten.add.Tensor %758, %706, %int1 : !torch.vtensor<[1,1024,14,14],f32>, !torch.vtensor<[1,1024,14,14],f32>, !torch.int -> !torch.vtensor<[1,1024,14,14],f32>
    %760 = torch.aten.relu %759 : !torch.vtensor<[1,1024,14,14],f32> -> !torch.vtensor<[1,1024,14,14],f32>
    %761 = torch.aten.quantize_per_tensor %760, %float1.562500e-02, %int0, %int12 : !torch.vtensor<[1,1024,14,14],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,1024,14,14],!torch.qint8>
    %762 = torch.aten.int_repr %761 : !torch.vtensor<[1,1024,14,14],!torch.qint8> -> !torch.vtensor<[1,1024,14,14],si8>
    %763 = torch.aten._make_per_tensor_quantized_tensor %762, %float1.562500e-02, %int0 : !torch.vtensor<[1,1024,14,14],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,1024,14,14],!torch.qint8>
    %764 = torch.aten.dequantize.self %763 : !torch.vtensor<[1,1024,14,14],!torch.qint8> -> !torch.vtensor<[1,1024,14,14],f32>
    %765 = torch.aten.quantize_per_tensor %69, %float1.953130e-03, %int0, %int12 : !torch.vtensor<[256,1024,1,1],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[256,1024,1,1],!torch.qint8>
    %766 = torch.aten.int_repr %765 : !torch.vtensor<[256,1024,1,1],!torch.qint8> -> !torch.vtensor<[256,1024,1,1],si8>
    %767 = torch.aten._make_per_tensor_quantized_tensor %766, %float1.953130e-03, %int0 : !torch.vtensor<[256,1024,1,1],si8>, !torch.float, !torch.int -> !torch.vtensor<[256,1024,1,1],!torch.qint8>
    %768 = torch.aten.quantize_per_tensor %70, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[256],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[256],!torch.qint8>
    %769 = torch.aten.int_repr %768 : !torch.vtensor<[256],!torch.qint8> -> !torch.vtensor<[256],si8>
    %770 = torch.aten._make_per_tensor_quantized_tensor %769, %float3.906250e-03, %int0 : !torch.vtensor<[256],si8>, !torch.float, !torch.int -> !torch.vtensor<[256],!torch.qint8>
    %771 = torch.aten.dequantize.self %770 : !torch.vtensor<[256],!torch.qint8> -> !torch.vtensor<[256],f32>
    %772 = torch.aten.quantize_per_tensor %771, %float3.051760e-05, %int0, %int14 : !torch.vtensor<[256],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[256],!torch.qint32>
    %773 = torch.aten.int_repr %772 : !torch.vtensor<[256],!torch.qint32> -> !torch.vtensor<[256],si32>
    %774 = torch.aten.convolution %763, %767, %773, %120, %122, %120, %false, %122, %int1 : !torch.vtensor<[1,1024,14,14],!torch.qint8>, !torch.vtensor<[256,1024,1,1],!torch.qint8>, !torch.vtensor<[256],si32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,256,14,14],si32>
    %775 = torch.aten._make_per_tensor_quantized_tensor %774, %float3.051760e-05, %int0 : !torch.vtensor<[1,256,14,14],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,256,14,14],!torch.qint32>
    %776 = torch.aten.relu %775 : !torch.vtensor<[1,256,14,14],!torch.qint32> -> !torch.vtensor<[1,256,14,14],!torch.qint32>
    %777 = torch.aten.int_repr %776 : !torch.vtensor<[1,256,14,14],!torch.qint32> -> !torch.vtensor<[1,256,14,14],si32>
    %778 = torch.aten._make_per_tensor_quantized_tensor %777, %float3.051760e-05, %int0 : !torch.vtensor<[1,256,14,14],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,256,14,14],!torch.qint32>
    %779 = torch.aten.dequantize.tensor %778 : !torch.vtensor<[1,256,14,14],!torch.qint32> -> !torch.vtensor<[1,256,14,14],f32>
    %780 = torch.aten.quantize_per_tensor %779, %float7.812500e-03, %int0, %int12 : !torch.vtensor<[1,256,14,14],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,256,14,14],!torch.qint8>
    %781 = torch.aten.int_repr %780 : !torch.vtensor<[1,256,14,14],!torch.qint8> -> !torch.vtensor<[1,256,14,14],si8>
    %782 = torch.aten._make_per_tensor_quantized_tensor %781, %float7.812500e-03, %int0 : !torch.vtensor<[1,256,14,14],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,256,14,14],!torch.qint8>
    %783 = torch.aten.quantize_per_tensor %71, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[256,256,3,3],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[256,256,3,3],!torch.qint8>
    %784 = torch.aten.int_repr %783 : !torch.vtensor<[256,256,3,3],!torch.qint8> -> !torch.vtensor<[256,256,3,3],si8>
    %785 = torch.aten._make_per_tensor_quantized_tensor %784, %float3.906250e-03, %int0 : !torch.vtensor<[256,256,3,3],si8>, !torch.float, !torch.int -> !torch.vtensor<[256,256,3,3],!torch.qint8>
    %786 = torch.aten.quantize_per_tensor %72, %float7.812500e-03, %int0, %int12 : !torch.vtensor<[256],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[256],!torch.qint8>
    %787 = torch.aten.int_repr %786 : !torch.vtensor<[256],!torch.qint8> -> !torch.vtensor<[256],si8>
    %788 = torch.aten._make_per_tensor_quantized_tensor %787, %float7.812500e-03, %int0 : !torch.vtensor<[256],si8>, !torch.float, !torch.int -> !torch.vtensor<[256],!torch.qint8>
    %789 = torch.aten.dequantize.self %788 : !torch.vtensor<[256],!torch.qint8> -> !torch.vtensor<[256],f32>
    %790 = torch.aten.quantize_per_tensor %789, %float3.051760e-05, %int0, %int14 : !torch.vtensor<[256],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[256],!torch.qint32>
    %791 = torch.aten.int_repr %790 : !torch.vtensor<[256],!torch.qint32> -> !torch.vtensor<[256],si32>
    %792 = torch.aten.convolution %782, %785, %791, %120, %120, %120, %false, %122, %int1 : !torch.vtensor<[1,256,14,14],!torch.qint8>, !torch.vtensor<[256,256,3,3],!torch.qint8>, !torch.vtensor<[256],si32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,256,14,14],si32>
    %793 = torch.aten._make_per_tensor_quantized_tensor %792, %float3.051760e-05, %int0 : !torch.vtensor<[1,256,14,14],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,256,14,14],!torch.qint32>
    %794 = torch.aten.relu %793 : !torch.vtensor<[1,256,14,14],!torch.qint32> -> !torch.vtensor<[1,256,14,14],!torch.qint32>
    %795 = torch.aten.int_repr %794 : !torch.vtensor<[1,256,14,14],!torch.qint32> -> !torch.vtensor<[1,256,14,14],si32>
    %796 = torch.aten._make_per_tensor_quantized_tensor %795, %float3.051760e-05, %int0 : !torch.vtensor<[1,256,14,14],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,256,14,14],!torch.qint32>
    %797 = torch.aten.dequantize.tensor %796 : !torch.vtensor<[1,256,14,14],!torch.qint32> -> !torch.vtensor<[1,256,14,14],f32>
    %798 = torch.aten.quantize_per_tensor %797, %float7.812500e-03, %int0, %int12 : !torch.vtensor<[1,256,14,14],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,256,14,14],!torch.qint8>
    %799 = torch.aten.int_repr %798 : !torch.vtensor<[1,256,14,14],!torch.qint8> -> !torch.vtensor<[1,256,14,14],si8>
    %800 = torch.aten._make_per_tensor_quantized_tensor %799, %float7.812500e-03, %int0 : !torch.vtensor<[1,256,14,14],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,256,14,14],!torch.qint8>
    %801 = torch.aten.quantize_per_tensor %73, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[1024,256,1,1],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1024,256,1,1],!torch.qint8>
    %802 = torch.aten.int_repr %801 : !torch.vtensor<[1024,256,1,1],!torch.qint8> -> !torch.vtensor<[1024,256,1,1],si8>
    %803 = torch.aten._make_per_tensor_quantized_tensor %802, %float3.906250e-03, %int0 : !torch.vtensor<[1024,256,1,1],si8>, !torch.float, !torch.int -> !torch.vtensor<[1024,256,1,1],!torch.qint8>
    %804 = torch.aten.quantize_per_tensor %74, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[1024],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1024],!torch.qint8>
    %805 = torch.aten.int_repr %804 : !torch.vtensor<[1024],!torch.qint8> -> !torch.vtensor<[1024],si8>
    %806 = torch.aten._make_per_tensor_quantized_tensor %805, %float3.906250e-03, %int0 : !torch.vtensor<[1024],si8>, !torch.float, !torch.int -> !torch.vtensor<[1024],!torch.qint8>
    %807 = torch.aten.dequantize.self %806 : !torch.vtensor<[1024],!torch.qint8> -> !torch.vtensor<[1024],f32>
    %808 = torch.aten.quantize_per_tensor %807, %float3.051760e-05, %int0, %int14 : !torch.vtensor<[1024],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1024],!torch.qint32>
    %809 = torch.aten.int_repr %808 : !torch.vtensor<[1024],!torch.qint32> -> !torch.vtensor<[1024],si32>
    %810 = torch.aten.convolution %800, %803, %809, %120, %122, %120, %false, %122, %int1 : !torch.vtensor<[1,256,14,14],!torch.qint8>, !torch.vtensor<[1024,256,1,1],!torch.qint8>, !torch.vtensor<[1024],si32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,1024,14,14],si32>
    %811 = torch.aten._make_per_tensor_quantized_tensor %810, %float3.051760e-05, %int0 : !torch.vtensor<[1,1024,14,14],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,1024,14,14],!torch.qint32>
    %812 = torch.aten.dequantize.tensor %811 : !torch.vtensor<[1,1024,14,14],!torch.qint32> -> !torch.vtensor<[1,1024,14,14],f32>
    %813 = torch.aten.quantize_per_tensor %812, %float7.812500e-03, %int0, %int12 : !torch.vtensor<[1,1024,14,14],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,1024,14,14],!torch.qint8>
    %814 = torch.aten.int_repr %813 : !torch.vtensor<[1,1024,14,14],!torch.qint8> -> !torch.vtensor<[1,1024,14,14],si8>
    %815 = torch.aten._make_per_tensor_quantized_tensor %814, %float7.812500e-03, %int0 : !torch.vtensor<[1,1024,14,14],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,1024,14,14],!torch.qint8>
    %816 = torch.aten.dequantize.self %815 : !torch.vtensor<[1,1024,14,14],!torch.qint8> -> !torch.vtensor<[1,1024,14,14],f32>
    %817 = torch.aten.add.Tensor %816, %764, %int1 : !torch.vtensor<[1,1024,14,14],f32>, !torch.vtensor<[1,1024,14,14],f32>, !torch.int -> !torch.vtensor<[1,1024,14,14],f32>
    %818 = torch.aten.relu %817 : !torch.vtensor<[1,1024,14,14],f32> -> !torch.vtensor<[1,1024,14,14],f32>
    %819 = torch.aten.quantize_per_tensor %818, %float1.562500e-02, %int0, %int12 : !torch.vtensor<[1,1024,14,14],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,1024,14,14],!torch.qint8>
    %820 = torch.aten.int_repr %819 : !torch.vtensor<[1,1024,14,14],!torch.qint8> -> !torch.vtensor<[1,1024,14,14],si8>
    %821 = torch.aten._make_per_tensor_quantized_tensor %820, %float1.562500e-02, %int0 : !torch.vtensor<[1,1024,14,14],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,1024,14,14],!torch.qint8>
    %822 = torch.aten.dequantize.self %821 : !torch.vtensor<[1,1024,14,14],!torch.qint8> -> !torch.vtensor<[1,1024,14,14],f32>
    %823 = torch.aten.quantize_per_tensor %75, %float1.953130e-03, %int0, %int12 : !torch.vtensor<[256,1024,1,1],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[256,1024,1,1],!torch.qint8>
    %824 = torch.aten.int_repr %823 : !torch.vtensor<[256,1024,1,1],!torch.qint8> -> !torch.vtensor<[256,1024,1,1],si8>
    %825 = torch.aten._make_per_tensor_quantized_tensor %824, %float1.953130e-03, %int0 : !torch.vtensor<[256,1024,1,1],si8>, !torch.float, !torch.int -> !torch.vtensor<[256,1024,1,1],!torch.qint8>
    %826 = torch.aten.quantize_per_tensor %76, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[256],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[256],!torch.qint8>
    %827 = torch.aten.int_repr %826 : !torch.vtensor<[256],!torch.qint8> -> !torch.vtensor<[256],si8>
    %828 = torch.aten._make_per_tensor_quantized_tensor %827, %float3.906250e-03, %int0 : !torch.vtensor<[256],si8>, !torch.float, !torch.int -> !torch.vtensor<[256],!torch.qint8>
    %829 = torch.aten.dequantize.self %828 : !torch.vtensor<[256],!torch.qint8> -> !torch.vtensor<[256],f32>
    %830 = torch.aten.quantize_per_tensor %829, %float3.051760e-05, %int0, %int14 : !torch.vtensor<[256],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[256],!torch.qint32>
    %831 = torch.aten.int_repr %830 : !torch.vtensor<[256],!torch.qint32> -> !torch.vtensor<[256],si32>
    %832 = torch.aten.convolution %821, %825, %831, %120, %122, %120, %false, %122, %int1 : !torch.vtensor<[1,1024,14,14],!torch.qint8>, !torch.vtensor<[256,1024,1,1],!torch.qint8>, !torch.vtensor<[256],si32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,256,14,14],si32>
    %833 = torch.aten._make_per_tensor_quantized_tensor %832, %float3.051760e-05, %int0 : !torch.vtensor<[1,256,14,14],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,256,14,14],!torch.qint32>
    %834 = torch.aten.relu %833 : !torch.vtensor<[1,256,14,14],!torch.qint32> -> !torch.vtensor<[1,256,14,14],!torch.qint32>
    %835 = torch.aten.int_repr %834 : !torch.vtensor<[1,256,14,14],!torch.qint32> -> !torch.vtensor<[1,256,14,14],si32>
    %836 = torch.aten._make_per_tensor_quantized_tensor %835, %float3.051760e-05, %int0 : !torch.vtensor<[1,256,14,14],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,256,14,14],!torch.qint32>
    %837 = torch.aten.dequantize.tensor %836 : !torch.vtensor<[1,256,14,14],!torch.qint32> -> !torch.vtensor<[1,256,14,14],f32>
    %838 = torch.aten.quantize_per_tensor %837, %float7.812500e-03, %int0, %int12 : !torch.vtensor<[1,256,14,14],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,256,14,14],!torch.qint8>
    %839 = torch.aten.int_repr %838 : !torch.vtensor<[1,256,14,14],!torch.qint8> -> !torch.vtensor<[1,256,14,14],si8>
    %840 = torch.aten._make_per_tensor_quantized_tensor %839, %float7.812500e-03, %int0 : !torch.vtensor<[1,256,14,14],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,256,14,14],!torch.qint8>
    %841 = torch.aten.quantize_per_tensor %77, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[256,256,3,3],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[256,256,3,3],!torch.qint8>
    %842 = torch.aten.int_repr %841 : !torch.vtensor<[256,256,3,3],!torch.qint8> -> !torch.vtensor<[256,256,3,3],si8>
    %843 = torch.aten._make_per_tensor_quantized_tensor %842, %float3.906250e-03, %int0 : !torch.vtensor<[256,256,3,3],si8>, !torch.float, !torch.int -> !torch.vtensor<[256,256,3,3],!torch.qint8>
    %844 = torch.aten.quantize_per_tensor %78, %float7.812500e-03, %int0, %int12 : !torch.vtensor<[256],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[256],!torch.qint8>
    %845 = torch.aten.int_repr %844 : !torch.vtensor<[256],!torch.qint8> -> !torch.vtensor<[256],si8>
    %846 = torch.aten._make_per_tensor_quantized_tensor %845, %float7.812500e-03, %int0 : !torch.vtensor<[256],si8>, !torch.float, !torch.int -> !torch.vtensor<[256],!torch.qint8>
    %847 = torch.aten.dequantize.self %846 : !torch.vtensor<[256],!torch.qint8> -> !torch.vtensor<[256],f32>
    %848 = torch.aten.quantize_per_tensor %847, %float3.051760e-05, %int0, %int14 : !torch.vtensor<[256],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[256],!torch.qint32>
    %849 = torch.aten.int_repr %848 : !torch.vtensor<[256],!torch.qint32> -> !torch.vtensor<[256],si32>
    %850 = torch.aten.convolution %840, %843, %849, %120, %120, %120, %false, %122, %int1 : !torch.vtensor<[1,256,14,14],!torch.qint8>, !torch.vtensor<[256,256,3,3],!torch.qint8>, !torch.vtensor<[256],si32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,256,14,14],si32>
    %851 = torch.aten._make_per_tensor_quantized_tensor %850, %float3.051760e-05, %int0 : !torch.vtensor<[1,256,14,14],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,256,14,14],!torch.qint32>
    %852 = torch.aten.relu %851 : !torch.vtensor<[1,256,14,14],!torch.qint32> -> !torch.vtensor<[1,256,14,14],!torch.qint32>
    %853 = torch.aten.int_repr %852 : !torch.vtensor<[1,256,14,14],!torch.qint32> -> !torch.vtensor<[1,256,14,14],si32>
    %854 = torch.aten._make_per_tensor_quantized_tensor %853, %float3.051760e-05, %int0 : !torch.vtensor<[1,256,14,14],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,256,14,14],!torch.qint32>
    %855 = torch.aten.dequantize.tensor %854 : !torch.vtensor<[1,256,14,14],!torch.qint32> -> !torch.vtensor<[1,256,14,14],f32>
    %856 = torch.aten.quantize_per_tensor %855, %float7.812500e-03, %int0, %int12 : !torch.vtensor<[1,256,14,14],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,256,14,14],!torch.qint8>
    %857 = torch.aten.int_repr %856 : !torch.vtensor<[1,256,14,14],!torch.qint8> -> !torch.vtensor<[1,256,14,14],si8>
    %858 = torch.aten._make_per_tensor_quantized_tensor %857, %float7.812500e-03, %int0 : !torch.vtensor<[1,256,14,14],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,256,14,14],!torch.qint8>
    %859 = torch.aten.quantize_per_tensor %79, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[1024,256,1,1],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1024,256,1,1],!torch.qint8>
    %860 = torch.aten.int_repr %859 : !torch.vtensor<[1024,256,1,1],!torch.qint8> -> !torch.vtensor<[1024,256,1,1],si8>
    %861 = torch.aten._make_per_tensor_quantized_tensor %860, %float3.906250e-03, %int0 : !torch.vtensor<[1024,256,1,1],si8>, !torch.float, !torch.int -> !torch.vtensor<[1024,256,1,1],!torch.qint8>
    %862 = torch.aten.quantize_per_tensor %80, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[1024],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1024],!torch.qint8>
    %863 = torch.aten.int_repr %862 : !torch.vtensor<[1024],!torch.qint8> -> !torch.vtensor<[1024],si8>
    %864 = torch.aten._make_per_tensor_quantized_tensor %863, %float3.906250e-03, %int0 : !torch.vtensor<[1024],si8>, !torch.float, !torch.int -> !torch.vtensor<[1024],!torch.qint8>
    %865 = torch.aten.dequantize.self %864 : !torch.vtensor<[1024],!torch.qint8> -> !torch.vtensor<[1024],f32>
    %866 = torch.aten.quantize_per_tensor %865, %float3.051760e-05, %int0, %int14 : !torch.vtensor<[1024],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1024],!torch.qint32>
    %867 = torch.aten.int_repr %866 : !torch.vtensor<[1024],!torch.qint32> -> !torch.vtensor<[1024],si32>
    %868 = torch.aten.convolution %858, %861, %867, %120, %122, %120, %false, %122, %int1 : !torch.vtensor<[1,256,14,14],!torch.qint8>, !torch.vtensor<[1024,256,1,1],!torch.qint8>, !torch.vtensor<[1024],si32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,1024,14,14],si32>
    %869 = torch.aten._make_per_tensor_quantized_tensor %868, %float3.051760e-05, %int0 : !torch.vtensor<[1,1024,14,14],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,1024,14,14],!torch.qint32>
    %870 = torch.aten.dequantize.tensor %869 : !torch.vtensor<[1,1024,14,14],!torch.qint32> -> !torch.vtensor<[1,1024,14,14],f32>
    %871 = torch.aten.quantize_per_tensor %870, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[1,1024,14,14],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,1024,14,14],!torch.qint8>
    %872 = torch.aten.int_repr %871 : !torch.vtensor<[1,1024,14,14],!torch.qint8> -> !torch.vtensor<[1,1024,14,14],si8>
    %873 = torch.aten._make_per_tensor_quantized_tensor %872, %float3.906250e-03, %int0 : !torch.vtensor<[1,1024,14,14],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,1024,14,14],!torch.qint8>
    %874 = torch.aten.dequantize.self %873 : !torch.vtensor<[1,1024,14,14],!torch.qint8> -> !torch.vtensor<[1,1024,14,14],f32>
    %875 = torch.aten.add.Tensor %874, %822, %int1 : !torch.vtensor<[1,1024,14,14],f32>, !torch.vtensor<[1,1024,14,14],f32>, !torch.int -> !torch.vtensor<[1,1024,14,14],f32>
    %876 = torch.aten.relu %875 : !torch.vtensor<[1,1024,14,14],f32> -> !torch.vtensor<[1,1024,14,14],f32>
    %877 = torch.aten.quantize_per_tensor %876, %float1.562500e-02, %int0, %int12 : !torch.vtensor<[1,1024,14,14],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,1024,14,14],!torch.qint8>
    %878 = torch.aten.int_repr %877 : !torch.vtensor<[1,1024,14,14],!torch.qint8> -> !torch.vtensor<[1,1024,14,14],si8>
    %879 = torch.aten._make_per_tensor_quantized_tensor %878, %float1.562500e-02, %int0 : !torch.vtensor<[1,1024,14,14],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,1024,14,14],!torch.qint8>
    %880 = torch.aten.dequantize.self %879 : !torch.vtensor<[1,1024,14,14],!torch.qint8> -> !torch.vtensor<[1,1024,14,14],f32>
    %881 = torch.aten.quantize_per_tensor %81, %float1.953130e-03, %int0, %int12 : !torch.vtensor<[256,1024,1,1],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[256,1024,1,1],!torch.qint8>
    %882 = torch.aten.int_repr %881 : !torch.vtensor<[256,1024,1,1],!torch.qint8> -> !torch.vtensor<[256,1024,1,1],si8>
    %883 = torch.aten._make_per_tensor_quantized_tensor %882, %float1.953130e-03, %int0 : !torch.vtensor<[256,1024,1,1],si8>, !torch.float, !torch.int -> !torch.vtensor<[256,1024,1,1],!torch.qint8>
    %884 = torch.aten.quantize_per_tensor %82, %float1.953130e-03, %int0, %int12 : !torch.vtensor<[256],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[256],!torch.qint8>
    %885 = torch.aten.int_repr %884 : !torch.vtensor<[256],!torch.qint8> -> !torch.vtensor<[256],si8>
    %886 = torch.aten._make_per_tensor_quantized_tensor %885, %float1.953130e-03, %int0 : !torch.vtensor<[256],si8>, !torch.float, !torch.int -> !torch.vtensor<[256],!torch.qint8>
    %887 = torch.aten.dequantize.self %886 : !torch.vtensor<[256],!torch.qint8> -> !torch.vtensor<[256],f32>
    %888 = torch.aten.quantize_per_tensor %887, %float3.051760e-05, %int0, %int14 : !torch.vtensor<[256],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[256],!torch.qint32>
    %889 = torch.aten.int_repr %888 : !torch.vtensor<[256],!torch.qint32> -> !torch.vtensor<[256],si32>
    %890 = torch.aten.convolution %879, %883, %889, %120, %122, %120, %false, %122, %int1 : !torch.vtensor<[1,1024,14,14],!torch.qint8>, !torch.vtensor<[256,1024,1,1],!torch.qint8>, !torch.vtensor<[256],si32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,256,14,14],si32>
    %891 = torch.aten._make_per_tensor_quantized_tensor %890, %float3.051760e-05, %int0 : !torch.vtensor<[1,256,14,14],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,256,14,14],!torch.qint32>
    %892 = torch.aten.relu %891 : !torch.vtensor<[1,256,14,14],!torch.qint32> -> !torch.vtensor<[1,256,14,14],!torch.qint32>
    %893 = torch.aten.int_repr %892 : !torch.vtensor<[1,256,14,14],!torch.qint32> -> !torch.vtensor<[1,256,14,14],si32>
    %894 = torch.aten._make_per_tensor_quantized_tensor %893, %float3.051760e-05, %int0 : !torch.vtensor<[1,256,14,14],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,256,14,14],!torch.qint32>
    %895 = torch.aten.dequantize.tensor %894 : !torch.vtensor<[1,256,14,14],!torch.qint32> -> !torch.vtensor<[1,256,14,14],f32>
    %896 = torch.aten.quantize_per_tensor %895, %float7.812500e-03, %int0, %int12 : !torch.vtensor<[1,256,14,14],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,256,14,14],!torch.qint8>
    %897 = torch.aten.int_repr %896 : !torch.vtensor<[1,256,14,14],!torch.qint8> -> !torch.vtensor<[1,256,14,14],si8>
    %898 = torch.aten._make_per_tensor_quantized_tensor %897, %float7.812500e-03, %int0 : !torch.vtensor<[1,256,14,14],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,256,14,14],!torch.qint8>
    %899 = torch.aten.quantize_per_tensor %83, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[256,256,3,3],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[256,256,3,3],!torch.qint8>
    %900 = torch.aten.int_repr %899 : !torch.vtensor<[256,256,3,3],!torch.qint8> -> !torch.vtensor<[256,256,3,3],si8>
    %901 = torch.aten._make_per_tensor_quantized_tensor %900, %float3.906250e-03, %int0 : !torch.vtensor<[256,256,3,3],si8>, !torch.float, !torch.int -> !torch.vtensor<[256,256,3,3],!torch.qint8>
    %902 = torch.aten.quantize_per_tensor %84, %float7.812500e-03, %int0, %int12 : !torch.vtensor<[256],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[256],!torch.qint8>
    %903 = torch.aten.int_repr %902 : !torch.vtensor<[256],!torch.qint8> -> !torch.vtensor<[256],si8>
    %904 = torch.aten._make_per_tensor_quantized_tensor %903, %float7.812500e-03, %int0 : !torch.vtensor<[256],si8>, !torch.float, !torch.int -> !torch.vtensor<[256],!torch.qint8>
    %905 = torch.aten.dequantize.self %904 : !torch.vtensor<[256],!torch.qint8> -> !torch.vtensor<[256],f32>
    %906 = torch.aten.quantize_per_tensor %905, %float3.051760e-05, %int0, %int14 : !torch.vtensor<[256],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[256],!torch.qint32>
    %907 = torch.aten.int_repr %906 : !torch.vtensor<[256],!torch.qint32> -> !torch.vtensor<[256],si32>
    %908 = torch.aten.convolution %898, %901, %907, %120, %120, %120, %false, %122, %int1 : !torch.vtensor<[1,256,14,14],!torch.qint8>, !torch.vtensor<[256,256,3,3],!torch.qint8>, !torch.vtensor<[256],si32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,256,14,14],si32>
    %909 = torch.aten._make_per_tensor_quantized_tensor %908, %float3.051760e-05, %int0 : !torch.vtensor<[1,256,14,14],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,256,14,14],!torch.qint32>
    %910 = torch.aten.relu %909 : !torch.vtensor<[1,256,14,14],!torch.qint32> -> !torch.vtensor<[1,256,14,14],!torch.qint32>
    %911 = torch.aten.int_repr %910 : !torch.vtensor<[1,256,14,14],!torch.qint32> -> !torch.vtensor<[1,256,14,14],si32>
    %912 = torch.aten._make_per_tensor_quantized_tensor %911, %float3.051760e-05, %int0 : !torch.vtensor<[1,256,14,14],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,256,14,14],!torch.qint32>
    %913 = torch.aten.dequantize.tensor %912 : !torch.vtensor<[1,256,14,14],!torch.qint32> -> !torch.vtensor<[1,256,14,14],f32>
    %914 = torch.aten.quantize_per_tensor %913, %float1.562500e-02, %int0, %int12 : !torch.vtensor<[1,256,14,14],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,256,14,14],!torch.qint8>
    %915 = torch.aten.int_repr %914 : !torch.vtensor<[1,256,14,14],!torch.qint8> -> !torch.vtensor<[1,256,14,14],si8>
    %916 = torch.aten._make_per_tensor_quantized_tensor %915, %float1.562500e-02, %int0 : !torch.vtensor<[1,256,14,14],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,256,14,14],!torch.qint8>
    %917 = torch.aten.quantize_per_tensor %85, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[1024,256,1,1],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1024,256,1,1],!torch.qint8>
    %918 = torch.aten.int_repr %917 : !torch.vtensor<[1024,256,1,1],!torch.qint8> -> !torch.vtensor<[1024,256,1,1],si8>
    %919 = torch.aten._make_per_tensor_quantized_tensor %918, %float3.906250e-03, %int0 : !torch.vtensor<[1024,256,1,1],si8>, !torch.float, !torch.int -> !torch.vtensor<[1024,256,1,1],!torch.qint8>
    %920 = torch.aten.quantize_per_tensor %86, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[1024],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1024],!torch.qint8>
    %921 = torch.aten.int_repr %920 : !torch.vtensor<[1024],!torch.qint8> -> !torch.vtensor<[1024],si8>
    %922 = torch.aten._make_per_tensor_quantized_tensor %921, %float3.906250e-03, %int0 : !torch.vtensor<[1024],si8>, !torch.float, !torch.int -> !torch.vtensor<[1024],!torch.qint8>
    %923 = torch.aten.dequantize.self %922 : !torch.vtensor<[1024],!torch.qint8> -> !torch.vtensor<[1024],f32>
    %924 = torch.aten.quantize_per_tensor %923, %float6.103520e-05, %int0, %int14 : !torch.vtensor<[1024],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1024],!torch.qint32>
    %925 = torch.aten.int_repr %924 : !torch.vtensor<[1024],!torch.qint32> -> !torch.vtensor<[1024],si32>
    %926 = torch.aten.convolution %916, %919, %925, %120, %122, %120, %false, %122, %int1 : !torch.vtensor<[1,256,14,14],!torch.qint8>, !torch.vtensor<[1024,256,1,1],!torch.qint8>, !torch.vtensor<[1024],si32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,1024,14,14],si32>
    %927 = torch.aten._make_per_tensor_quantized_tensor %926, %float6.103520e-05, %int0 : !torch.vtensor<[1,1024,14,14],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,1024,14,14],!torch.qint32>
    %928 = torch.aten.dequantize.tensor %927 : !torch.vtensor<[1,1024,14,14],!torch.qint32> -> !torch.vtensor<[1,1024,14,14],f32>
    %929 = torch.aten.quantize_per_tensor %928, %float7.812500e-03, %int0, %int12 : !torch.vtensor<[1,1024,14,14],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,1024,14,14],!torch.qint8>
    %930 = torch.aten.int_repr %929 : !torch.vtensor<[1,1024,14,14],!torch.qint8> -> !torch.vtensor<[1,1024,14,14],si8>
    %931 = torch.aten._make_per_tensor_quantized_tensor %930, %float7.812500e-03, %int0 : !torch.vtensor<[1,1024,14,14],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,1024,14,14],!torch.qint8>
    %932 = torch.aten.dequantize.self %931 : !torch.vtensor<[1,1024,14,14],!torch.qint8> -> !torch.vtensor<[1,1024,14,14],f32>
    %933 = torch.aten.add.Tensor %932, %880, %int1 : !torch.vtensor<[1,1024,14,14],f32>, !torch.vtensor<[1,1024,14,14],f32>, !torch.int -> !torch.vtensor<[1,1024,14,14],f32>
    %934 = torch.aten.relu %933 : !torch.vtensor<[1,1024,14,14],f32> -> !torch.vtensor<[1,1024,14,14],f32>
    %935 = torch.aten.quantize_per_tensor %934, %float7.812500e-03, %int0, %int12 : !torch.vtensor<[1,1024,14,14],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,1024,14,14],!torch.qint8>
    %936 = torch.aten.int_repr %935 : !torch.vtensor<[1,1024,14,14],!torch.qint8> -> !torch.vtensor<[1,1024,14,14],si8>
    %937 = torch.aten._make_per_tensor_quantized_tensor %936, %float7.812500e-03, %int0 : !torch.vtensor<[1,1024,14,14],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,1024,14,14],!torch.qint8>
    %938 = torch.aten.quantize_per_tensor %87, %float1.953130e-03, %int0, %int12 : !torch.vtensor<[512,1024,1,1],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[512,1024,1,1],!torch.qint8>
    %939 = torch.aten.int_repr %938 : !torch.vtensor<[512,1024,1,1],!torch.qint8> -> !torch.vtensor<[512,1024,1,1],si8>
    %940 = torch.aten._make_per_tensor_quantized_tensor %939, %float1.953130e-03, %int0 : !torch.vtensor<[512,1024,1,1],si8>, !torch.float, !torch.int -> !torch.vtensor<[512,1024,1,1],!torch.qint8>
    %941 = torch.aten.quantize_per_tensor %88, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[512],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[512],!torch.qint8>
    %942 = torch.aten.int_repr %941 : !torch.vtensor<[512],!torch.qint8> -> !torch.vtensor<[512],si8>
    %943 = torch.aten._make_per_tensor_quantized_tensor %942, %float3.906250e-03, %int0 : !torch.vtensor<[512],si8>, !torch.float, !torch.int -> !torch.vtensor<[512],!torch.qint8>
    %944 = torch.aten.dequantize.self %943 : !torch.vtensor<[512],!torch.qint8> -> !torch.vtensor<[512],f32>
    %945 = torch.aten.quantize_per_tensor %944, %float1.525880e-05, %int0, %int14 : !torch.vtensor<[512],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[512],!torch.qint32>
    %946 = torch.aten.int_repr %945 : !torch.vtensor<[512],!torch.qint32> -> !torch.vtensor<[512],si32>
    %947 = torch.aten.convolution %937, %940, %946, %120, %122, %120, %false, %122, %int1 : !torch.vtensor<[1,1024,14,14],!torch.qint8>, !torch.vtensor<[512,1024,1,1],!torch.qint8>, !torch.vtensor<[512],si32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,512,14,14],si32>
    %948 = torch.aten._make_per_tensor_quantized_tensor %947, %float1.525880e-05, %int0 : !torch.vtensor<[1,512,14,14],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,512,14,14],!torch.qint32>
    %949 = torch.aten.relu %948 : !torch.vtensor<[1,512,14,14],!torch.qint32> -> !torch.vtensor<[1,512,14,14],!torch.qint32>
    %950 = torch.aten.int_repr %949 : !torch.vtensor<[1,512,14,14],!torch.qint32> -> !torch.vtensor<[1,512,14,14],si32>
    %951 = torch.aten._make_per_tensor_quantized_tensor %950, %float1.525880e-05, %int0 : !torch.vtensor<[1,512,14,14],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,512,14,14],!torch.qint32>
    %952 = torch.aten.dequantize.tensor %951 : !torch.vtensor<[1,512,14,14],!torch.qint32> -> !torch.vtensor<[1,512,14,14],f32>
    %953 = torch.aten.quantize_per_tensor %952, %float7.812500e-03, %int0, %int12 : !torch.vtensor<[1,512,14,14],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,512,14,14],!torch.qint8>
    %954 = torch.aten.int_repr %953 : !torch.vtensor<[1,512,14,14],!torch.qint8> -> !torch.vtensor<[1,512,14,14],si8>
    %955 = torch.aten._make_per_tensor_quantized_tensor %954, %float7.812500e-03, %int0 : !torch.vtensor<[1,512,14,14],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,512,14,14],!torch.qint8>
    %956 = torch.aten.quantize_per_tensor %89, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[512,512,3,3],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[512,512,3,3],!torch.qint8>
    %957 = torch.aten.int_repr %956 : !torch.vtensor<[512,512,3,3],!torch.qint8> -> !torch.vtensor<[512,512,3,3],si8>
    %958 = torch.aten._make_per_tensor_quantized_tensor %957, %float3.906250e-03, %int0 : !torch.vtensor<[512,512,3,3],si8>, !torch.float, !torch.int -> !torch.vtensor<[512,512,3,3],!torch.qint8>
    %959 = torch.aten.quantize_per_tensor %90, %float7.812500e-03, %int0, %int12 : !torch.vtensor<[512],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[512],!torch.qint8>
    %960 = torch.aten.int_repr %959 : !torch.vtensor<[512],!torch.qint8> -> !torch.vtensor<[512],si8>
    %961 = torch.aten._make_per_tensor_quantized_tensor %960, %float7.812500e-03, %int0 : !torch.vtensor<[512],si8>, !torch.float, !torch.int -> !torch.vtensor<[512],!torch.qint8>
    %962 = torch.aten.dequantize.self %961 : !torch.vtensor<[512],!torch.qint8> -> !torch.vtensor<[512],f32>
    %963 = torch.aten.quantize_per_tensor %962, %float3.051760e-05, %int0, %int14 : !torch.vtensor<[512],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[512],!torch.qint32>
    %964 = torch.aten.int_repr %963 : !torch.vtensor<[512],!torch.qint32> -> !torch.vtensor<[512],si32>
    %965 = torch.aten.convolution %955, %958, %964, %121, %120, %120, %false, %122, %int1 : !torch.vtensor<[1,512,14,14],!torch.qint8>, !torch.vtensor<[512,512,3,3],!torch.qint8>, !torch.vtensor<[512],si32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,512,7,7],si32>
    %966 = torch.aten._make_per_tensor_quantized_tensor %965, %float3.051760e-05, %int0 : !torch.vtensor<[1,512,7,7],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,512,7,7],!torch.qint32>
    %967 = torch.aten.relu %966 : !torch.vtensor<[1,512,7,7],!torch.qint32> -> !torch.vtensor<[1,512,7,7],!torch.qint32>
    %968 = torch.aten.int_repr %967 : !torch.vtensor<[1,512,7,7],!torch.qint32> -> !torch.vtensor<[1,512,7,7],si32>
    %969 = torch.aten._make_per_tensor_quantized_tensor %968, %float3.051760e-05, %int0 : !torch.vtensor<[1,512,7,7],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,512,7,7],!torch.qint32>
    %970 = torch.aten.dequantize.tensor %969 : !torch.vtensor<[1,512,7,7],!torch.qint32> -> !torch.vtensor<[1,512,7,7],f32>
    %971 = torch.aten.quantize_per_tensor %970, %float1.562500e-02, %int0, %int12 : !torch.vtensor<[1,512,7,7],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,512,7,7],!torch.qint8>
    %972 = torch.aten.int_repr %971 : !torch.vtensor<[1,512,7,7],!torch.qint8> -> !torch.vtensor<[1,512,7,7],si8>
    %973 = torch.aten._make_per_tensor_quantized_tensor %972, %float1.562500e-02, %int0 : !torch.vtensor<[1,512,7,7],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,512,7,7],!torch.qint8>
    %974 = torch.aten.quantize_per_tensor %91, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[2048,512,1,1],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[2048,512,1,1],!torch.qint8>
    %975 = torch.aten.int_repr %974 : !torch.vtensor<[2048,512,1,1],!torch.qint8> -> !torch.vtensor<[2048,512,1,1],si8>
    %976 = torch.aten._make_per_tensor_quantized_tensor %975, %float3.906250e-03, %int0 : !torch.vtensor<[2048,512,1,1],si8>, !torch.float, !torch.int -> !torch.vtensor<[2048,512,1,1],!torch.qint8>
    %977 = torch.aten.quantize_per_tensor %92, %float1.562500e-02, %int0, %int12 : !torch.vtensor<[2048],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[2048],!torch.qint8>
    %978 = torch.aten.int_repr %977 : !torch.vtensor<[2048],!torch.qint8> -> !torch.vtensor<[2048],si8>
    %979 = torch.aten._make_per_tensor_quantized_tensor %978, %float1.562500e-02, %int0 : !torch.vtensor<[2048],si8>, !torch.float, !torch.int -> !torch.vtensor<[2048],!torch.qint8>
    %980 = torch.aten.dequantize.self %979 : !torch.vtensor<[2048],!torch.qint8> -> !torch.vtensor<[2048],f32>
    %981 = torch.aten.quantize_per_tensor %980, %float6.103520e-05, %int0, %int14 : !torch.vtensor<[2048],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[2048],!torch.qint32>
    %982 = torch.aten.int_repr %981 : !torch.vtensor<[2048],!torch.qint32> -> !torch.vtensor<[2048],si32>
    %983 = torch.aten.convolution %973, %976, %982, %120, %122, %120, %false, %122, %int1 : !torch.vtensor<[1,512,7,7],!torch.qint8>, !torch.vtensor<[2048,512,1,1],!torch.qint8>, !torch.vtensor<[2048],si32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,2048,7,7],si32>
    %984 = torch.aten._make_per_tensor_quantized_tensor %983, %float6.103520e-05, %int0 : !torch.vtensor<[1,2048,7,7],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,2048,7,7],!torch.qint32>
    %985 = torch.aten.dequantize.tensor %984 : !torch.vtensor<[1,2048,7,7],!torch.qint32> -> !torch.vtensor<[1,2048,7,7],f32>
    %986 = torch.aten.quantize_per_tensor %985, %float1.562500e-02, %int0, %int12 : !torch.vtensor<[1,2048,7,7],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,2048,7,7],!torch.qint8>
    %987 = torch.aten.int_repr %986 : !torch.vtensor<[1,2048,7,7],!torch.qint8> -> !torch.vtensor<[1,2048,7,7],si8>
    %988 = torch.aten._make_per_tensor_quantized_tensor %987, %float1.562500e-02, %int0 : !torch.vtensor<[1,2048,7,7],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,2048,7,7],!torch.qint8>
    %989 = torch.aten.dequantize.self %988 : !torch.vtensor<[1,2048,7,7],!torch.qint8> -> !torch.vtensor<[1,2048,7,7],f32>
    %990 = torch.aten.quantize_per_tensor %93, %float1.562500e-02, %int0, %int12 : !torch.vtensor<[2048,1024,1,1],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[2048,1024,1,1],!torch.qint8>
    %991 = torch.aten.int_repr %990 : !torch.vtensor<[2048,1024,1,1],!torch.qint8> -> !torch.vtensor<[2048,1024,1,1],si8>
    %992 = torch.aten._make_per_tensor_quantized_tensor %991, %float1.562500e-02, %int0 : !torch.vtensor<[2048,1024,1,1],si8>, !torch.float, !torch.int -> !torch.vtensor<[2048,1024,1,1],!torch.qint8>
    %993 = torch.aten.quantize_per_tensor %94, %float7.812500e-03, %int0, %int12 : !torch.vtensor<[2048],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[2048],!torch.qint8>
    %994 = torch.aten.int_repr %993 : !torch.vtensor<[2048],!torch.qint8> -> !torch.vtensor<[2048],si8>
    %995 = torch.aten._make_per_tensor_quantized_tensor %994, %float7.812500e-03, %int0 : !torch.vtensor<[2048],si8>, !torch.float, !torch.int -> !torch.vtensor<[2048],!torch.qint8>
    %996 = torch.aten.dequantize.self %995 : !torch.vtensor<[2048],!torch.qint8> -> !torch.vtensor<[2048],f32>
    %997 = torch.aten.quantize_per_tensor %996, %float1.220700e-04, %int0, %int14 : !torch.vtensor<[2048],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[2048],!torch.qint32>
    %998 = torch.aten.int_repr %997 : !torch.vtensor<[2048],!torch.qint32> -> !torch.vtensor<[2048],si32>
    %999 = torch.aten.convolution %937, %992, %998, %121, %122, %120, %false, %122, %int1 : !torch.vtensor<[1,1024,14,14],!torch.qint8>, !torch.vtensor<[2048,1024,1,1],!torch.qint8>, !torch.vtensor<[2048],si32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,2048,7,7],si32>
    %1000 = torch.aten._make_per_tensor_quantized_tensor %999, %float1.220700e-04, %int0 : !torch.vtensor<[1,2048,7,7],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,2048,7,7],!torch.qint32>
    %1001 = torch.aten.dequantize.tensor %1000 : !torch.vtensor<[1,2048,7,7],!torch.qint32> -> !torch.vtensor<[1,2048,7,7],f32>
    %1002 = torch.aten.quantize_per_tensor %1001, %float1.562500e-02, %int0, %int12 : !torch.vtensor<[1,2048,7,7],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,2048,7,7],!torch.qint8>
    %1003 = torch.aten.int_repr %1002 : !torch.vtensor<[1,2048,7,7],!torch.qint8> -> !torch.vtensor<[1,2048,7,7],si8>
    %1004 = torch.aten._make_per_tensor_quantized_tensor %1003, %float1.562500e-02, %int0 : !torch.vtensor<[1,2048,7,7],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,2048,7,7],!torch.qint8>
    %1005 = torch.aten.dequantize.self %1004 : !torch.vtensor<[1,2048,7,7],!torch.qint8> -> !torch.vtensor<[1,2048,7,7],f32>
    %1006 = torch.aten.add.Tensor %989, %1005, %int1 : !torch.vtensor<[1,2048,7,7],f32>, !torch.vtensor<[1,2048,7,7],f32>, !torch.int -> !torch.vtensor<[1,2048,7,7],f32>
    %1007 = torch.aten.relu %1006 : !torch.vtensor<[1,2048,7,7],f32> -> !torch.vtensor<[1,2048,7,7],f32>
    %1008 = torch.aten.quantize_per_tensor %1007, %float3.125000e-02, %int0, %int12 : !torch.vtensor<[1,2048,7,7],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,2048,7,7],!torch.qint8>
    %1009 = torch.aten.int_repr %1008 : !torch.vtensor<[1,2048,7,7],!torch.qint8> -> !torch.vtensor<[1,2048,7,7],si8>
    %1010 = torch.aten._make_per_tensor_quantized_tensor %1009, %float3.125000e-02, %int0 : !torch.vtensor<[1,2048,7,7],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,2048,7,7],!torch.qint8>
    %1011 = torch.aten.dequantize.self %1010 : !torch.vtensor<[1,2048,7,7],!torch.qint8> -> !torch.vtensor<[1,2048,7,7],f32>
    %1012 = torch.aten.quantize_per_tensor %95, %float4.882810e-04, %int0, %int12 : !torch.vtensor<[512,2048,1,1],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[512,2048,1,1],!torch.qint8>
    %1013 = torch.aten.int_repr %1012 : !torch.vtensor<[512,2048,1,1],!torch.qint8> -> !torch.vtensor<[512,2048,1,1],si8>
    %1014 = torch.aten._make_per_tensor_quantized_tensor %1013, %float4.882810e-04, %int0 : !torch.vtensor<[512,2048,1,1],si8>, !torch.float, !torch.int -> !torch.vtensor<[512,2048,1,1],!torch.qint8>
    %1015 = torch.aten.quantize_per_tensor %96, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[512],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[512],!torch.qint8>
    %1016 = torch.aten.int_repr %1015 : !torch.vtensor<[512],!torch.qint8> -> !torch.vtensor<[512],si8>
    %1017 = torch.aten._make_per_tensor_quantized_tensor %1016, %float3.906250e-03, %int0 : !torch.vtensor<[512],si8>, !torch.float, !torch.int -> !torch.vtensor<[512],!torch.qint8>
    %1018 = torch.aten.dequantize.self %1017 : !torch.vtensor<[512],!torch.qint8> -> !torch.vtensor<[512],f32>
    %1019 = torch.aten.quantize_per_tensor %1018, %float1.525880e-05, %int0, %int14 : !torch.vtensor<[512],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[512],!torch.qint32>
    %1020 = torch.aten.int_repr %1019 : !torch.vtensor<[512],!torch.qint32> -> !torch.vtensor<[512],si32>
    %1021 = torch.aten.convolution %1010, %1014, %1020, %120, %122, %120, %false, %122, %int1 : !torch.vtensor<[1,2048,7,7],!torch.qint8>, !torch.vtensor<[512,2048,1,1],!torch.qint8>, !torch.vtensor<[512],si32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,512,7,7],si32>
    %1022 = torch.aten._make_per_tensor_quantized_tensor %1021, %float1.525880e-05, %int0 : !torch.vtensor<[1,512,7,7],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,512,7,7],!torch.qint32>
    %1023 = torch.aten.relu %1022 : !torch.vtensor<[1,512,7,7],!torch.qint32> -> !torch.vtensor<[1,512,7,7],!torch.qint32>
    %1024 = torch.aten.int_repr %1023 : !torch.vtensor<[1,512,7,7],!torch.qint32> -> !torch.vtensor<[1,512,7,7],si32>
    %1025 = torch.aten._make_per_tensor_quantized_tensor %1024, %float1.525880e-05, %int0 : !torch.vtensor<[1,512,7,7],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,512,7,7],!torch.qint32>
    %1026 = torch.aten.dequantize.tensor %1025 : !torch.vtensor<[1,512,7,7],!torch.qint32> -> !torch.vtensor<[1,512,7,7],f32>
    %1027 = torch.aten.quantize_per_tensor %1026, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[1,512,7,7],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,512,7,7],!torch.qint8>
    %1028 = torch.aten.int_repr %1027 : !torch.vtensor<[1,512,7,7],!torch.qint8> -> !torch.vtensor<[1,512,7,7],si8>
    %1029 = torch.aten._make_per_tensor_quantized_tensor %1028, %float3.906250e-03, %int0 : !torch.vtensor<[1,512,7,7],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,512,7,7],!torch.qint8>
    %1030 = torch.aten.quantize_per_tensor %97, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[512,512,3,3],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[512,512,3,3],!torch.qint8>
    %1031 = torch.aten.int_repr %1030 : !torch.vtensor<[512,512,3,3],!torch.qint8> -> !torch.vtensor<[512,512,3,3],si8>
    %1032 = torch.aten._make_per_tensor_quantized_tensor %1031, %float3.906250e-03, %int0 : !torch.vtensor<[512,512,3,3],si8>, !torch.float, !torch.int -> !torch.vtensor<[512,512,3,3],!torch.qint8>
    %1033 = torch.aten.quantize_per_tensor %98, %float1.562500e-02, %int0, %int12 : !torch.vtensor<[512],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[512],!torch.qint8>
    %1034 = torch.aten.int_repr %1033 : !torch.vtensor<[512],!torch.qint8> -> !torch.vtensor<[512],si8>
    %1035 = torch.aten._make_per_tensor_quantized_tensor %1034, %float1.562500e-02, %int0 : !torch.vtensor<[512],si8>, !torch.float, !torch.int -> !torch.vtensor<[512],!torch.qint8>
    %1036 = torch.aten.dequantize.self %1035 : !torch.vtensor<[512],!torch.qint8> -> !torch.vtensor<[512],f32>
    %1037 = torch.aten.quantize_per_tensor %1036, %float1.525880e-05, %int0, %int14 : !torch.vtensor<[512],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[512],!torch.qint32>
    %1038 = torch.aten.int_repr %1037 : !torch.vtensor<[512],!torch.qint32> -> !torch.vtensor<[512],si32>
    %1039 = torch.aten.convolution %1029, %1032, %1038, %120, %120, %120, %false, %122, %int1 : !torch.vtensor<[1,512,7,7],!torch.qint8>, !torch.vtensor<[512,512,3,3],!torch.qint8>, !torch.vtensor<[512],si32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,512,7,7],si32>
    %1040 = torch.aten._make_per_tensor_quantized_tensor %1039, %float1.525880e-05, %int0 : !torch.vtensor<[1,512,7,7],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,512,7,7],!torch.qint32>
    %1041 = torch.aten.relu %1040 : !torch.vtensor<[1,512,7,7],!torch.qint32> -> !torch.vtensor<[1,512,7,7],!torch.qint32>
    %1042 = torch.aten.int_repr %1041 : !torch.vtensor<[1,512,7,7],!torch.qint32> -> !torch.vtensor<[1,512,7,7],si32>
    %1043 = torch.aten._make_per_tensor_quantized_tensor %1042, %float1.525880e-05, %int0 : !torch.vtensor<[1,512,7,7],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,512,7,7],!torch.qint32>
    %1044 = torch.aten.dequantize.tensor %1043 : !torch.vtensor<[1,512,7,7],!torch.qint32> -> !torch.vtensor<[1,512,7,7],f32>
    %1045 = torch.aten.quantize_per_tensor %1044, %float1.562500e-02, %int0, %int12 : !torch.vtensor<[1,512,7,7],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,512,7,7],!torch.qint8>
    %1046 = torch.aten.int_repr %1045 : !torch.vtensor<[1,512,7,7],!torch.qint8> -> !torch.vtensor<[1,512,7,7],si8>
    %1047 = torch.aten._make_per_tensor_quantized_tensor %1046, %float1.562500e-02, %int0 : !torch.vtensor<[1,512,7,7],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,512,7,7],!torch.qint8>
    %1048 = torch.aten.quantize_per_tensor %99, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[2048,512,1,1],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[2048,512,1,1],!torch.qint8>
    %1049 = torch.aten.int_repr %1048 : !torch.vtensor<[2048,512,1,1],!torch.qint8> -> !torch.vtensor<[2048,512,1,1],si8>
    %1050 = torch.aten._make_per_tensor_quantized_tensor %1049, %float3.906250e-03, %int0 : !torch.vtensor<[2048,512,1,1],si8>, !torch.float, !torch.int -> !torch.vtensor<[2048,512,1,1],!torch.qint8>
    %1051 = torch.aten.quantize_per_tensor %100, %float1.562500e-02, %int0, %int12 : !torch.vtensor<[2048],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[2048],!torch.qint8>
    %1052 = torch.aten.int_repr %1051 : !torch.vtensor<[2048],!torch.qint8> -> !torch.vtensor<[2048],si8>
    %1053 = torch.aten._make_per_tensor_quantized_tensor %1052, %float1.562500e-02, %int0 : !torch.vtensor<[2048],si8>, !torch.float, !torch.int -> !torch.vtensor<[2048],!torch.qint8>
    %1054 = torch.aten.dequantize.self %1053 : !torch.vtensor<[2048],!torch.qint8> -> !torch.vtensor<[2048],f32>
    %1055 = torch.aten.quantize_per_tensor %1054, %float6.103520e-05, %int0, %int14 : !torch.vtensor<[2048],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[2048],!torch.qint32>
    %1056 = torch.aten.int_repr %1055 : !torch.vtensor<[2048],!torch.qint32> -> !torch.vtensor<[2048],si32>
    %1057 = torch.aten.convolution %1047, %1050, %1056, %120, %122, %120, %false, %122, %int1 : !torch.vtensor<[1,512,7,7],!torch.qint8>, !torch.vtensor<[2048,512,1,1],!torch.qint8>, !torch.vtensor<[2048],si32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,2048,7,7],si32>
    %1058 = torch.aten._make_per_tensor_quantized_tensor %1057, %float6.103520e-05, %int0 : !torch.vtensor<[1,2048,7,7],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,2048,7,7],!torch.qint32>
    %1059 = torch.aten.dequantize.tensor %1058 : !torch.vtensor<[1,2048,7,7],!torch.qint32> -> !torch.vtensor<[1,2048,7,7],f32>
    %1060 = torch.aten.quantize_per_tensor %1059, %float1.562500e-02, %int0, %int12 : !torch.vtensor<[1,2048,7,7],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,2048,7,7],!torch.qint8>
    %1061 = torch.aten.int_repr %1060 : !torch.vtensor<[1,2048,7,7],!torch.qint8> -> !torch.vtensor<[1,2048,7,7],si8>
    %1062 = torch.aten._make_per_tensor_quantized_tensor %1061, %float1.562500e-02, %int0 : !torch.vtensor<[1,2048,7,7],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,2048,7,7],!torch.qint8>
    %1063 = torch.aten.dequantize.self %1062 : !torch.vtensor<[1,2048,7,7],!torch.qint8> -> !torch.vtensor<[1,2048,7,7],f32>
    %1064 = torch.aten.add.Tensor %1063, %1011, %int1 : !torch.vtensor<[1,2048,7,7],f32>, !torch.vtensor<[1,2048,7,7],f32>, !torch.int -> !torch.vtensor<[1,2048,7,7],f32>
    %1065 = torch.aten.relu %1064 : !torch.vtensor<[1,2048,7,7],f32> -> !torch.vtensor<[1,2048,7,7],f32>
    %1066 = torch.aten.quantize_per_tensor %1065, %float3.125000e-02, %int0, %int12 : !torch.vtensor<[1,2048,7,7],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,2048,7,7],!torch.qint8>
    %1067 = torch.aten.int_repr %1066 : !torch.vtensor<[1,2048,7,7],!torch.qint8> -> !torch.vtensor<[1,2048,7,7],si8>
    %1068 = torch.aten._make_per_tensor_quantized_tensor %1067, %float3.125000e-02, %int0 : !torch.vtensor<[1,2048,7,7],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,2048,7,7],!torch.qint8>
    %1069 = torch.aten.dequantize.self %1068 : !torch.vtensor<[1,2048,7,7],!torch.qint8> -> !torch.vtensor<[1,2048,7,7],f32>
    %1070 = torch.aten.quantize_per_tensor %101, %float4.882810e-04, %int0, %int12 : !torch.vtensor<[512,2048,1,1],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[512,2048,1,1],!torch.qint8>
    %1071 = torch.aten.int_repr %1070 : !torch.vtensor<[512,2048,1,1],!torch.qint8> -> !torch.vtensor<[512,2048,1,1],si8>
    %1072 = torch.aten._make_per_tensor_quantized_tensor %1071, %float4.882810e-04, %int0 : !torch.vtensor<[512,2048,1,1],si8>, !torch.float, !torch.int -> !torch.vtensor<[512,2048,1,1],!torch.qint8>
    %1073 = torch.aten.quantize_per_tensor %102, %float7.812500e-03, %int0, %int12 : !torch.vtensor<[512],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[512],!torch.qint8>
    %1074 = torch.aten.int_repr %1073 : !torch.vtensor<[512],!torch.qint8> -> !torch.vtensor<[512],si8>
    %1075 = torch.aten._make_per_tensor_quantized_tensor %1074, %float7.812500e-03, %int0 : !torch.vtensor<[512],si8>, !torch.float, !torch.int -> !torch.vtensor<[512],!torch.qint8>
    %1076 = torch.aten.dequantize.self %1075 : !torch.vtensor<[512],!torch.qint8> -> !torch.vtensor<[512],f32>
    %1077 = torch.aten.quantize_per_tensor %1076, %float1.525880e-05, %int0, %int14 : !torch.vtensor<[512],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[512],!torch.qint32>
    %1078 = torch.aten.int_repr %1077 : !torch.vtensor<[512],!torch.qint32> -> !torch.vtensor<[512],si32>
    %1079 = torch.aten.convolution %1068, %1072, %1078, %120, %122, %120, %false, %122, %int1 : !torch.vtensor<[1,2048,7,7],!torch.qint8>, !torch.vtensor<[512,2048,1,1],!torch.qint8>, !torch.vtensor<[512],si32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,512,7,7],si32>
    %1080 = torch.aten._make_per_tensor_quantized_tensor %1079, %float1.525880e-05, %int0 : !torch.vtensor<[1,512,7,7],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,512,7,7],!torch.qint32>
    %1081 = torch.aten.relu %1080 : !torch.vtensor<[1,512,7,7],!torch.qint32> -> !torch.vtensor<[1,512,7,7],!torch.qint32>
    %1082 = torch.aten.int_repr %1081 : !torch.vtensor<[1,512,7,7],!torch.qint32> -> !torch.vtensor<[1,512,7,7],si32>
    %1083 = torch.aten._make_per_tensor_quantized_tensor %1082, %float1.525880e-05, %int0 : !torch.vtensor<[1,512,7,7],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,512,7,7],!torch.qint32>
    %1084 = torch.aten.dequantize.tensor %1083 : !torch.vtensor<[1,512,7,7],!torch.qint32> -> !torch.vtensor<[1,512,7,7],f32>
    %1085 = torch.aten.quantize_per_tensor %1084, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[1,512,7,7],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,512,7,7],!torch.qint8>
    %1086 = torch.aten.int_repr %1085 : !torch.vtensor<[1,512,7,7],!torch.qint8> -> !torch.vtensor<[1,512,7,7],si8>
    %1087 = torch.aten._make_per_tensor_quantized_tensor %1086, %float3.906250e-03, %int0 : !torch.vtensor<[1,512,7,7],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,512,7,7],!torch.qint8>
    %1088 = torch.aten.quantize_per_tensor %103, %float7.812500e-03, %int0, %int12 : !torch.vtensor<[512,512,3,3],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[512,512,3,3],!torch.qint8>
    %1089 = torch.aten.int_repr %1088 : !torch.vtensor<[512,512,3,3],!torch.qint8> -> !torch.vtensor<[512,512,3,3],si8>
    %1090 = torch.aten._make_per_tensor_quantized_tensor %1089, %float7.812500e-03, %int0 : !torch.vtensor<[512,512,3,3],si8>, !torch.float, !torch.int -> !torch.vtensor<[512,512,3,3],!torch.qint8>
    %1091 = torch.aten.quantize_per_tensor %104, %float1.562500e-02, %int0, %int12 : !torch.vtensor<[512],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[512],!torch.qint8>
    %1092 = torch.aten.int_repr %1091 : !torch.vtensor<[512],!torch.qint8> -> !torch.vtensor<[512],si8>
    %1093 = torch.aten._make_per_tensor_quantized_tensor %1092, %float1.562500e-02, %int0 : !torch.vtensor<[512],si8>, !torch.float, !torch.int -> !torch.vtensor<[512],!torch.qint8>
    %1094 = torch.aten.dequantize.self %1093 : !torch.vtensor<[512],!torch.qint8> -> !torch.vtensor<[512],f32>
    %1095 = torch.aten.quantize_per_tensor %1094, %float3.051760e-05, %int0, %int14 : !torch.vtensor<[512],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[512],!torch.qint32>
    %1096 = torch.aten.int_repr %1095 : !torch.vtensor<[512],!torch.qint32> -> !torch.vtensor<[512],si32>
    %1097 = torch.aten.convolution %1087, %1090, %1096, %120, %120, %120, %false, %122, %int1 : !torch.vtensor<[1,512,7,7],!torch.qint8>, !torch.vtensor<[512,512,3,3],!torch.qint8>, !torch.vtensor<[512],si32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,512,7,7],si32>
    %1098 = torch.aten._make_per_tensor_quantized_tensor %1097, %float3.051760e-05, %int0 : !torch.vtensor<[1,512,7,7],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,512,7,7],!torch.qint32>
    %1099 = torch.aten.relu %1098 : !torch.vtensor<[1,512,7,7],!torch.qint32> -> !torch.vtensor<[1,512,7,7],!torch.qint32>
    %1100 = torch.aten.int_repr %1099 : !torch.vtensor<[1,512,7,7],!torch.qint32> -> !torch.vtensor<[1,512,7,7],si32>
    %1101 = torch.aten._make_per_tensor_quantized_tensor %1100, %float3.051760e-05, %int0 : !torch.vtensor<[1,512,7,7],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,512,7,7],!torch.qint32>
    %1102 = torch.aten.dequantize.tensor %1101 : !torch.vtensor<[1,512,7,7],!torch.qint32> -> !torch.vtensor<[1,512,7,7],f32>
    %1103 = torch.aten.quantize_per_tensor %1102, %float1.562500e-02, %int0, %int12 : !torch.vtensor<[1,512,7,7],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,512,7,7],!torch.qint8>
    %1104 = torch.aten.int_repr %1103 : !torch.vtensor<[1,512,7,7],!torch.qint8> -> !torch.vtensor<[1,512,7,7],si8>
    %1105 = torch.aten._make_per_tensor_quantized_tensor %1104, %float1.562500e-02, %int0 : !torch.vtensor<[1,512,7,7],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,512,7,7],!torch.qint8>
    %1106 = torch.aten.quantize_per_tensor %105, %float7.812500e-03, %int0, %int12 : !torch.vtensor<[2048,512,1,1],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[2048,512,1,1],!torch.qint8>
    %1107 = torch.aten.int_repr %1106 : !torch.vtensor<[2048,512,1,1],!torch.qint8> -> !torch.vtensor<[2048,512,1,1],si8>
    %1108 = torch.aten._make_per_tensor_quantized_tensor %1107, %float7.812500e-03, %int0 : !torch.vtensor<[2048,512,1,1],si8>, !torch.float, !torch.int -> !torch.vtensor<[2048,512,1,1],!torch.qint8>
    %1109 = torch.aten.quantize_per_tensor %106, %float7.812500e-03, %int0, %int12 : !torch.vtensor<[2048],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[2048],!torch.qint8>
    %1110 = torch.aten.int_repr %1109 : !torch.vtensor<[2048],!torch.qint8> -> !torch.vtensor<[2048],si8>
    %1111 = torch.aten._make_per_tensor_quantized_tensor %1110, %float7.812500e-03, %int0 : !torch.vtensor<[2048],si8>, !torch.float, !torch.int -> !torch.vtensor<[2048],!torch.qint8>
    %1112 = torch.aten.dequantize.self %1111 : !torch.vtensor<[2048],!torch.qint8> -> !torch.vtensor<[2048],f32>
    %1113 = torch.aten.quantize_per_tensor %1112, %float1.220700e-04, %int0, %int14 : !torch.vtensor<[2048],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[2048],!torch.qint32>
    %1114 = torch.aten.int_repr %1113 : !torch.vtensor<[2048],!torch.qint32> -> !torch.vtensor<[2048],si32>
    %1115 = torch.aten.convolution %1105, %1108, %1114, %120, %122, %120, %false, %122, %int1 : !torch.vtensor<[1,512,7,7],!torch.qint8>, !torch.vtensor<[2048,512,1,1],!torch.qint8>, !torch.vtensor<[2048],si32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[1,2048,7,7],si32>
    %1116 = torch.aten._make_per_tensor_quantized_tensor %1115, %float1.220700e-04, %int0 : !torch.vtensor<[1,2048,7,7],si32>, !torch.float, !torch.int -> !torch.vtensor<[1,2048,7,7],!torch.qint32>
    %1117 = torch.aten.dequantize.tensor %1116 : !torch.vtensor<[1,2048,7,7],!torch.qint32> -> !torch.vtensor<[1,2048,7,7],f32>
    %1118 = torch.aten.quantize_per_tensor %1117, %float1.562500e-02, %int0, %int12 : !torch.vtensor<[1,2048,7,7],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,2048,7,7],!torch.qint8>
    %1119 = torch.aten.int_repr %1118 : !torch.vtensor<[1,2048,7,7],!torch.qint8> -> !torch.vtensor<[1,2048,7,7],si8>
    %1120 = torch.aten._make_per_tensor_quantized_tensor %1119, %float1.562500e-02, %int0 : !torch.vtensor<[1,2048,7,7],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,2048,7,7],!torch.qint8>
    %1121 = torch.aten.dequantize.self %1120 : !torch.vtensor<[1,2048,7,7],!torch.qint8> -> !torch.vtensor<[1,2048,7,7],f32>
    %1122 = torch.aten.add.Tensor %1121, %1069, %int1 : !torch.vtensor<[1,2048,7,7],f32>, !torch.vtensor<[1,2048,7,7],f32>, !torch.int -> !torch.vtensor<[1,2048,7,7],f32>
    %1123 = torch.aten.relu %1122 : !torch.vtensor<[1,2048,7,7],f32> -> !torch.vtensor<[1,2048,7,7],f32>
    %1124 = torch.aten.quantize_per_tensor %1123, %float6.250000e-02, %int0, %int12 : !torch.vtensor<[1,2048,7,7],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,2048,7,7],!torch.qint8>
    %1125 = torch.aten.int_repr %1124 : !torch.vtensor<[1,2048,7,7],!torch.qint8> -> !torch.vtensor<[1,2048,7,7],si8>
    %1126 = torch.aten._make_per_tensor_quantized_tensor %1125, %float6.250000e-02, %int0 : !torch.vtensor<[1,2048,7,7],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,2048,7,7],!torch.qint8>
    %1127 = torch.aten.dequantize.self %1126 : !torch.vtensor<[1,2048,7,7],!torch.qint8> -> !torch.vtensor<[1,2048,7,7],f32>
    %1128 = torch.prim.ListConstruct %int7, %int7 : (!torch.int, !torch.int) -> !torch.list<int>
    %1129 = torch.aten.avg_pool2d %1127, %1128, %120, %122, %false, %false, %none : !torch.vtensor<[1,2048,7,7],f32>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.bool, !torch.none -> !torch.vtensor<[1,2048,1,1],f32>
    %1130 = torch.aten.mul.Tensor %1129, %0 : !torch.vtensor<[1,2048,1,1],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[1,2048,1,1],f32>
    %1131 = torch.aten.quantize_per_tensor %1130, %float3.125000e-02, %int0, %int12 : !torch.vtensor<[1,2048,1,1],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,2048,1,1],!torch.qint8>
    %1132 = torch.aten.int_repr %1131 : !torch.vtensor<[1,2048,1,1],!torch.qint8> -> !torch.vtensor<[1,2048,1,1],si8>
    %1133 = torch.aten._make_per_tensor_quantized_tensor %1132, %float3.125000e-02, %int0 : !torch.vtensor<[1,2048,1,1],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,2048,1,1],!torch.qint8>
    %1134 = torch.aten.dequantize.self %1133 : !torch.vtensor<[1,2048,1,1],!torch.qint8> -> !torch.vtensor<[1,2048,1,1],f32>
    %1135 = torch.prims.collapse %1134, %int1, %int3 : !torch.vtensor<[1,2048,1,1],f32>, !torch.int, !torch.int -> !torch.vtensor<[1,2048],f32>
    %1136 = torch.prims.collapse %1135, %int0, %int0 : !torch.vtensor<[1,2048],f32>, !torch.int, !torch.int -> !torch.vtensor<[1,2048],f32>
    %1137 = torch.aten.quantize_per_tensor %107, %float3.906250e-03, %int0, %int12 : !torch.vtensor<[1000,2048],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1000,2048],!torch.qint8>
    %1138 = torch.aten.int_repr %1137 : !torch.vtensor<[1000,2048],!torch.qint8> -> !torch.vtensor<[1000,2048],si8>
    %1139 = torch.aten.quantize_per_tensor %108, %float4.882810e-04, %int0, %int12 : !torch.vtensor<[1000],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1000],!torch.qint8>
    %1140 = torch.aten.int_repr %1139 : !torch.vtensor<[1000],!torch.qint8> -> !torch.vtensor<[1000],si8>
    %1141 = torch.aten._make_per_tensor_quantized_tensor %1140, %float4.882810e-04, %int0 : !torch.vtensor<[1000],si8>, !torch.float, !torch.int -> !torch.vtensor<[1000],!torch.qint8>
    %1142 = torch.aten.dequantize.self %1141 : !torch.vtensor<[1000],!torch.qint8> -> !torch.vtensor<[1000],f32>
    %1143 = torch.aten.transpose.int %1138, %int0, %int1 : !torch.vtensor<[1000,2048],si8>, !torch.int, !torch.int -> !torch.vtensor<[2048,1000],si8>
    %1144 = torch.aten._make_per_tensor_quantized_tensor %1143, %float3.906250e-03, %int0 : !torch.vtensor<[2048,1000],si8>, !torch.float, !torch.int -> !torch.vtensor<[2048,1000],!torch.qint8>
    %1145 = torch.aten.mm %1136, %1144 : !torch.vtensor<[1,2048],f32>, !torch.vtensor<[2048,1000],!torch.qint8> -> !torch.vtensor<[1,1000],f32>
    %1146 = torch.aten.add.Tensor %1145, %1142, %int1 : !torch.vtensor<[1,1000],f32>, !torch.vtensor<[1000],f32>, !torch.int -> !torch.vtensor<[1,1000],f32>
    %1147 = torch.aten.quantize_per_tensor %1146, %float6.250000e-02, %int0, %int12 : !torch.vtensor<[1,1000],f32>, !torch.float, !torch.int, !torch.int -> !torch.vtensor<[1,1000],!torch.qint8>
    %1148 = torch.aten.int_repr %1147 : !torch.vtensor<[1,1000],!torch.qint8> -> !torch.vtensor<[1,1000],si8>
    %1149 = torch.aten._make_per_tensor_quantized_tensor %1148, %float6.250000e-02, %int0 : !torch.vtensor<[1,1000],si8>, !torch.float, !torch.int -> !torch.vtensor<[1,1000],!torch.qint8>
    %1150 = torch.aten.dequantize.self %1149 : !torch.vtensor<[1,1000],!torch.qint8> -> !torch.vtensor<[1,1000],f32>
    return %1150 : !torch.vtensor<[1,1000],f32>
  }
}

