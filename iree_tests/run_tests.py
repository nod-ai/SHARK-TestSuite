# Copyright 2024 Advanced Micro Devices
#
# Licensed under the Apache License v2.0 with LLVM Exceptions.
# See https://llvm.org/LICENSE.txt for license information.
# SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception

from pathlib import Path
import subprocess
import sys

THIS_DIR = Path(__file__).parent
REPO_ROOT = THIS_DIR.parent

# Test directory generated by iree_tests/onnx/scripts/generate_onnx_tests.py
# TODO(scotttodd): flag to run across any compatible test suite folder
TEST_SOURCE_ROOT = REPO_ROOT / "iree_tests/onnx/node/generated"

# Write lists of tests that passed/failed to run.
RUN_SUCCESSES_FILE = REPO_ROOT / "iree_tests/onnx/node/run_successes.txt"
RUN_FAILURES_FILE = REPO_ROOT / "iree_tests/onnx/node/run_failures.txt"


def find_tests(root_dir_path):
    test_dir_paths = []
    for test_dir_path in root_dir_path.iterdir():
        if not test_dir_path.is_dir():
            continue
        config_flagfile_path = test_dir_path / "config_cpu_flags.txt"
        if config_flagfile_path.exists():
            test_dir_paths.append(test_dir_path)

    print(f"Found {len(test_dir_paths)} tests")
    return test_dir_paths


def run_test(test_dir_path):
    config_flagfile_path = test_dir_path / "config_cpu_flags.txt"
    test_data_flagfile_path = test_dir_path / "test_data_flags.txt"

    exec_args = [
        "iree-run-module",
        f"--flagfile={config_flagfile_path.name}",
        f"--flagfile={test_data_flagfile_path.name}",
    ]
    # print("  Exec:", " ".join(exec_args))
    ret = subprocess.run(exec_args, capture_output=True, cwd=test_dir_path)
    if ret.returncode != 0:
        # print(
        #     f"  {test_dir_path.name[5:]} run failed,\n    stdout: {ret.stdout},\n    stderr: {ret.stderr}",
        #     file=sys.stderr,
        # )
        print(f"  {test_dir_path.name[5:]} run failed", file=sys.stderr)
        return False

    return True


if __name__ == "__main__":
    test_dir_paths = find_tests(TEST_SOURCE_ROOT)

    # TODO(scotttodd): clear .vmfb files from source/build dir?
    RUN_FAILURES_FILE.unlink(missing_ok=True)

    print(f"Compiling tests in '{TEST_SOURCE_ROOT}'")

    print("******************************************************************")
    passed_runs = []
    failed_runs = []
    # TODO(scotttodd): parallelize this (or move into a test runner like pytest)
    for i in range(len(test_dir_paths)):
        test_dir_path = test_dir_paths[i]
        test_name = test_dir_path.name

        current_number = str(i).rjust(4, "0")
        progress_str = f"[{current_number}/{len(test_dir_paths)}]"
        print(f"{progress_str}: Running {test_name}")

        test_dir_path = Path(TEST_SOURCE_ROOT) / test_name
        run_result = run_test(test_dir_path)
        if run_result:
            passed_runs.append(test_name)
        else:
            failed_runs.append(test_name)
    print("******************************************************************")

    passed_runs.sort()
    failed_runs.sort()

    with open(RUN_SUCCESSES_FILE, "wt") as f:
        f.write("\n".join(passed_runs))
    with open(RUN_FAILURES_FILE, "wt") as f:
        f.write("\n".join(failed_runs))

    print(f"Run pass count: {len(passed_runs)}")
    print(f"Run fail count: {len(failed_runs)}")
